{"meta":{"title":"派大星の博客","subtitle":"记录生活中的点点滴滴","description":"直到这一刻微笑着说话为止，我至少留下了一公升眼泪","author":"派大星","url":"http://pdxblog.top","root":"/"},"pages":[{"title":"tags","date":"2020-01-25T12:57:30.000Z","updated":"2020-01-25T12:58:43.644Z","comments":true,"path":"tags/index.html","permalink":"http://pdxblog.top/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-01-24T14:29:44.000Z","updated":"2020-01-24T14:30:26.053Z","comments":true,"path":"categories/index.html","permalink":"http://pdxblog.top/categories/index.html","excerpt":"","text":""},{"title":"派大星","date":"2020-01-25T13:20:10.000Z","updated":"2020-01-25T13:22:20.493Z","comments":true,"path":"about/index.html","permalink":"http://pdxblog.top/about/index.html","excerpt":"","text":"扎实的专业知识是我最大的财富；认真踏实是我做事的原则；不断超越创新是我追求的目标；"}],"posts":[{"title":"k8s监控","slug":"k8监控","date":"2020-02-27T16:00:00.000Z","updated":"2020-02-28T07:01:56.573Z","comments":true,"path":"k8监控.html","link":"","permalink":"http://pdxblog.top/k8%E7%9B%91%E6%8E%A7.html","excerpt":"","text":"一、k8s的UI访问界面-dashboard General-purpose web UI for Kubernetes clusters 用于Kubernetes集群的通用web UI 在dashbord中，虽然可以做到创建、删除、修改资源等操作，但通常情况下，我们会把它当作监控k8s集群的软件 dashboard能够直观的看到rc、deployment、pod、services等k8s组件的运行情况和日志信息。 1、从Github搜索dasgboard，下载yaml文件 1234567[root@master ~]# mkdir dashboard[root@master ~]# cd dashboard/[root@master dashboard]# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc5/aio/deploy/recommended.yaml[root@master dashboard]# lsrecommended.yaml[root@node01 ~]# docker pull kubernetesui/dashboard:v2.0.0-rc5[root@node02 ~]# docker pull kubernetesui/dashboard:v2.0.0-rc5 2、运行yaml文件： 1234567891011121314151617181920修改service类型类NodePort#在40行的spec字段修改[root@master dashboard]# vim recommended.yamlspec: type: NodePort[root@master dashboard]# kubectl apply -f recommended.yaml namespace/kubernetes-dashboard createdserviceaccount/kubernetes-dashboard createdservice/kubernetes-dashboard createdsecret/kubernetes-dashboard-certs createdsecret/kubernetes-dashboard-csrf createdsecret/kubernetes-dashboard-key-holder createdconfigmap/kubernetes-dashboard-settings createdrole.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createddeployment.apps/kubernetes-dashboard createdservice/dashboard-metrics-scraper createddeployment.apps/dashboard-metrics-scraper created 123456789101112[root@master dashboard]# kubectl get pod -n kubernetes-dashboard NAME READY STATUS RESTARTS AGEdashboard-metrics-scraper-7f5767668b-f7nh6 1/1 Running 0 9m32skubernetes-dashboard-57b4bcc994-2rj9k 1/1 Running 0 9m32s[root@master dashboard]# kubectl get svc -n kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdashboard-metrics-scraper ClusterIP 10.111.237.119 &lt;none&gt; 8000/TCP 16mkubernetes-dashboard NodePort 10.107.77.172 &lt;none&gt; 443:30361/TCP 16m[root@master dashboard]# kubectl get deployments. -n kubernetes-dashboard NAME READY UP-TO-DATE AVAILABLE AGEdashboard-metrics-scraper 1/1 1 1 11m #将收集到的数据制作成图表的形式kubernetes-dashboard 1/1 1 1 11m 3、通过浏览器访问：https://192.168.1.70:30361 两种登录方式： kubeconfig：配置文件 Token：令牌 基于Token的方法登录dashboard 1、创建一个dashboaed的管理用户 12[root@master dashboard]# kubectl create serviceaccount dashboard-admin -n kube-system serviceaccount/dashboard-admin created 2、将这个用户绑定为集群管理用户 1[root@master dashboard]# kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin 3、获取Token 1234567891011#得到Token的名称[root@master dashboard]# kubectl get secrets -n kube-system | grep dashboard-admindashboard-admin-token-mwht2 kubernetes.io/service-account-token 3 5m30s#查看上述得到的secret资源的详细信息，会得到Token[root@master dashboard]# kubectl get secrets -n kube-system dashboard-admin-token-mwht2 NAME TYPE DATA AGEdashboard-admin-token-mwht2 kubernetes.io/service-account-token 3 7m19s//这个类型不是Opaque，说明不是隐藏的，我们可以看到他的详细信息#获取详细信息，得到Token[root@master dashboard]# kubectl describe secrets -n kube-system dashboard-admin-token-mwht2token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tbXdodDIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNjliODE4YjYtOTA3Zi00NTBmLWI3NjgtMTc2ODIyM2Y1OTIyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.WCcVrx6oXs0k7-0hQOOik30ZPJl0sNeQE987PHv_Jm9ZpLQ4P9VIQdN49uRvNsd7DF4Ozgu5enWFvNsiaCDmYHauK2LAoHbDBURE9wGx8VlMaaquZ1B_ur4lOluP6Ha3wdZB64fEdtrg-6-DjIS7SC2Kqr2Bcl8NeRdtABh3cufgJ2EQoU40-FUy-0ahegYixIrrQ-DXgZeGrXP79RzHmBXaSwbRwTqWXwNf0e25on_gCiiMC-MVmbZ0MXhNNv-jc8uD2obaEUTdOCLg__f482Zy7xLEMjBv9eVn0P5u7c8r45VfDs08zK4Leh5GI4KIgcuxt37TCtfmEz5XEoTLnA 4、在浏览器上使用Token登录 PS：如果是使用的是旧版本的dashboard，使用谷歌浏览器登录，可能不成功，需要换成其他的浏览器，比如火狐 //如果没有显示，就说明serviceaccount，没有绑定账号，就说明没有权限，就什么都看不到 这里我们可以创建资源，有三种方式 从表单创建默认的是Deployment资源对象 还有一些扩容缩容、更新，删除的操作 除了基于Token的方法登录dashboard，还有基于kuberconfig配置文件的登录方式 1、获取Tonke 1234567#得到Token的名称[root@master dashboard]# kubectl get secrets -n kube-system | grep dashboard-admindashboard-admin-token-mwht2 kubernetes.io/service-account-token 3 5m30s#查看上述得到的secret资源的详细信息，会得到Token[root@master dashboard]# kubectl get secrets -n kube-system dashboard-admin-token-mwht2 NAME TYPE DATA AGEdashboard-admin-token-mwht2 kubernetes.io/service-account-token 3 7m19s 2、生成kubeconfig配置文件 12345678910#设置一个环境变量代表获取Token[root@master dashboard]# DASH_TOKEN=$(kubectl get secrets -n kube-system dashboard-admin-token-mwht2 -o jsonpath=&#123;.data.token&#125; | base64 -d)#将k8s集群的配置信息写入kubeconfig配置文件中[root@master dashboard]# kubectl config set-cluster kubernetes --server=192.168.1.70:6443 --kubeconfig=/root/.dashboard-admin.conf#将Token写入配置文件里[root@master dashboard]# kubectl config set-credentials dashboard-admin --token=$DASH_TOKEN --kubeconfig=/root/.dashboard-admin.conf[root@master dashboard]# kubectl config set-context dashboard-admin@kubernetes --cluster=kubernetes --user=dashboard-admin --kubeconfig=/root/.dashboard-admin.conf[root@master dashboard]# kubectl config use-context dashboard-admin@kubernetes --kubeconfig=/root/.dashboard-admin.conf 3、将生成的/root/.dashboard-admin.config的配置文件，导出并保存 1[root@master dashboard]# sz /root/.dashboard-admin.conf 4、从浏览器选择kubeconfig的登录方式，然后导入配置文件即可 二、weave-scope监控k8s集群ScopeWeave Scope是Weaveworks开发的监控工具。Weave Scope在Kubernetes集群中生成进程，容器和主机的映射，以帮助实时了解Docker容器。还可基于图形UI管理容器并在容器上运行诊断命令 1、在Github上直接搜索scope，找到yaml文件并下载下来 2、往下拉找到kubernetes，点击 3、将这个yaml文件下载下来 1[root@master ~]# wget https://cloud.weave.works/k8s/scope.yaml 4、修改yaml，修改service的端口类型 123456789101112131415161718192021222324[root@master ~]# vim scope.yaml#在212行的spec字段中添加 type: NodePort#保存并退出，运行yaml文件root@master ~]# kubectl apply -f scope.yaml namespace/weave createdserviceaccount/weave-scope createdclusterrole.rbac.authorization.k8s.io/weave-scope createdclusterrolebinding.rbac.authorization.k8s.io/weave-scope createddeployment.apps/weave-scope-app createdservice/weave-scope-app createddeployment.apps/weave-scope-cluster-agent createddaemonset.apps/weave-scope-agent created[root@master ~]# kubectl get deployments. -n weave NAME READY UP-TO-DATE AVAILABLE AGEweave-scope-app 1/1 1 1 26m #展示信息weave-scope-cluster-agent 1/1 1 1 26m #收集信息[root@master ~]# kubectl get pod -n weave NAME READY STATUS RESTARTS AGEweave-scope-agent-jv4g8 1/1 Running 0 21sweave-scope-agent-kw7x9 1/1 Running 0 21sweave-scope-agent-vnqks 1/1 Running 0 21sweave-scope-app-78cff98cbc-nx6p5 1/1 Running 0 21sweave-scope-cluster-agent-7cc889fbbf-tnrhv 1/1 Running 0 21s 5、查看端口，使用浏览器访问：192.168.1.70:30366 123[root@master ~]# kubectl get svc -n weave NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEweave-scope-app NodePort 10.110.46.45 &lt;none&gt; 80:30366/TCP 41s 在scope的web界面中，可以查看很多的东西，pod、node节点等详细信息，包括打开容器的终端，查看其日志信息等等…… 三、PrometheusPrometheus可以原生地监测Kubernetes，Prometheus Operator简化了Kubernetes上的Prometheus设置，并允许使用Prometheus适配器提供自定义指标API。 Prometheus提供强大的查询语言和内置仪表板，用于查询和可视化数据 PS：在这里部署的prometheus，并不是Prometheus官网提供的，而是使用的coreOS提供的Prometheus项目 Prometheus各个组件的作用： MetricsServer：是k8s集群资源使用情况的聚合器，收集数据给k8s集群内使用，如kubectl，hpa，scheduler等 Prometheus Operator：是一个系统检测和警报工具箱，用来存储监控数据 Prometheus node-exporter：收集k8s集群内资源的数据，指定告警规则 Prometheus：收集apiServer，scheduler，contorller-manager，kubelet组件的数据，通过http协议传输 Grnfana：可视化数据统计和监控平台 1、克隆Prometheus的项目地址到本地 12345[root@master ~]# mkdir prometheus[root@master ~]# cd prometheus/[root@master prometheus]# git clone https://github.com/coreos/kube-prometheus.git[root@master prometheus]# lskube-prometheus 2、修改grafana-service.yaml文件，使用NodePort的暴露方式，暴露的端口为31001 12345678910[root@master prometheus]# cd kube-prometheus/manifests/[root@master manifests]# vim grafana-service.yaml#在spec字段下添加spec: type: NodePort ports: - name: http port: 3000 targetPort: http nodePort: 31001 3、修改prometheus-service.yaml文件，使用NodePort的暴露方式，暴露的端口为31002 123456789[root@master manifests]# vim prometheus-service.yaml#在spec字段下添加spec: type: NodePort ports: - name: web port: 9090 targetPort: web nodePort: 31002 4、修改alertmanager-service.yaml（配置告警模板）文件，使用NodePort的暴露方式，暴露的端口为31003 123456789[root@master manifests]# vim alertmanager-service.yaml#在spec字段下添加spec: type: NodePort ports: - name: web port: 9093 targetPort: web nodePort: 31003 5、将这个目录中的yaml，全部运行，是运行以上yaml文件的基础环境配置 12345[root@master manifests]# cd setup/ #如果想要运行上面的yaml，首先要运行基础环境的设置[root@master setup]# cd ..[root@master manifests]# pwd/root/prometheus/kube-prometheus/manifests[root@master manifests]# kubectl apply -f setup/ 6、运行主yaml文件 1234[root@master manifests]# cd ..[root@master kube-prometheus]# pwd/root/prometheus/kube-prometheus[root@master kube-prometheus]# kubectl apply -f manifests/ 7、浏览器访问：192.168.1.70:31001//根据提示修改密码，然后保存登录 //将这三个导入一下 浏览器访问grafan官网：https://grafana.com/导入监控模板 搜索prometheus，选择相应的模板 复制ID号 回到grafan，导入模板 部署成功以后，就可运行一条命令，查看资源使用情况（MetricsServer必须部署成功） 12345[root@master ~]# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% master 182m 9% 1380Mi 80% node01 383m 19% 1402Mi 81% node02 396m 19% 1406Mi 81% 总结dashboard： 可以查看集群中应用的运行状态，也能够修改、创建k8s集群中的个各种资源 用于Kubernetes集群的通用web UI，在dasgboard中，虽然可以做到创建、删除、修改资源等操作，但通常情况下，我们会把它仿作监控k8s集群的软件。dashboard能够直观的看到rc、deployment、pod、service等k8s组件与逆行的情况和日志信息 weave-scope： 可以查看集群中应用的运行状态，也能够修改、创建k8s集群中的个各种资源 Weave Scope是Weaveworks开发的监控工具。Weave Scope在Kubernetes集群中生成进程，容器和主机的映射，以帮助实时了解Docker容器。还可基于图形UI管理容器并在容器上运行诊断命令 Prometheus： Prometheus是一个开源系统监控和报警工具。 Prometheus服务可以直接通过目标拉取数据，或者间接地通过中间网关拉取数据。它在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中，PromQL和其他API可视化展示收集的数据在K8s中，关于集群的资源有metrics度量值的概念，有各种不同的exporter可以通过api接口对外提供各种度量值的及时数据，prometheus在与k8s融合工作的过程中就是通过与这些提供metric值的exporter进行交互，获取数据，整合数据，展示数据，触发告警的过程 Prometheus可以原生地监测Kubernetes，Prometheus Operator简化了Kubernetes上的Prometheus设置，并允许使用Prometheus适配器提供自定义指标API。 Prometheus提供强大的查询语言和内置仪表板，用于查询和可视化数据","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"Ingress实现虚拟主机和Https代理访问","slug":"Ingress实现虚拟主机","date":"2020-02-25T16:00:00.000Z","updated":"2020-02-28T07:01:19.747Z","comments":true,"path":"Ingress实现虚拟主机.html","link":"","permalink":"http://pdxblog.top/Ingress%E5%AE%9E%E7%8E%B0%E8%99%9A%E6%8B%9F%E4%B8%BB%E6%9C%BA.html","excerpt":"","text":"Ingress实现虚拟主机和Https代理访问虚拟主机，也叫“网站空间”，就是把一台运行在互联网上的物理服务器划分成多个“虚拟”服务器。虚拟主机技术极大的促进了网络技术的应用和普及。同时虚拟主机的租用服务也成了网络时代的一种新型经济形式 1、首先确定要运行Ingress-nginx-controller服务 123[root@master ~]# kubectl get pod -n ingress-nginx NAME READY STATUS RESTARTS AGEnginx-ingress-controller-5954d475b6-ktpf9 1/1 Running 1 43h 2、将Ingress-nginx-controller暴露为一个service资源对象 123[root@master ~]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.100.97.246 &lt;none&gt; 80:32007/TCP,443:30741/TCP 43h 3、创建一个Deployment资源和一个Service资源，并相互关联 1234567891011121314151617181920212223242526272829[root@master ~]# vim deploy1.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: deploy1spec: replicas: 2 template: metadata: labels: app: nginx1 spec: containers: - name: nginx1 image: nginx---apiVersion: v1kind: Servicemetadata: name: svc-1spec: selector: app: nginx1 ports: - port: 80 targetPort: 80[root@master ~]# kubectl apply -f deploy1.yaml deployment.extensions/deploy1 createdservice/svc-1 created 12345678[root@master ~]# kubectl get pod NAME READY STATUS RESTARTS AGEdeploy1-7df6778547-v6ww9 1/1 Running 0 2m33sdeploy1-7df6778547-vkvwf 1/1 Running 0 2m33s[root@master ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 52dsvc-1 ClusterIP 10.109.213.247 &lt;none&gt; 80/TCP 3m17s 4、创建另外“一对”服务（delpoy2.yaml和svc-2） 1234567891011121314151617181920212223242526272829[root@master ~]# vim deploy2.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: deploy2spec: replicas: 2 template: metadata: labels: app: nginx2 spec: containers: - name: nginx2 image: nginx #这里没有更换镜像，使用相同的nginx镜像---apiVersion: v1kind: Servicemetadata: name: svc-2spec: selector: app: nginx2 ports: - port: 80 targetPort: 80[root@master ~]# kubectl apply -f deploy2.yaml deployment.extensions/deploy2 createdservice/svc-2 created 12345678[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEdeploy2-7b6786d8bf-6xnjs 1/1 Running 0 19sdeploy2-7b6786d8bf-dvjqt 1/1 Running 0 19s[root@master ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 52dsvc-2 ClusterIP 10.106.67.155 &lt;none&gt; 80/TCP 24s 4、创建Ingress规则 12345678910111213141516171819202122232425262728293031[root@master ~]# vim ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-1spec: rules: - host: www1.bdqn.com http: paths: - path: / backend: serviceName: svc-1 servicePort: 80---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-2spec: rules: - host: www2.bdqn.com http: paths: - path: / backend: serviceName: svc-2 servicePort: 80[root@master ~]# kubectl apply -f ingress.yaml ingress.extensions/ingress-1 createdingress.extensions/ingress-2 created 123456789101112131415[root@master ~]# kubectl describe ingresses. ingress-1Rules: Host Path Backends ---- ---- -------- www1.bdqn.com / svc-1:80 (10.244.1.4:80,10.244.2.4:80)[root@master ~]# kubectl describe ingresses. ingress-1Rules: Host Path Backends ---- ---- -------- www2.bdqn.com / svc-2:80 (10.244.1.5:80,10.244.2.5:80)[root@master ~]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.100.97.246 &lt;none&gt; 80:32007/TCP,443:30741/TCP 43h 5、由于实验环境限制（这个域名是假的），所以自己用来模拟一个域名 123在windows上添加域名解析：C:\\Windows\\System32\\drivers\\etc192.168.1.70 www1.bdqn.com192.168.1.70 www2.bdqn.com Ingress资源实现https代理访问在上面的操作中，实现了使用ingress-nginx为后端所有pod提供一个统一的入口，那么，有一个非常严肃的问题需要考虑，就是如何为我们的pod配置CA证书来实现HTTPS访问？在pod中直接配置CA么？那需要进行多少重复性的操作？而且，pod是随时可能被kubelet杀死再创建的。当然这些问题有很多解决方法，比如直接将CA配置到镜像中，但是这样又需要很多个CA证书。 这里有更简便的一种方法，就拿上面的情况来说，后端有多个pod，pod与service进行关联，service又被ingress规则发现并动态写入到ingress-nginx-controller容器中，然后又为ingress-nginx-controller创建了一个Service映射到群集节点上的端口，来供client来访问。 在上面的一系列流程中，关键的点就在于Ingress规则，我们只需要在Ingress的yaml文件中，为域名配置CA证书即可，只要可以通过HTTPS访问到域名，至于这个域名是怎么关联到后端提供服务的pod，这就是属于k8s群集内部的通信了，即便是使用http来通信，也无伤大雅 1、生成一个证书： 1234567891011[root@master ~]# mkdir https[root@master ~]# cd https[root@master https]# openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=testsvc /0=testsvc\"Generating a 2048 bit RSA private key......................................................................................+++............+++writing new private key to 'tls.key'-----Subject Attribute 0 has no known NID, skipped[root@master https]# lstls.crt tls.key 2、创建secret资源，保存证书： 12[root@master https]# kubectl create secret tls tls-secret --key=tls.key --cert tls.crtsecret/tls-secret created 3、创建一个Deployment资源对象，用来模拟web服务 1234567891011121314151617181920212223242526272829[root@master https]# vim deploy3.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: deploy3spec: replicas: 2 template: metadata: labels: app: nginx3 spec: containers: - name: nginx3 image: nginx---apiVersion: v1kind: Servicemetadata: name: svc-3spec: selector: app: nginx3 ports: - port: 80 targetPort: 80[root@master https]# kubectl apply -f deploy3.yamldeployment.extensions/deploy3 createdservice/svc-3 created 12345678910[root@master https]# kubectl get podNAME READY STATUS RESTARTS AGEdeploy3-5c545fcc5f-4n9bw 1/1 Running 0 17sdeploy3-5c545fcc5f-7b4g2 1/1 Running 0 17s[root@master https]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 52dsvc-3 ClusterIP 10.97.212.56 &lt;none&gt; 80/TCP 22m[root@master https]# curl -I 10.97.212.56HTTP/1.1 200 OK 4、创建对应的Ingress规则 12345678910111213141516171819202122[root@master https]# vim ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-3spec: tls: #引用CA证书 - hosts: - www3.bdqn.com secretName: tls-secret rules: - host: www3.bdqn.com http: paths: - path: / backend: serviceName: svc-3 servicePort: 80[root@master https]# kubectl apply -f ingress.yaml ingress.extensions/ingress-3 created//同样，添加域名解析192.168.1.70 www3.bdqn.com 5、查找对应service-NodePort的443端口映射的端口，直接用浏览器访问即可 123[root@master https]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.100.97.246 &lt;none&gt; 80:32007/TCP,443:30741/TCP 44h 通过浏览器访问：https://www3.bdqn.com:30741","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"Ingress原理及配置","slug":"Ingress","date":"2020-02-23T16:00:00.000Z","updated":"2020-02-24T06:19:01.787Z","comments":true,"path":"Ingress.html","link":"","permalink":"http://pdxblog.top/Ingress.html","excerpt":"","text":"Ingress在Kubernetes中，服务和Pod的IP地址仅可以在集群网络内部使用，对于集群外的应用是不可见的。为了使外部的应用能够访问集群内的服务，在Kubernetes中目前提供了以下几种方案： NodePort LoadBalancer Ingress NodePort：简单的来说就是通过Service资源对象，为后端的Pod提供一个统一的访问入口，然后将Service的统一访问接口映射到集群节点上，最终实现client通过映射到集群节点上的端口访问到后端Pod提供的服务 但是，这种方法有个弊端，就是当新生成一个pod服务就需要创建对应的service将其映射到节点端口，当运行的pod过多时，我们节点暴露给client端的端口也会随之增加，这样我们整个k8s群集的危险系数就会增加，因为我们在搭建群集之处，官方明确指出，必须关闭firewalld防火墙及清空iptables规则，现在我们又暴露了那么多端口给client，安全系数可想而知 Ingress就解决了这个弊端： 简单的理解：原先暴露的service，现在给定一个统一的访问入口 Ingress资源对象的组成： Ingress-nginx-controller： 将新加入的Ingress转化为反向代理服务器的配置文件，并使之生效（动态的感知k8s集群内Ingress资源的变化，通过lua脚本实现） Ingress： 将反向代理服务器抽象成一个Ingress对象，每添加一个新的服务，只需要写一个新的Ingress的yaml文件即可，或修改已经存在的Ingress规则的yaml 在k8s集群前边部署一个反向代理服务器，这个服务器代理着k8s集群内部的service资源 Ingress-nginx可以解决什么问题： 动态的配置服务 ​ 如果按照传统方式, 当新增加一个服务时, 我们可能需要在流量入口加一个反向代理指向我们新的k8s服务. 而如果用了Ingress-nginx, 只需要配置好这个服务, 当服务启动时, 会自动注册到Ingress的中, 不需要而外的操作 减少不必要的端口暴露 ​ 配置过k8s的都清楚, 第一步是要关闭防火墙的, 主要原因是k8s的很多服务会以NodePort方式映射出去, 这样就相当于给宿主机打了很多孔, 既不安全也不优雅. 而Ingress可以避免这个问题, 除了Ingress自身服务可能需要映射出去, 其他服务都不要用NodePort方式 Ingress-nginx工作原理： 1）Ingress controller通过和kubernetes api交互，动态的去感知集群中Ingress规则变化，2）然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置，3）再写到nginx-ingress-controller的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中，4）然后reload一下使配置生效。以此达到域名分别配置和动态更新的问题 基于Nginx的Ingress controller根据不同的开发公司，又分为两种： k8s社区版的：Ingress-nginx nginx公司自己开发的：nginx-ingress Ingress-nginx配置实例：1）创建一个web服务，用deployment资源，用httpd奖项，然后创建一个service资源与之关联 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@master ~]# vim deploy_1.yamlapiVersion: v1kind: Namespacemetadata: name: bdqn-ns labels: name: bdqn-ns---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: httpd-deploy namespace: bdqn-nsspec: replicas: 2 template: metadata: labels: app: bdqn-ns spec: containers: - name: httpd image: httpd---apiVersion: v1kind: Servicemetadata: name: httpd-svc namespace: bdqn-nsspec: type: NodePort selector: app: bdqn-ns ports: - name: httpd-port port: 80 targetPort: 80 nodePort: 31033[root@master ~]# kubectl apply -f deploy_1.yaml namespace/bdqn-ns createddeployment.extensions/httpd-deploy createdservice/httpd-svc created 1234567891011121314151617181920[root@master ~]# kubectl get svc -n bdqn-ns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEhttpd-svc NodePort 10.97.86.190 &lt;none&gt; 80:31033/TCP 3m31s[root@master ~]# kubectl get pod -n bdqn-ns NAME READY STATUS RESTARTS AGEhttpd-deploy-966699d76-25wkn 1/1 Running 0 3m33shttpd-deploy-966699d76-6cdwf 1/1 Running 0 3m34s[root@master ~]# kubectl get deployments. -n bdqn-ns NAME READY UP-TO-DATE AVAILABLE AGEhttpd-deploy 2/2 2 2 3m37s[root@master ~]# kubectl describe svc -n bdqn-nsSelector: app=bdqn-nsType: NodePortIP: 10.97.86.190Port: httpd-port 80/TCPTargetPort: 80/TCPNodePort: httpd-port 31033/TCPEndpoints: 10.244.1.2:80,10.244.2.2:80Session Affinity: NoneExternal Traffic Policy: Cluster 2）创建一个web服务，用deployment资源，用tomcat镜像，然后创建一个service资源与之关联 1234567891011121314151617181920212223242526272829303132333435[root@master ~]# vim deploy_2.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: tomcat-deploy namespace: bdqn-nsspec: replicas: 2 template: metadata: labels: app: bdqn-tomcat spec: containers: - name: tomcat image: tomcat:8.5.45---apiVersion: v1kind: Servicemetadata: name: tomcat-svc namespace: bdqn-nsspec: type: NodePort selector: app: bdqn-tomcat ports: - name: tomcat-port port: 8080 targetPort: 8080 nodePort: 32033[root@master ~]# kubectl apply -f deploy_2.yaml deployment.extensions/tomcat-deploy createdservice/tomcat-svc created 1234567891011121314[root@master ~]# kubectl get deployments. -n bdqn-ns NAME READY UP-TO-DATE AVAILABLE AGEhttpd-deploy 2/2 2 2 9m58stomcat-deploy 2/2 2 2 58s[root@master ~]# kubectl get pod -n bdqn-ns NAME READY STATUS RESTARTS AGEhttpd-deploy-966699d76-25wkn 1/1 Running 0 10mhttpd-deploy-966699d76-6cdwf 1/1 Running 0 10mtomcat-deploy-759dc8c885-9wgqw 1/1 Running 0 70stomcat-deploy-759dc8c885-9xmhj 1/1 Running 0 70s[root@master ~]# kubectl get svc -n bdqn-ns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEhttpd-svc NodePort 10.97.86.190 &lt;none&gt; 80:31033/TCP 10mtomcat-svc NodePort 10.98.122.36 &lt;none&gt; 8080:32033/TCP 75s 3）创建Ingress -nginx-controller 12345678910[root@master ~]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.29.0/deploy/static/mandatory.yaml//将yaml文件下载下来在yaml文件中添加：hostNetwork: true spec: //在212行的spec字段下添加 hostNetwork: true //添加这行就行 # wait up to five minutes for the drain of connections[root@master ~]# kubectl apply -f mandatory.yaml[root@master ~]# kubectl get pod -n ingress-nginx NAME READY STATUS RESTARTS AGEnginx-ingress-controller-5954d475b6-xtzbc 1/1 Running 0 16m hostNetwork: true 在deployment资源中，如果添加了此字段，意味着Pod中运行的应用可以直接使用node节点的端口，这样node节点主机所在网络的其他主机，就可以通过访问该端口访问此应用。（类似于docker映射到宿主机的端口） 4）创建Ingress资源：（定义Ingress规则） 12345678910111213141516171819202122232425262728293031[root@master ~]# vim ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: bdqn-ingress namespace: bdqn-ns annotations: nginx.ingress.kubernetes.io/rewrite-target: / #这个千万不要写错，不然后面无法访问spec: rules: - host: ingress.bdqn.com http: paths: - path: / backend: serviceName: httpd-svc servicePort: 80 - path: /tomcat backend: serviceName: tomcat-svc servicePort: 8080[root@master ~]# kubectl apply -f ingress.yaml ingress.extensions/bdqn-ingress created[root@master ~]# kubectl describe ingresses. -n bdqn-ns bdqn-ingressRules: Host Path Backends ---- ---- -------- ingress.bdqn.com / httpd-svc:80 (10.244.1.5:80,10.244.2.4:80) /tomcat httpd-tomcat:8080 (10.244.1.4:8080,10.244.2.5:8080)//如果没有这个信息说明Ingress创建的有问题 1234567891011121314151617[root@master ~]# kubectl exec -it -n ingress-nginx nginx-ingress-controller-5954d475b6-wkqr2 sh/etc/nginx $ cat nginx.conf//没创建Ingress之前这些值都是空的，这就是动态的感知，然后写入配置文件 location / &#123; set $namespace \"bdqn-ns\"; set $ingress_name \"bdqn-ingress\"; set $service_name \"httpd-svc\"; set $service_port \"80\"; set $location_path \"/\"; location ~* \"^/tomcat\" &#123; set $namespace \"bdqn-ns\"; set $ingress_name \"bdqn-ingress\"; set $service_name \"tomcat-svc\"; set $service_port \"8080\"; set $location_path \"/tomcat\"; 因为域名是自定义的，所以要配置域名解析，修改windows的host文件，将IP与域名绑定 1234567//查看Ingress-controller运行在哪个节点，IP 是 ingress-controller Pod运行所在的节点[root@master ~]# kubectl get pod -n ingress-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-ingress-controller-5954d475b6-wkqr2 1/1 Running 1 44h 192.168.1.50 node01 &lt;none&gt; &lt;none&gt;//找到host文件，进行修改C:\\Windows\\System32\\drivers\\etc192.168.1.50 ingress.bdqn.com 现在已经达到了我们想要的功能，现在可以通过ingress.bdqn.com访问httpd服务，通过ingress.bdqn.com/tomcat访问tomcat服务 在上面的访问测试中，虽然访问到了对应的服务，但是有一个弊端，就是在做DNS解析的时候，只能指定Ingress-nginx容器所在的节点IP。而指定k8s集群内部的其他节点IP（包括master）都是不可以访问到的，如果这个节点一旦宕机，Ingress-nginx容器被转移到其他节点上运行（不考虑节点标签的问题，其实保持Ingress-nginx的yaml文件中默认的标签的话，那么每个节点都是有那个标签的）。随之还要我们手动去更改DNS解析的IP（要更改为Ingress-nginx容器所在节点的IP，通过命令“kubectl get pod -n ingress-nginx -o wide”可以查看到其所在节点），很是麻烦 所以就要为ingress资源对象创建一个Service（NodePort），这样在配置DNS解析的时候，就可以通过Ingress.bdqn.com 所有node节点，包括master节点的IP来配置，很方便 5）创建service资源： 12345[root@master ~]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.29.0/deploy/static/provider/baremetal/service-nodeport.yaml[root@master ~]# kubectl apply -f service-nodeport.yaml[root@master ~]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.100.167.12 &lt;none&gt; 80:32756/TCP,443:30501/TCP 2m7s Service-Nodeport 因为Ingress-nginx-controller运行在了集群内的其中一个节点，为了保证即使这个节点宕机，我们对应的域名仍然能够正常的访问服务，所以我们将Ingress-nginx-controller也暴露为一个service资源 至此，这个域名就可以和集群中任意节点的 32756/30501端口进行绑定了","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"ConfigMap、Secret","slug":"ConfigMap、Secret","date":"2020-02-16T16:00:00.000Z","updated":"2020-02-17T07:16:38.374Z","comments":true,"path":"ConfigMap、Secret.html","link":"","permalink":"http://pdxblog.top/ConfigMap%E3%80%81Secret.html","excerpt":"","text":"ConfigMap、Secret为什么有这两个东西： 我们在kubernetes上部署应用的时候，经常会需要传一些配置给我们的应用，比如数据库地址啊，用户名密码啊之类的。我们要做到这个，有好多种方案，比如： 我们可以直接在打包镜像的时候写在应用配置文件里面，但是这种方式的坏处显而易见而且非常明显。 我们可以在配置文件里面通过env环境变量传入，但是这样的话我们要修改env就必须去修改yaml文件，而且需要重启所有的container才行。 我们可以在应用启动的时候去数据库或者某个特定的地方拿，没问题！但是第一，实现起来麻烦；第二，如果配置的地方变了怎么办？ 当然还有别的方案，但是各种方案都有各自的问题。 而且，还有一个问题就是，如果说我的一个配置，是要多个应用一起使用的，以上除了第三种方案，都没办法进行配置的共享，就是说我如果要改配置的话，那得一个一个手动改。假如我们有100个应用，就得改100份配置，以此类推…… kubernetes对这个问题提供了一个很好的解决方案，就是用ConfigMap和Secret 应用场景： 镜像往往是一个应用的基础，还有很多需要自定义的参数或配置，例如资源的消耗、日志的位置级别等等，这些配置可能会有很多，因此不能放入镜像中，Kubernetes中提供了Configmap来实现向容器中提供配置文件或环境变量来实现不同配置，从而实现了镜像配置与镜像本身解耦，使容器应用做到不依赖于环境配置 Secret资源对象： 可以保存轻量的敏感信息，比如数据库的用户名和密码或者认证秘钥等。它保存的数据是以秘文的方式存放的 configMap资源对象： 和Secret一样，拥有大多数共同的特性，但是区别是，configMap保存的是一些不太重要的信息，它保存的数据是以明文的方式存放的。 当我们创建上述两种资源对象时，其实就是将这两种资源对象存储的信息写入了k8s群集中的etcd数据中心 Secret与ConfigMap的异同： 相同之处： 都是用来保存轻量级信息的，可以供其他资源对象（Deployment、RC、RS和Pod）进行挂载使用 这两种资源对象的创建方法（4种）及引用方法（2种）都是一样的，都是以键值对的方式进行存储的 不同之处： Secret是用来保存敏感信息的，而configMap是用来保存一些不太重要的数据的，具体表现在当我们执行“kubectl describe ….”命令时，Secret这种类型的资源对象时查看不到其具体的信息的，而configMap是可以查看到其保存的具体内容的 Secret:Secret：用于保存一些敏感信息，比如数据库的用户名密码或者密钥。这些数据是比较少量的，将这些信息放在 secret中比放在 pod 的定义或者 docker 镜像中来说更加安全和灵活 用户可以创建自己的secret，系统也会有自己的secret 内置 secret 1[root@master ~]# kubectl get secrets -n kube-system Secret有三种类型： Opaque：base64编码格式的Secret，用来存储密码、密钥等；但数据也通过base64 –decode解码得到原始数据，所有加密性很弱 kubernetes.io/dockerconfigjson：用来存储私有docker registry的认证信息 kubernetes.io/service-account-token： 用于被serviceaccount引用，serviceaccout创建时Kubernetes会默认创建对应的secret。Pod如果使用了serviceaccount，对应的secret会自动挂载到Pod目录/run/secrets/ kubernetes.io/serviceaccount中 举例：保存数据库的用户名和密码 用户名：root 密码：123.com 有四种方法： 1、通过- -from-literal（文字的）： 也就是说需要保存什么，直接写出来就行 注意：每一个–from-literal只能保存一条信息 1[root@master ~]# kubectl create secret generic mysecret1 --from-literal=username=root --from-literal=password=123.com generic：通用的、一般的加密方式 1234[root@master ~]# kubectl get secrets NAME TYPE DATA AGEdefault-token-x9ptl kubernetes.io/service-account-token 3 42dmysecret1 Opaque（不透明的） 2 2m13s Opaque（不透明的）：也就是说你看不到 12345678[root@master ~]# kubectl describe secretsType: OpaqueData====password: 7 bytesusername: 4 bytes//这里看不到真正的值是什么 2、- -from-file（文件）： 同样，每一个只能保存一条信息 12345678[root@master ~]# echo root &gt; username[root@master ~]# echo 123.com &gt; password[root@master ~]# kubectl create secret generic mysecret2 --from-file=username --from-file=password [root@master ~]# kubectl get secrets NAME TYPE DATA AGEdefault-token-x9ptl kubernetes.io/service-account-token 3 42dmysecret1 Opaque 2 28mmysecret2 Opaque 2 28s 既然是通过文件创建的，那么把文件删除，这个secret是否还在 1234567[root@master ~]# rm -rf username password [root@master ~]# kubectl get secrets NAME TYPE DATA AGEdefault-token-x9ptl kubernetes.io/service-account-token 3 42dmysecret1 Opaque 2 29mmysecret2 Opaque 2 92s//它确实还会存在 3、通过- -from-env-file： 这种方法可以把用户名和密码写在一个文件里面，这样就比前两种方便 123456789101112131415[root@master ~]# vim env.txtusername=rootpassword=123.com[root@master ~]# kubectl create secret generic mysecret3 --from-env-file=env.txt[root@master ~]# kubectl get secrets NAME TYPE DATA AGEdefault-token-x9ptl kubernetes.io/service-account-token 3 42dmysecret1 Opaque 2 34mmysecret2 Opaque 2 6m27smysecret3 Opaque 2 16s[root@master ~]# kubectl describe secretsData====password: 7 bytesusername: 4 bytes 4、通过yaml配置文件 先看一下yaml怎么写 1234567891011121314[root@master ~]# kubectl get secrets mysecret1 -o yamlapiVersion: v1data: password: MTIzLmNvbQ== username: cm9vdA==kind: Secretmetadata: creationTimestamp: \"2020-02-14T02:01:34Z\" name: mysecret1 namespace: default resourceVersion: \"13766\" selfLink: /api/v1/namespaces/default/secrets/mysecret1 uid: ffce8c7a-2dfc-4958-9e3b-0fcb50d00ccatype: Opaque 可以看到数据是被加密后写入yaml文件里的，所以我们写的时候不能直接写数据，而是要加密一下 把保存的数据加密： 通过base64方式： 1234[root@master ~]# echo root | base64cm9vdAo=[root@master ~]# echo 123.com | base64MTIzLmNvbQo= 创建secret资源对象： 12345678910111213141516[root@master ~]# vim secret4.yamlapiVersion: v1kind: Secretmetadata: name: mysecret4data: username: cm9vdAo= password: MTIzLmNvbQo=[root@master ~]# kubectl apply -f secret4.yaml[root@master ~]# kubectl get secrets NAME TYPE DATA AGEdefault-token-x9ptl kubernetes.io/service-account-token 3 42dmysecret1 Opaque 2 42mmysecret2 Opaque 2 14mmysecret3 Opaque 2 8mmysecret4 Opaque 2 18s 这种方法虽然说我们看不到真正的数据是什么，但是这种方式也是不安全的，每一次编码后的数据是一样的，是由规律的，同样它是可以被解码的 解码： 12[root@master ~]# echo -n cm9vdAo | base64 --decoderoot 如何来使用Secret资源： Secret 可以作为数据卷被挂载，或作为环境变量 暴露出来以供 pod 中的容器使用。它们也可以被系统的其他部分使用，而不直接暴露在 pod 内 两种方法： 1、以Volume挂载的方式 1234567891011121314151617181920212223242526//创建Pod来引用secret[root@master ~]# vim pod.yamlapiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 volumeMounts: - name: secret-test mountPath: \"/etc/secret-test\" readOnly: true //是否只读，也就是说对于/etc/secret-test只有只读的权限，不能修改 volumes: - name: secret-test secret: secretName: mysecret1[root@master ~]# kubectl apply -f pod.yaml[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEmypod 1/1 Running 0 2m4s 进入容器查看是否有我们保存的数据 123456789[root@master ~]# kubectl exec -it mypod /bin/sh/ # cd /etc/secret-test/etc/secret-test # cat usernameroot/etc/secret-test # cat password123.com/etc/secret-test # echo admin &gt; username/bin/sh: can't create username: Read-only file system//这个文件也是不能修改的，因为是只读文件 还可以自定义存放数据的文件名： 1234567891011121314151617181920212223//在volumes字段下追加items字段： volumes: - name: secret-test secret: secretName: mysecret1 items: - key: username path: my-group/my-username - key: password path: my-group/my-password[root@master ~]# kubectl apply -f pod.yaml pod/mypod created[root@master ~]# kubectl exec -it mypod /bin/sh/ # cd /etc/secret-test//etc/secret-test # lsmy-group/etc/secret-test # cd my-group//etc/secret-test/..2020_02_17_01_40_09.035305611/my-group # lsmy-username my-password/etc/secret-test/..2020_02_17_01_40_09.035305611/my-group # cat my-username root/etc/secret-test/..2020_02_17_01_40_09.035305611/my-group # cat my-password 123.com 2、以环境变量的方式 123456789101112131415161718192021222324252627282930[root@master ~]# cp pod.yaml pod-env.yaml [root@master ~]# vim pod-env.yamlapiVersion: v1kind: Podmetadata: name: mypod2spec: containers: - name: mypod2 image: busybox args: - /bin/sh - -c - sleep 300000 env: - name: SECRET_USERNAME valueFrom: secretKeyRef: //翻译过来就是机密键引用，提取mysecret2里面的数据到SECRET_USERNAME name: mysecret2 key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: //与上面同理 name: mysecret2 key: password[root@master ~]# kubectl apply -f pod-env.yaml[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEmypod 1/1 Running 0 18mmypod2 1/1 Running 0 79s 同样，进入pod查看 12345[root@master ~]# kubectl exec -it mypod2 /bin/sh/ # echo $SECRET_USERNAMEroot/ # echo $SECRET_PASSWORD123.com 如果现在将secret资源内保存的数据进行更新，使用此数据的应用内，数据是否也会更新 更新mysecret1的数据：把password—–&gt;123.com—-&gt;admin 12345678910111213[root@master ~]# echo admin | base64YWRtaW4K[root@master ~]# kubectl get secrets zhbsecret2 -o yaml//可以通过edit命令直接修改：[root@master ~]# kubectl edit secrets zhbsecret2data: username: cm9vdAo= password: YWRtaW4K //更改[root@master ~]# kubectl exec -it mypod /bin/sh/ # cd /etc/secret-test/my-group//etc/secret-test/..2020_02_17_01_57_25.223684048/my-group # cat my-password admin//可以看到已经跟着改变了 注意： ​ 这里引用数据是以volumes挂载使用数据的方式，才会实时更新 那么，以环境变量的方式引用的数据，是否会实时更新？ 12345678[root@master ~]# kubectl edit secrets mysecret4data: username: cm9vdAo= password: YWRtaW4K[root@master ~]# kubectl exec -it mypod2 /bin/sh/ # echo $SECRET_PASSWORD123.com//没有变化 总结： ​ 如果引用secret数据的应用，要求会随着secret资源对象内保存的数据的更新而实时更新，那么应该使用volumes挂载的方式引用资源。因为用环境变量的方式引用不会实时更新数据 ConfigMap: 和Secret资源类似，不同之处在于，secret资源保存的是敏感信息，而configmap保存的方式是以明文的方式存放的数据 什么是ConfigMap： ConfigMap对像是一系列配置的集合，k8s会将这一集合注入到对应的Pod对像中，并为容器成功启动使用。注入的方式一般有两种，一种是挂载存储卷，一种是传递变量。ConfigMap被引用之前必须存在，属于名称空间级别，不能跨名称空间使用，内容明文显示。ConfigMap内容修改后，对应的pod必须重启或者重新加载配置 创建ConfigMap的4种方式： username：adam age：18 和secretc创建的方式一模一样 1、通过- -from-literal（文字的）： 1234567891011121314[root@master ~]# kubectl create configmap myconfigmap1 --from-literal=username=adam --from-literal=age=18configmap/myconfigmap1 created[root@master ~]# kubectl get configmaps NAME DATA AGEmyconfigmap1 2 32s[root@master ~]# kubectl describe configmapsData====age:----18username:----adam 2、- -from-file（文件）： 12345678[root@master ~]# touch adam &gt; username[root@master ~]# touch 18 &gt; age[root@master ~]# kubectl create configmap myconfigmap2 --from-file=username --from-file=age configmap/myconfigmap2 created[root@master ~]# kubectl get configmaps NAME DATA AGEmyconfigmap1 2 4m23smyconfigmap2 2 63s 3、通过- -from-env-file： 12345678910[root@master ~]# vim env.txtusername&#x3D;adamage&#x3D;18[root@master ~]# kubectl create configmap myconfigmap3 --from-env-file&#x3D;env.txt configmap&#x2F;myconfigmap3 created[root@master ~]# kubectl get configmaps NAME DATA AGEmyconfigmap1 2 16mmyconfigmap2 2 5m18smyconfigmap3 2 8m56s 4、通过yaml配置文件 12345678910111213141516[root@master ~]# vim configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: myconfigmap4data: username: adam age: \"18\"[root@master ~]# kubectl apply -f configmap.yaml configmap/myconfigmap4 created[root@master ~]# kubectl get configmaps NAME DATA AGEmyconfigmap1 2 16mmyconfigmap2 2 5m18smyconfigmap3 2 8m56smyconfigmap4 2 4m8s 使用configmap：和secret一样，有两种方法 第一种方法是： 以volumes挂载的方式引用资源 1234567891011121314151617181920212223[root@master ~]# vim v-pod.yamlapiVersion: v1kind: Podmetadata: name: pod1spec: containers: - name: pod1 image: busybox args: - bin/sh - -c - sleep 300000 volumeMounts: - name: cmp-test mountPath: \"/etc/cmp-test\" readOnly: true volumes: - name: cmp-test configMap: name: myconfigmap1[root@master ~]# kubectl apply -f v-pod.yaml pod/pod1 created 第二种方式： 以环境变量的方式引用资源 123456789101112131415161718192021222324252627282930[root@master ~]# vim e-pod.yaml apiVersion: v1kind: Podmetadata: name: pod2spec: containers: - name: pod2 image: busybox args: - bin/sh - -c - sleep 300000 env: - name: CONFIGMAP_NAME valueFrom: configMapKeyRef: name: myconfigmap2 key: username - name: CONFIGMAP_AGE valueFrom: configMapKeyRef: name: myconfigmap2 key: age[root@master ~]# kubectl apply -f e-pod.yaml pod/pod2 created[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEpod1 1/1 Running 0 5m34spod2 1/1 Running 0 48s 如果现在将confgimap资源内保存的数据进行更新，使用此数据的应用内，数据是否也会更新 123456789[root@master ~]# kubectl edit configmaps myconfigmap1data: age: \"18\" username: root[root@master ~]# kubectl exec -it pod1 /bin/sh/ # cd /etc/cmp-test//etc/cmp-test # cat username root//这里会跟着更新，如果操作过快，它会反应不过来，你就话看不到改变，稍微等一下就行了 那么，以环境变量的方式引用的数据，是否会实时更新？ 1234567891011121314151617181920212223242526[root@master ~]# kubectl exec -it pod2 /bin/sh/ # echo $CONFIGMAP_AGE18/ # echo $CONFIGMAP_NAMEadam/ # exit[root@master ~]# kubectl edit configmaps myconfigmap2data: age: | 18 username: | root[root@master ~]# kubectl describe configmaps myconfigmap2Data====age:----18username:----root[root@master ~]# kubectl exec -it pod2 /bin/sh/ # echo $CONFIGMAP_NAMEadam//和secret一样，是不会更新的 小结Secret：用于存放一些敏感信息，比如数据库的用户名密码、密钥等，以密文的方式保存 创建Secret资源对象的四种方式： –from-literal（文字的）：需要保存什么内容直接写出来，一次只能保存一条 –from-file（文件）：把需要保存的内容写到文件里面，通过–from-file指定这个文件。一次只能保存一条 –from-env-file（环境变量）：把想要保存的内容都写入一个文件里面，通过–from-env-file指定 通过yaml配置文件：在data字段写入要保存的内容，注意是以密文的格式写入（使用base64的方式加密就行） 引用Secret资源的两种方法： Volumes挂载的方式： 环境变量的方式： ConfigMap：和Secret一样，拥有大多数共同的特性，但是区别是，configMap保存的是一些不太重要的信息，它保存的数据是以明文的方式存放的，使用describe来查看是，能看到真正的信息 创建ConfigMap资源对象的四种方式和引用ConfigMap的两种方式一摸一样 –from-literal（文字的） –from-file（文件） –from-env-file（环境变量） 通过yaml配置文件 Volumes挂载的方式： 环境变量的方式： 如果引用secret、CongigMap数据的应用，要求会随着secret、ConfigMap资源对象内保存的数据的更新而实时更新，那么应该使用volumes挂载的方式引用资源。因为用环境变量的方式引用不会实时更新数据","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"StatefulSet","slug":"StatefulSet","date":"2020-02-11T16:00:00.000Z","updated":"2020-02-14T03:25:48.428Z","comments":true,"path":"StatefulSet.html","link":"","permalink":"http://pdxblog.top/StatefulSet.html","excerpt":"","text":"StatefulSetRC、RS、Deployment、DS（DaemonSet）这些Pod控制器都是面向无状态的服务，它们所管理的Pod的IP、名字、启停顺序等都是随机的 这些Pod控制器都有一个相同点 ​ template（模板）：根据模板创建出来的Pod，它们的状态都是一摸一样的（除了名称、IP、域名之外） ​ 可以理解为：任何一个Pod都可以被删除，然后用新生成的Pod进行替换 StatefulSet： 顾名思义：有状态的集合，管理所有有状态的服务，比如MySQL、MongoDB集群等 它之前的名字是：PetSet Pet：宠物 把之前按无状态的服务比喻为牛、羊等牲畜。把有状态的服务比喻为：宠物 StatefulSet本质上是Deployment的一种变体，在v1.9版本中已成为GA版本，它为了解决有状态服务的问题，它所管理的Pod拥有固定的Pod名称，启停顺序，在StatefulSet中，Pod名字称为网络标识(hostname)，还必须要用到共享存储 有状态的服务：后端生成的每一个Pod都具有自己的唯一性，不可随意被删除 需要记录前一次或者多次通信中的相关事件，以作为下一次通信的分类标准。比如：mysql等数据库服务。（Pod的名称不能随意变化，数据持久化的目录也是不一样的，每一个Pod都有自己独有的数据持久化存储目录） 一个小实例： 12345678910111213141516171819202122232425262728293031323334353637383940[root@master ~]# vim statefulset.yamlapiVersion: v1kind: Servicemetadata: name: headless-svc labels: app: headless-svcspec: ports: - port: 80 selector: app: headless-pod clusterIP: None---apiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-testspec: serviceName: headless-svc replicas: 3 selector: matchLabels: app: headless-pod template: metadata: labels: app: headless-pod spec: containers: - name: myhttpd image: httpd ports: - containerPort: 80[root@master ~]# kubectl apply -f statefulset.yaml service/headless-svc createdstatefulset.apps/statefulset-test created[root@master ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEheadless-svc ClusterIP None &lt;none&gt; 80/TCP 4m Deployment控制的pod的名称由来： ​ Deployment+RS+随机字符串（Pod的名称），没有顺序的额，可以被随意替代 StategulSet的三个组成部分： 1、headless-svc：无头服务。因为没有IP地址，所以它不具备负载均衡的功能了 作用：为后端的每一个Pod去命名 因为statefulset要求Pod的名称是有顺序的，每一个Pod都不能被随意取代，也就是说即使Pod重建之后，名称依然不变 12345[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEstatefulset-test-0 1/1 Running 0 23mstatefulset-test-1 1/1 Running 0 22mstatefulset-test-2 1/1 Running 0 22m 1234567[root@master ~]# kubectl delete pod statefulset-test-0[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEstatefulset-test-0 0/1 ContainerCreating 0 6sstatefulset-test-1 1/1 Running 0 25mstatefulset-test-2 1/1 Running 0 25m//Pod重建之后名称没有发生变化 2、statefulset：定义具体的应用 3、volumeClaimTeplates：自动创建PVC，为后端的Pod提供专有的存储 存储卷申请模板，创建PVC，指定pvc名称大小，将自动创建pvc，且pvc必须由存储类供应 为什么需要 headless service 无头服务？在用Deployment时，每一个Pod名称是没有顺序的，是随机字符串，因此是Pod名称是无序的，但是在statefulset中要求必须是有序 ，每一个pod不能被随意取代，pod重建后pod名称还是一样的。而pod IP是变化的，所以是以Pod名称来识别。pod名称是pod唯一性的标识符，必须持久稳定有效。这时候要用到无头服务，它可以给每个Pod一个唯一的名称 。为什么需要volumeClaimTemplate？对于有状态的副本集都会用到持久存储，对于分布式系统来讲，它的最大特点是数据是不一样的，所以各个节点不能使用同一存储卷，每个节点有自已的专用存储，但是如果在Deployment中的Pod template里定义的存储卷，是所有副本集共用一个存储卷，数据是相同的，因为是基于模板来的 ，而statefulset中每个Pod都要自已的专有存储卷，所以statefulset的存储卷就不能再用Pod模板来创建了，于是statefulSet使用volumeClaimTemplate，称为卷申请模板，它会为每个Pod生成不同的pvc，并绑定pv， 从而实现各pod有专用存储。这就是为什么要用volumeClaimTemplate的原因 每一个pod—&gt;对应一个pvc—-&gt;每一个pvc对应一个pv ​ storageclass：自动创建PV ​ 需要解决：自动创建PVC—–&gt;volumeClaimTeplates 一、创建StorageClass资源对象 ​ 1、基于NFS服务，创建NFS服务 123[root@master ~]# showmount -eExport list for master:/nfsdata * ​ 2、创建rbac权限 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@master ~]# vim rbac-rolebind.yamlapiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runnerrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: default //这个字段必须要写，不然会报错roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io[root@master ~]# kubectl apply -f rbac-rolebind.yaml serviceaccount/nfs-provisioner unchangedclusterrole.rbac.authorization.k8s.io/nfs-provisioner-runner unchangedclusterrolebinding.rbac.authorization.k8s.io/run-nfs-provisioner created ​ 3、创建Deployment资源对象，用Pod代替真正的NFS服务 1234567891011121314151617181920212223242526272829303132333435363738[root@master ~]# vim nfs-deployment.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: bdqn - name: NFS_SERVER value: 192.168.1.70 - name: NFS_PATH value: /nfsdata volumes: - name: nfs-client-root nfs: server: 192.168.1.70 path: /nfsdata[root@master ~]# kubectl apply -f nfs-deployment.yaml deployment.extensions/nfs-client-provisioner created[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEnfs-client-provisioner-f6cb6688b-5zn8d 1/1 Running 0 7s ​ 4、创建storaclass 123456789101112[root@master ~]# vim test-storageclass.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: sc-nfsprovisioner: bdqnreclaimPolicy: Retain[root@master ~]# kubectl apply -f test-storageclass.yaml storageclass.storage.k8s.io/sc-nfs created[root@master ~]# kubectl get scNAME PROVISIONER AGEsc-nfs bdqn 10s 二、解决自动创建PVC 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@master ~]# vim statefulset.yamlapiVersion: v1kind: Servicemetadata: name: headless-svc labels: app: headless-svcspec: ports: - port: 80 name: myweb selector: app: headless-pod clusterIP: None---apiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-testspec: serviceName: headless-svc replicas: 3 selector: matchLabels: app: headless-pod template: metadata: labels: app: headless-pod spec: containers: - image: httpd name: myhttpd ports: - containerPort: 80 name: httpd volumeMounts: - mountPath: /mnt name: test volumeClaimTemplates: //自动的创建PVC - metadata: name: test annotations: //这是指定storageclass volume.beta.kubernetes.io/storage-class: sc-nfs spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi[root@master ~]# kubectl apply -f statefulset.yaml service/headless-svc createdstatefulset.apps/statefulset-test created[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEnfs-client-provisioner-f6cb6688b-5zn8d 1/1 Running 0 13mstatefulset-test-0 1/1 Running 0 23sstatefulset-test-1 1/1 Running 0 16sstatefulset-test-2 1/1 Running 0 9s 注意： ​ 如果生成的Pod，第一个出现了问题，后面的都不会生成 根据volumeClaimTemplates自动创建的PVC： 12345[root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-statefulset-test-0 Bound pvc-3a105a9d-5892-4080-a993-20fd2540cd3e 100Mi RWO sc-nfs 46mtest-statefulset-test-1 Bound pvc-123bf53d-a72b-4bfa-a901-bb98efcda056 100Mi RWO sc-nfs 45mtest-statefulset-test-2 Bound pvc-8529c668-5b38-4024-93af-b18fa238b0ba 100Mi RWO sc-nfs 45m 如果集群中没有StorageClass的动态供应PVC的机制，也可以提前手动创建多个PV、PVC，手动创建的PVC名称必须符合之后创建的StatefulSet命名规则：(volumeClaimTemplates.name)-(pod_name) Statefulset名称为statefulset-test 三个Pod副本: statefulset-test-0，statefulset-test-1，statefulset-test-2 volumeClaimTemplates名称为：test 那么自动创建出来的PVC名称为test-statefulset-test-[0-2]，为每个Pod创建一个PVC 规律总结： 匹配Pod name(网络标识)的模式为：$(statefulset名称)-$(序号)，比如上面的示例：statefulset-test-0，statefulset-test-1，statefulset-test-2。 StatefulSet为每个Pod副本创建了一个DNS域名，这个域名的格式为： $(podname).(headless server name)，也就意味着服务间是通过Pod域名来通信而非Pod IP，因为当Pod所在Node发生故障时，Pod会被飘移到其它Node上，Pod IP会发生变化，但是Pod域名不会有变化。 StatefulSet使用Headless服务来控制Pod的域名，这个域名的FQDN为：$(service name).$(namespace).svc.cluster.local，其中，“cluster.local”指的是集群的域名。 根据volumeClaimTemplates，为每个Pod创建一个pvc，pvc的命名规则匹配模式：(volumeClaimTemplates.name)-(pod_name)，比如上面的volumeMounts.name=test， Pod name=statefulset-test-[0-2]，因此创建出来的PVC是test-statefulset-test-0，test-statefulset-test-1，test-statefulset-test-2 删除Pod不会删除其pvc，手动删除pvc将自动释放pv。关于Cluster Domain、headless service名称、StatefulSet 名称如何影响StatefulSet的Pod的 StatefulSet的启停顺序： 有序部署：部署StatefulSet时，如果有多个Pod副本，它们会被顺序地创建（从0到N-1）并且，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态。 有序删除：当Pod被删除时，它们被终止的顺序是从N-1到0。 有序扩展：当对Pod执行扩展操作时，与部署一样，它前面的Pod必须都处于Running和Ready状态 StatefulSet Pod管理策略： 在v1.7以后，通过允许修改Pod排序策略，同时通过.spec.podManagementPolicy字段确保其身份的唯一性。 OrderedReady：上述的启停顺序，默认设置。 Parallel：告诉StatefulSet控制器并行启动或终止所有Pod，并且在启动或终止另一个Pod之前不等待前一个Pod变为Running and Ready或完全终止 StatefulSet使用场景： 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现 有序收缩，有序删除（即从N-1到0） 更新策略： 在Kubernetes 1.7及更高版本中，通过.spec.updateStrategy字段允许配置或禁用Pod、labels、source request/limits、annotations自动滚动更新功能。 OnDelete：通过.spec.updateStrategy.type 字段设置为OnDelete，StatefulSet控制器不会自动更新StatefulSet中的Pod。用户必须手动删除Pod，以使控制器创建新的Pod。 RollingUpdate：通过.spec.updateStrategy.type 字段设置为RollingUpdate，实现了Pod的自动滚动更新，如果.spec.updateStrategy未指定，则此为默认策略。StatefulSet控制器将删除并重新创建StatefulSet中的每个Pod。它将以Pod终止（从最大序数到最小序数）的顺序进行，一次更新每个Pod。在更新下一个Pod之前，必须等待这个Pod Running and Ready。 Partitions：通过指定 .spec.updateStrategy.rollingUpdate.partition 来对 RollingUpdate 更新策略进行分区，如果指定了分区，则当 StatefulSet 的 .spec.template 更新时，具有大于或等于分区序数的所有 Pod 将被更新。 具有小于分区的序数的所有 Pod 将不会被更新，即使删除它们也将被重新创建。如果 StatefulSet 的 .spec.updateStrategy.rollingUpdate.partition 大于其 .spec.replicas，则其 .spec.template 的更新将不会传播到 Pod。在大多数情况下，不需要使用分区 StatefulSet注意事项： 还在beta状态，需要kubernetes v1.5版本以上才支持 所有Pod的Volume必须使用PersistentVolume或者是管理员事先创建好 为了保证数据安全，删除StatefulSet时不会删除Volume StatefulSet需要一个Headless Service来定义DNS domain，需要在StatefulSet之前创建好 目前StatefulSet还没有feature complete，比如更新操作还需要手动patch 更多可以参考：https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/ 进入容器，验证持久化是否成功 123456789101112[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEnfs-client-provisioner-f6cb6688b-5zn8d 1/1 Running 0 30mstatefulset-test-0 1/1 Running 0 17mstatefulset-test-1 1/1 Running 0 16mstatefulset-test-2 1/1 Running 0 16m[root@master ~]# kubectl exec -it statefulset-test-0 /bin/sh# cd /mnt# touch testfile# exit[root@master ~]# ls /nfsdata/default-test-statefulset-test-0-pvc-3a105a9d-5892-4080-a993-20fd2540cd3e/testfile 小结Deployment、RC、RS、DS： 这些Pod控制器都是面向无状态的服务，他们管理的Pod的IP、名字、启停顺序等都是随机的 这些根据模板创建出来的Pod，特们的状态都是一摸一样的（除了名称、IP、域名之外） 任何一个Pod都可以被删除，然后用因生成的Pod进项替换 StatefulSet： 顾名思义：有状态的集合，管理所有的有状态服务，比如MySQL集群等 后端生成的每一个Pod都具有自己的唯一性，不可被随意删除 需要记录前一次或者多次通信中的相关事件，以作为下一次通信的分类标准 Pod的名称不能随意变化，数据持久化的目录也是不一样的，每一个Pod又都自己独有的数据持久化存储目录 扩容、缩容：在此过程中，Pod的生成或删除操作也是有顺序性的 12345678[root@master ~]# kubectl get pod -n zhbNAME READY STATUS RESTARTS AGEnfs-client-provisioner-f6cb6688b-x2zls 1/1 Running 1 42hstatefulset-test-0 1/1 Running 1 42hstatefulset-test-1 1/1 Running 1 42hstatefulset-test-2 1/1 Running 1 42hstatefulset-test-3 1/1 Running 0 76sstatefulset-test-4 1/1 Running 0 66s 升级操作： 1[root@master ~]# kubectl explain sts.spec.updateStrategy.rollingUpdate.partition partition：如果partition后面的值等于N，N+的都会更新，默认值为0（所有都会更新） 如果N等于2，那么它会从statefulset-test-2开始更新，以此类推 statefulset-test-0statefulset-test-1statefulset-test-2statefulset-test-3statefulset-test-4","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"k8s的存储类","slug":"k8s的存储类","date":"2020-02-10T16:00:00.000Z","updated":"2020-02-12T07:04:51.200Z","comments":true,"path":"k8s的存储类.html","link":"","permalink":"http://pdxblog.top/k8s%E7%9A%84%E5%AD%98%E5%82%A8%E7%B1%BB.html","excerpt":"","text":"k8s存储类如果，k8s集群中，有很多类似的PV，PVC在去向PV申请空间的时候，不仅会考虑名称以及访问控制模式，还会考虑你申请空间的大小，会分配给你最合适大小的PV 运行一个web服务，采用Deployment资源，基于nginx镜像。数据持久化目录为nginx服务的主访问目录：/usr/share/nginx/html 创建一个PVC，与上述资源进行关联 ​ 先创建两个PV：web-pv1（1G），web-pv2（2G） 123456789101112131415161718192021[root@master ~]# vim web1.yamlapiVersion: v1kind: PersistentVolumemetadata: name: web-pv1spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/web1 server: 192.168.1.70[root@master ~]# mkdir /nfsdata/web1[root@master ~]# kubectl apply -f web1.yaml persistentvolume/web-pv1 created[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEweb-pv1 1Gi RWO Recycle Available nfs 6s 1234567891011121314151617181920[root@master ~]# vim web2.yamlapiVersion: v1kind: PersistentVolumemetadata: name: web-pv2spec: capacity: storage: 2Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/web2 server: 192.168.1.70[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEweb-pv1 1Gi RWO Recycle Available nfs 97sweb-pv2 2Gi RWO Recycle Available nfs 6s[root@master ~]# mkdir /nfsdata/web2 123456789101112131415161718192021[root@master ~]# vim web.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: test-webspec: template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx volumeMounts: - name: test-web mountPath: /usr/share/nginx/html volumes: - name: test-web persistentVolumeClaim: claimName: web-pvc 123456789101112131415161718192021222324[root@master ~]# vim web-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: web-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs[root@master ~]# kubectl apply -f web-pvc.yaml[root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEweb-pvc Bound web-pv1 1Gi RWO nfs 6s[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEweb-pv1 1Gi RWO Recycle Bound default&#x2F;web-pvc nfs 9m8sweb-pv2 2Gi RWO Recycle Available nfs 7m37s[root@master ~]# kubectl apply -f web.yaml[root@master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-web-67989b6d78-2b774 1&#x2F;1 Running 0 2m1s 10.244.2.5 node02 &lt;none&gt; &lt;none&gt; 如果名称和访问模式都一样，它会考虑空间大小进行分配，分配比较接近的PV进行关联 12345678[root@master ~]# kubectl exec -it test-web-67989b6d78-2b774 /bin/bashroot@test-web-67989b6d78-2b774:/# cd /usr/share/nginx/html/root@test-web-67989b6d78-2b774:/usr/share/nginx/html# echo 12345 &gt; index.htmlroot@test-web-67989b6d78-2b774:/usr/share/nginx/html# exitexitcommand terminated with exit code 127[root@master ~]# curl 10.244.2.512345 很多的服务，很多的资源对象 ​ 如果要去创建服务，做数据持久化，需要预先知道可用PV有哪些？ ​ 如果为了这个服务去提前创建PV，那么我们还需要知道，这个服务大概需要多大的空间？ Storage Class（存储类）：它可以动态的自动的创建所需要的 PV PV是运维人员来创建的，开发操作PVC，可是大规模集群中可能会有很多PV，如果这些PV都需要运维手动来处理这也是一件很繁琐的事情，所以就有了动态供给概念，也就是Dynamic Provisioning。而我们上面的创建的PV都是静态供给方式，也就是Static Provisioning。而动态供给的关键就是StorageClass，它的作用就是创建PV模板。 创建StorageClass里面需要定义PV属性比如存储类型、大小等；另外创建这种PV需要用到存储插件。最终效果是，用户提交PVC，里面指定存储类型，如果符合我们定义的StorageClass，则会为其自动创建PV并进行绑定 存储类（Storage class）是k8s资源类型的一种，它是有管理员为管理PV更加方便创建的一个逻辑组，可以按照存储系统的性能高低，或者综合服务质量，备份策略等分类。不过k8s本身不知道类别到底是什么，它这是作为一个描述 Provisioner（供给方、提供者）： 及提供了存储资源的存储系统。k8s内建有多重供给方，这些供给方的名字都以“kubernetes.io”为前缀。并且还可以自定义 Parmeters（参数）： 存储类使用参数描述要关联到的存储卷，注意不同的供给方参数也不同 ReclaimPolicy： PV的回收策略，可用值有Delete(默认)和Retain 基于StorageClass的动态存储供应整体过程如下图所示： 1）集群管理员预先创建存储类（StorageClass）； 2）用户创建使用存储类的持久化存储声明(PVC：PersistentVolumeClaim)； 3）存储持久化声明通知系统，它需要一个持久化存储(PV: PersistentVolume)； 4）系统读取存储类的信息； 5）系统基于存储类的信息，在后台自动创建PVC需要的PV； 6）用户创建一个使用PVC的Pod； 7）Pod中的应用通过PVC进行数据的持久化； 8）而PVC使用PV进行数据的最终持久化处理。 更多可以参考：https://www.kubernetes.org.cn/4078.html 1）确定基于NFS服务来做的sc，NFS服务需要开启 123[root@master ~]# showmount -eExport list for master:/nfsdata * 2）需要RBAC权限 RBAC： rbac是k8s的API安全策略，是基于用户的访问权限 规定了谁可以有什么样的权限 为了给SC资源操作k8s集群的权限 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@master ~]# vim rbac-rolebind.yamlkind: NamespaceapiVersion: v1metadata: name: bdqn-test---apiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: bdqn-test---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: bdqn-testrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: bdqn-testroleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io[root@master ~]# kubectl apply -f rbac-rilebind.yaml namespace/bdqn-test createdserviceaccount/nfs-provisioner createdclusterrole.rbac.authorization.k8s.io/nfs-provisioner-runner createdclusterrolebinding.rbac.authorization.k8s.io/run-nfs-provisioner created 3）nfs-deployment. 作用：其实它是NFS的客户端，但是它通过K8S的内置的NFS驱动挂载远端的NFS服务器到本地目录；然后将自身作为storage provider，关联storage class 123456789101112131415161718192021222324252627282930313233343536[root@master ~]# vim nfs-deployment.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisioner namespace: bdqn-testspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME //提供者的名称 value: bdqn-test - name: NFS_SERVER //nfs服务器地址 value: 192.168.1.70 - name: NFS_PATH value: /nfsdata volumes: - name: nfs-client-root nfs: server: 192.168.1.70 path: /nfsdata[root@master ~]# kubectl apply -f nfs-deployment.yaml deployment.extensions/nfs-client-provisioner created 4）创建storageclass 12345678910[root@master ~]# vim test-storageclass.yamlapiVersion: storage.k8s.io&#x2F;v1kind: StorageClassmetadata: name: sc-nfs namespace: bdqn-test &#x2F;&#x2F;属于哪个名称空间provisioner: bdqn-test &#x2F;&#x2F;供给着，和nfs-deployment的名称要一样：value: bdqn-testreclaimPolicy: Retain[root@master ~]# kubectl apply -f test-storageclass.yaml storageclass.storage.k8s.io&#x2F;sc-nfs created provisioner: bdqn-test //通过preovisioner字段关联到上述Deployment 5）创建PVC 12345678910111213141516171819[root@master ~]# vim test-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-claim namespace: bdqn-testspec: storageClassName: sc-nfs accessModes: - ReadWriteMany resources: requests: storage: 20Mi[root@master ~]# kubectl apply -f test-pvc.yaml persistentvolumeclaim/test-claim created[root@master ~]# kubectl get pvc -n bdqn-test NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-claim Bound pvc-0c606810-9f93-441b-bc6b-391d7813dcab 20Mi RWX sc-nfs 2m15s 它会为我们自动生成一个pv 12345[root@master ~]# ls /nfsdata/bdqn-test-test-claim-pvc-0c606810-9f93-441b-bc6b-391d7813dcab[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-0c606810-9f93-441b-bc6b-391d7813dcab 20Mi RWX Delete Bound bdqn-test/test-claim sc-nfs 4m24s 6）创建Pod测试 12345678910111213141516171819202122232425262728[root@master ~]# vim test-pod.yamlkind: PodapiVersion: v1metadata: name: test-pod namespace: bdqn-testspec: containers: - name: test-pod image: busybox args: - /bin/sh - -c - sleep 30000 volumeMounts: - name: nfs-pvc mountPath: /test restartPolicy: OnFailure volumes: - name: nfs-pvc persistentVolumeClaim: claimName: test-claim[root@master ~]# kubectl apply -f test-pod.yaml pod/test-pod created[root@master ~]# kubectl get pod -n bdqn-test NAME READY STATUS RESTARTS AGEnfs-client-provisioner-57f49c99c7-lhd8n 1/1 Running 0 27mtest-pod 1/1 Running 0 32s 1234567[root@master ~]# kubectl exec -it test-pod -n bdqn-test /bin/sh/ # cd /test/test # touch test-file/test # echo 123456 &gt; test-file /test # exit[root@master ~]# cat /nfsdata/bdqn-test-test-claim-pvc-0c606810-9f93-441b-bc6b-391d7813dcab/test-file 123456 1234567[root@master ~]# kubectl exec -it -n bdqn-test nfs-client-provisioner-57f49c99c7-lhd8n /bin/sh/ # ls /persistentvolumes / # cd /persistentvolumes//persistentvolumes # lsbdqn-test-test-claim-pvc-0c606810-9f93-441b-bc6b-391d7813dcab#这个目录和/nfsdata下面的目录一样","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"k8s数据持久化","slug":"k8s数据持久化","date":"2020-02-10T08:53:18.320Z","updated":"2020-02-14T03:50:17.919Z","comments":true,"path":"k8s数据持久化.html","link":"","permalink":"http://pdxblog.top/k8s%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96.html","excerpt":"","text":"k8s数据持久化Docker容器是有生命周期的，因此数据卷可以实现数据持久化 数据卷主要解决的问题： 数据持久性：当我们写入数据时，文件都是暂时性的存在，当容器崩溃后，host就会将这个容器杀死，然后重新从镜像创建容器，数据就会丢失 数据共享：在同一个Pod中运行容器，会存在共享文件的需求 Volume：emptyDir（空目录）：使用情况比较少，一般只做临时使用，类似Docker数据 持久化的：docker manager volume，该数据卷初始分配时，是一个空目录，同一个Pod中的容器可以对该目录有执行读写操作，并且共享数据 ​ 使用场景：在同一个Pod里，不同的容器，共享数据卷 ​ 如果容器被删除，数据仍然存在，如果Pod被删除，数据也会被删除 使用实例： 123456789101112131415161718192021222324252627282930313233343536[root@master ~]# vim emptyDir.yamlapiVersion: v1kind: Podmetadata: name: producer-consumerspec: containers: - image: busybox name: producer volumeMounts: - mountPath: /producer_dir //容器内的路径 name: shared-volume //指定本地的目录名 args: - /bin/sh - -c - echo \"hello k8s\" &gt; /producer_dir/hello; sleep 30000 - image: busybox name: consumer volumeMounts: - mountPath: /consumer_dir name: shared-volume args: - /bin/sh - -c - cat /consumer_dir/hello; sleep 30000 volumes: - name: shared-volume //这里的名字必须与上面的Pod的mountPath的name相对应 emptyDir: &#123;&#125; //定义数据持久化类型，即表示空目录[root@master ~]# kubectl apply -f emptyDir.yaml[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGE producer-consumer 2/2 Running 0 14s[root@master ~]# kubectl logs producer-consumer consumer hello k8s 使用inspect查看挂载的目录在哪（查看Mount字段） 123456789101112131415161718192021222324252627282930313233[root@master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESproducer-consumer 2/2 Running 0 69s 10.244.1.2 node01 &lt;none&gt; &lt;none&gt;//可以看到容器运行在node01上，在node01上找到这个容器并查看并查看详细信息[root@node01 ~]# docker psCONTAINER ID IMAGEf117beb235cf busybox13c7a18109a1 busybox[root@node01 ~]# docker inspect 13c7a18109a1 \"Mounts\": [ &#123; \"Type\": \"bind\", \"Source\": \"/var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volume\", \"Destination\": \"/producer_dir\", //容器内的挂载目录 \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\"//再查看另一个容器[root@node01 ~]# docker inspect f117beb235cf \"Mounts\": [ &#123; \"Type\": \"bind\", \"Source\": \"/var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volume\", \"Destination\": \"/consumer_dir\", //容器内的挂载目录 \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\"//可以看到两个容器使用的同一个挂载目录[root@node01 ~]# cd /var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volume[root@node01 shared-volume]# lshello[root@node01 shared-volume]# cat hello hello k8s 将容器删除，验证目录是否存在 1234567[root@node01 ~]# docker rm -f 13c7a18109a1 13c7a18109a1[root@node01 ~]# docker psCONTAINER ID IMAGEa809717b1aa5 busyboxf117beb235cf busybox//它会重新生成一个新的容器，来达到我们用户所期望的状态，所以这个目录还是存在的 删除Pod 1234[root@master ~]# kubectl delete pod producer-consumer[root@master ~]# ls /var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volumels: 无法访问/var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volume: 没有那个文件或目录//Pod删除后数据也会被删除 hostPath Volume（使用场景也比较少）：类似Docker数据持久化的：bind mount 将Pod所在节点的文件系统上某一个文件或目录挂载进容器内 ​ 如果Pod被删除，数据会保留，相比较emptyDir会好一点，不过，一旦host崩溃，hostPath也无法访问 docker或者k8s集群本身的存储会采用hostPath这种方式 k8s集群中会有很多pod，如果都是用hostPath Volume的话管理起来很不方便，所以就用到了PV Persistent Volume | PV（持久卷）提前做好的，数据持久化的数据存放目录 是集群中的一块存储空间，由集群管理员管理或者由Storage class（存储类）自动管理，PV和pod、deployment、Service一样，都是一个资源对象 PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 该API对象捕获存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统 Psesistent Volume Claim | PVC（持久卷使用声明|申请） PVC代表用户使用存储的请求，应用申请PV持久化空间的一个申请、声明。K8s集群可能会有多个PV，你需要不停的为不同的应用创建多个PV 它类似于pod。Pod消耗节点资源，PVC消耗存储资源。 pod可以请求特定级别的资源（CPU和内存）。 权限要求可以请求特定的大小和访问模式 更多可以参考：https://www.kubernetes.org.cn/pvpvcstorageclass 基于NFS服务来做的PV 12345678910[root@master ~]# yum -y install nfs-utils (需要节点全部下载，会报挂载类型错误)[root@master ~]# yum -y install rpcbind[root@master ~]# mkdir &#x2F;nfsdata[root@master ~]# vim &#x2F;etc&#x2F;exports&#x2F;nfsdata *(rw,sync,no_root_squash)[root@master ~]# systemctl start rpcbind[root@master ~]# systemctl start nfs-server[root@master ~]# showmount -eExport list for master:&#x2F;nfsdata * 1.创建PV（实际的存储目录） 2.创建PVC 3.创建pod 创建PV资源对象： 12345678910111213141516171819[root@master ~]# vim nfs-pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: test-pvspec: capacity: //PV容量的大小 storage: 1Gi accessModes: //PV支持的访问模式 - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle //PV的存储空间的回收策略是什么 storageClassName: nfs nfs: path: /nfsdata/pv1 server: 192.168.1.70[root@master ~]# kubectl apply -f nfs-pv.yaml[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtest-pv 1Gi RWO Recycle Available nfs 9m30s accessModes: （PV支持的访问模式） ​ - ReadWriteOnce：能以读-写的方式mount到单个节点 ​ - ReadWriteMany：能以读-写的方式mount到多个节点 ​ - ReadOnlyMany：能以只读的方式mount到多个节点 persistentVolumeReclaimPolicy：（PV的存储空间的回收策略是什么） ​ Recycle：自动清除数据 ​ Retain：需要管理员手动回收 ​ Delete：云存储专用。直接删除数据 PV和PVC相互的关联：通过的是storageClassName &amp;&amp; accessModes 创建PVC 12345678910111213141516[root@master ~]# vim nfs-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-pvcspec: accessModes: //访问模式 - ReadWriteOnce resources: requests: storage: 1Gi //申请的容量大小 storageClassName: nfs //向哪个PV申请[root@master ~]# kubectl apply -f nfs-pvc.yaml[root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-pvc Bound test-pv 1Gi RWO nfs 14s PV的应用：创建一个Pod资源： 123456789101112131415161718192021[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-podspec: containers: - name: pod1 image: busybox args: - /bin/sh - -c - sleep 30000 volumeMounts: - mountPath: \"/mydata\" name: mydata volumes: - name: mydata persistentVolumeClaim: claimName: test-pvc[root@master ~]# kubectl apply -f pod.yaml 之前创建PV的时候指定的挂载目录是/nfsdata/pv1，我们并没有创建pv1这个目录，所以这个pod是运行不成功的。 以下是排错方法： kubectl describe kubectl logs /var/log/messages 查看该节点的kubelet的日志 123//使用kubectl describe[root@master ~]# kubectl describe pod test-podmount.nfs: mounting 192.168.1.70:/nfsdata/pv1 failed, reason given by server: No such file or directory //提示没有文件或目录 创建目录，再查看pod状态： 123[root@master ~]# mkdir /nfsdata/pv1NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-pod 1/1 Running 0 12m 10.244.1.3 node01 &lt;none&gt; &lt;none&gt; 验证是否应用成功： 123456[root@master ~]# kubectl exec test-pod touch /mydata/hello[root@master ~]# ls /nfsdata/pv1/hello[root@master ~]# echo 123 &gt; /nfsdata/pv1/hello [root@master ~]# kubectl exec test-pod cat /mydata/hello123 删除Pod，验证回收策略（Recycle）： 12345678[root@master ~]# kubectl delete pod test-pod[root@master ~]# kubectl delete pvc test-pvc[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtest-pv 1Gi RWO Recycle Available nfs 42h[root@master ~]# ls &#x2F;nfsdata&#x2F;pv1&#x2F;[root@master ~]#&#x2F;&#x2F;验证成功，数据已经回收 通常情况下不会设置为自动删除，不然就和emptyDir就差不多了 删除pv，修改回收策略： 之前是先创建PV—&gt;PVC—&gt;Pod，现在调整一下，先创建PV—&gt;—Pod—&gt;PVC 12345678910111213141516171819202122[root@master ~]# vim nfs-pv.yaml persistentVolumeReclaimPolicy: Retain[root@master ~]# kubectl apply -f nfs-pv.yaml [root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtest-pv 1Gi RWO Retain Available nfs 7s[root@master ~]# kubectl apply -f pod.yaml [root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEtest-pod 0&#x2F;1 Pending 0 5s &#x2F;&#x2F;Pending正在被调度[root@master ~]# kubectl describe pod test-podEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 41s (x2 over 41s) default-scheduler persistentvolumeclaim &quot;test-pvc&quot; not found&#x2F;&#x2F;没有发现对应的pvc创建pvc[root@master ~]# kubectl apply -f nfs-pvc.yaml[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEtest-pod 1&#x2F;1 Running 0 114s 验证Retain（管理员手动删除）回收策略： 1234567891011[root@master ~]# kubectl exec test-pod touch /mydata/k8s[root@master ~]# ls /nfsdata/pv1/k8s[root@master ~]# kubectl delete pod test-pod [root@master ~]# kubectl delete pvc test-pvc[root@master ~]# ls /nfsdata/pv1/k8s//可以看到并没有回收[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtest-pv 1Gi RWO Retain Available nfs 6s mysql对数据持久化的应用： //这里就不再创建PV，PVC了，用之前的就行 1234[root@master ~]# kubectl apply -f nfs-pvc.yaml [root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-pvc Bound test-pv 1Gi RWO nfs 7s 创建Deploment资源对象，mysql容器 123456789101112131415161718192021222324252627282930[root@master ~]# vim mysql.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: test-mysqlspec: selector: matchLabels: //基于等值的标签 app: mysql template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: 123.com volumeMounts: - name: mysql-storage mountPath: /var/lib/mysql volumes: - name: mysql-storage persistentVolumeClaim: claimName: test-pvc[root@master ~]# kubectl get deployments.NAME READY UP-TO-DATE AVAILABLE AGEtest-mysql 1/1 1 1 61s 进入容器创建数据，验证是否应用PV： 123456789101112131415161718[root@master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-mysql-569f8df4db-fnnxc 1/1 Running 0 32m 10.244.1.5 node01 &lt;none&gt; &lt;none&gt;[root@master ~]# kubectl exec -it test-mysql-569f8df4db-fnnxc -- mysql -u root -p123.commysql&gt; create database yun33; //创建数据库mysql&gt; use yun33; //选择使用数据路Database changedmysql&gt; create table my_id( id int(4)); 创建表mysql&gt; insert my_id values(9527); //在表中插入数据mysql&gt; select * from my_id; //查看表中所有数据+------+| id |+------+| 9527 |+------+1 row in set (0.00 sec)[root@master ~]# ls /nfsdata/pv1/auto.cnf ibdata1 ib_logfile0 ib_logfile1 k8s mysql performance_schema yun33 关闭node01节点，模拟节点宕机： 1234567891011121314151617[root@master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSIONmaster Ready master 36d v1.15.0node01 NotReady &lt;none&gt; 36d v1.15.0node02 Ready &lt;none&gt; 36d v1.15.0[root@master ~]# kubectl get pod -o wide -wNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-mysql-569f8df4db-fnnxc 1/1 Running 0 36m 10.244.1.5 node01 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-fnnxc 1/1 Terminating 0 38m 10.244.1.5 node01 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-2m5rd 0/1 Pending 0 0s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-2m5rd 0/1 Pending 0 0s &lt;none&gt; node02 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-2m5rd 0/1 ContainerCreating 0 0s &lt;none&gt; node02 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-2m5rd 1/1 Running 0 2s 10.244.2.4 node02 &lt;none&gt; &lt;none&gt;[root@master ~]# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-mysql-569f8df4db-2m5rd 1/1 Running 0 20s 10.244.2.4 node02 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-fnnxc 1/1 Terminating 0 38m 10.244.1.5 node01 &lt;none&gt; &lt;none&gt; 验证：在node02上新生成的pod，它内部是否有我们创建的数据 12345678910111213141516171819202122232425262728293031323334[root@master ~]# kubectl exec -it test-mysql-569f8df4db-2m5rd -- mysql -u root -p123.commysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || yun33 |+--------------------+4 rows in set (0.01 sec)mysql&gt; use yun33;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+-----------------+| Tables_in_yun33 |+-----------------+| my_id |+-----------------+1 row in set (0.01 sec)mysql&gt; select * from my_id;+------+| id |+------+| 9527 |+------+1 row in set (0.01 sec)[root@master ~]# ls /nfsdata/pv1/auto.cnf ibdata1 ib_logfile0 ib_logfile1 k8s mysql performance_schema yun33 Pod不断的重启： ​ 1.swap，没有关闭。导致集群运行不正常 ​ 2.内存不足，运行服务也会重启 小结负责把PVC绑定到PV的是一个持久化存储卷控制循环，这个控制器也是kube-manager-controller的一部分运行在master上。而真正把目录挂载到容器上的操作是在POD所在主机上发生的，所以通过kubelet来完成。而且创建PV以及PVC的绑定是在POD被调度到某一节点之后进行的，完成这些操作，POD就可以运行了。下面梳理一下挂载一个PV的过程： 用户提交一个包含PVC的POD 调度器把根据各种调度算法把该POD分配到某个节点，比如node01 Node01上的kubelet等待Volume Manager准备存储设备 PV控制器调用存储插件创建PV并与PVC进行绑定 Attach/Detach Controller或Volume Manager通过存储插件实现设备的attach。（这一步是针对块设备存储） Volume Manager等待存储设备变为可用后，挂载该设备到/var/lib/kubelet/pods//volumes/kubernetes.io~/目录上 Kubelet被告知卷已经准备好，开始启动POD，通过映射方式挂载到容器中","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"Docker swarm","slug":"Docker swarm","date":"2020-01-24T16:00:00.000Z","updated":"2020-02-02T09:31:18.463Z","comments":true,"path":"Docker swarm.html","link":"","permalink":"http://pdxblog.top/Docker%20swarm.html","excerpt":"","text":"Docker swarm docker swarm集群：三剑客之一 docker01 192.168.1.70 node1 docker02 192.168.1.50 node2 docker03 192.168.1.40 node3 关闭防火墙、禁用linux、3台dockerhost区别主机名，时间同步 1234[root@localhost ~]# setenforce 0[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# hostnamectl set-hostname node1[root@localhost ~]# su - docker版本必须是：v1.12版本开始 Swarm： 作用docker engin（引擎）的多个主机组成的集群 node： 每一个docker engin都是一个node（节点），分为manager和worker manager node： 负责执行编排和集群管理的工作，保持并维护swarm处于期望的状态，swarm可以有多个manager node，他们会自动协调并选举出一个Leader执行编排任务，但相反不能没有manager node worker node： 接受并执行由manager node派发的任务，并且默认manager node也是一个worker node，不过可以将它设置为manager-noly node，让他只负责编排和管理工作 service： 用来定义worker上执行的命令 1）初始化集群 12345678[root@node1 ~]# docker swarm init --advertise-addr 192.168.1.70Swarm initialized: current node (g26pbaqiozkn99qw9ngtgncke) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-3fp7qnihbzzy8u0esfjmmdfncdud4yh628e7bey5tu8fo3cl5p-4x021jwoh1bryxpwqd4bsj8xd 192.168.1.70:2377To add a manager to this swarm, run &#39;docker swarm join-token manager&#39; and follow the instructions. –advertise-addr：指定与其他Node通信的地址 上边返回的结果告诉我们：初始化成功，并且，如果想要添加work节点运行下面的命令： 1docker swarm join --token SWMTKN-1-3fp7qnihbzzy8u0esfjmmdfncdud4yh628e7bey5tu8fo3cl5p-4x021jwoh1bryxpwqd4bsj8xd 192.168.1.70:2377 PS：这里注意，token只有24小时的有效期 如果想要添加manager节点：运行下边的命令： 1docker swarm join-token manager 当两个节点加入成功之后，我们可以执行docker node ls查看节点详情 12345[root@node1 ~]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONg26pbaqiozkn99qw9ngtgncke * node1 Ready Active Leader 18.09.0w11nz7pzgmhq6wn5b51g6b8ao node2 Ready Active 18.09.0zqi7od9q0v7zo2tkabnseuqdf node3 Ready Active 18.09.0 基本操作命令： docker swarm leave：申请离开一个集群，之后查看节点状态会变成down然后通过manager node将其删除 docker node rm xxx：删除某个节点 docker swarm join-token {manager|worker}：生成令牌，可以是manager身份或worker身份 docker node demote（降级）：将swarm节点的manager降级为work docker node promote（升级）：将swarm节点的work升级为manager 2）部署docker swarm集群网络 overlay：覆盖型网络 1[root@node1 ~]# docker network create -d overlay --attachable docker attacheable：这个参数必须要加，否则不能用于容器 在创建网络的时候，我们并没有部署一个存储 服务，比如consul，那是因为docker swarm自带存储 3）部署一个图形化webUI界面 12[root@node1 ~]# docker run -d -p 8080:8080 -e HOST&#x3D;192.168.1.70 -e PORT&#x3D;8080 \\&gt; -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock --name viswalizer dockersamples&#x2F;visualizer 然后通过浏览器验证：192.168.1.70:8080 如果访问网页访问不到，需要开启路由转发： 12[root@node1 ~]# echo net.ipv4.ip_forward &#x3D; 1 &gt;&gt; &#x2F;etc&#x2F;sysctl.conf [root@node1 ~]# sysctl -p ) 4）创建service（服务） 1[root@node1 ~]# docker service create --replicas 1 --network docker --name web1 -p 80:80 nginx ) PS： 如果node2或node3宕机，这些服务会自动转到节点中没有宕机的host上，并继续运行 –replicas：副本数量 大概可以理解为以个副本等于一个容器 查看service： docker service ls 查看service信息： docker service ps xxx 删除server： docker service rm xxx 设置manager node不参加工作 1[root@node1 ~]# docker node update node1 --availability drain 5)搭建私有仓库 过程：略，详情请查看看https://blog.csdn.net/weixin_45636702/article/details/104002017 6）自定义镜像 要求：基于httpd镜像，更改访问界面内容。镜像tag版本为v1，对应主机页面内容为111，2222，3333 v1，v2，v3目录下的操作一样 12345678910[root@node01 ~]# mkdir &#123;v1,v2,v3&#125;[root@node01 ~]# cd v1[root@node01 v1]# vim index.html[root@node01 v1]# cat index.html111111111111111111111111111.................[root@node01 v1]# vim Dockerfile[root@node01 v1]# cat Dockerfile FROM httpdADD index.html &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs&#x2F;index.html 7）发布一个服务，基于上述镜像 要求：副本数量为3个，服务的名称为：bdqn 1[root@node01 ~]# docker service create --replicas 3 --name bdqn -p 80:80 192.168.1.70:5000&#x2F;httpd:v1 默认的ingress网络，包括创建的自定义overlay网络，为后端真正为用户提供服务的container，提供了一个统一的入口 随机映射的端口范围：30000-32767 8）服务的扩容与缩容 1[root@node01 ~]# docker service scale bdqn&#x3D;6 扩容与缩容可以直接通过scale进行设置副本数量 9）服务的升级与回滚 1[root@node01 ~]# docker service update --image 192.168.1.70:5000&#x2F;httpd:v2 bdqn //平滑的更新 1[root@node01 ~]# docker service update --image 192.168.1.70:5000&#x2F;httpd:v3 --update-parallelism 2 --update-delay 1m bdqn PS： 默认情况下，swarm一次只更新一个副本，并且两个副本之间没有等待时间，我们可以通过 –update-parallelisnm：设置并更新的副本数量 –update-delay：指定滚动更新时间间隔 //回滚操作 1[root@node01 ~]# docker service rollback bdqn PS： docker swarm的回滚操作，默认只能回滚到上一次的操作状态，并不能连续回滚操作","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/tags/Docker/"}]},{"title":"Docker架构+Docker镜像分层+Dockerfile","slug":"Docker架构+Docker镜像分层+Dockerfile","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.224Z","comments":true,"path":"Docker架构+Docker镜像分层+Dockerfile.html","link":"","permalink":"http://pdxblog.top/Docker%E6%9E%B6%E6%9E%84+Docker%E9%95%9C%E5%83%8F%E5%88%86%E5%B1%82+Dockerfile.html","excerpt":"","text":"Docker架构：) Docker架构总结： Docker是属于C/S架构，用户是使用 Docker Client 与 Docker Daemon 建立通信，并发送请求。请求接收后，Docker server通过http协议与路由，找到相应的 Handler 来执行请求 Docker Engine 是 Docker 架构中的运行引擎，同时也 Docker 运行的核心模块。Docker Engine 执行 Docker 内部的一系列工作，每一项工作都是以一个 Job 的形式的存在 Job 的运行过程中，当需要容器镜像时，则从 Docker Registry 中下载镜像，并通过镜像管理驱动 Graphdriver 将下载镜像以 Graph 的形式存储 当需要为 Docker 创建网络环境时，通过网络管理驱动 Networkdriver 创建并配置 Docker容器网络环境 当需要限制 Docker 容器运行资源或执行用户指令等操作时，则通过 Execdriver 来完成 Libcontainer 是一项独立的容器管理包，Networkdriver 以及 Execdriver 都是通过 Libcontainer 来实现具体对容器进行的操作 Docker镜像分层：Docker的最小镜像： 1[root@localhost ~]# docker pull hello-world 123FROM scratchCORP hello&#x2F;CMD [&quot;&#x2F;hello&quot;] Dockerfile的组成： 1）FROM：scratch（抓、挠） 2）COPY：hello/ 3）CMD：[“/hello”] base镜像(基础镜像)： Centos:7镜像的dockerfile 1234567891011FROM scratch &#x2F;&#x2F;从零开始构建ADD centos-7-x86 64-docker.tar.xz &#x2F;LABEL org. label-schema. schema-version&#x3D;&quot;1.0&quot;\\org. label-schema.namem&quot;centos Base Image&quot;\\org. label-schema.vendore&quot;Centos&quot;\\org. label-schema.Ticenses&quot;GPLv2&quot; \\org. labe1-schema.build-date&quot;20190305CMD [&quot;&#x2F;bin&#x2F;bash&quot;] 1[root@localhost test]# docker build -t centos7-vim-net-tools:12-11 . ) Dockerfile镜像分层总结： 镜像是容器的基石，容器是镜像运行后的实例，当镜像运行为容器之后，对镜像的所有数据仅有只读权限，如果需要对镜像源文件进行修改或删除操作时，此时是在容器层（可写层）进行的，用到了COW（copy on write）写时复制机制 Docker镜像的缓存特性 创建一个新的Dockerfile文件 12345FROM centos:7RUN yum -y install vimRUN yum -y install net-toolsRUN yum -y install wgetCMD [&quot;&#x2F;bin&#x2F;bash&quot;] 1[root@localhost ~]# docker build -t new-centos . 1）如果在相同层，有用到相同的镜像，可以不必再去下载，可以直接使用缓存 创建一个新的Dockefile文件 12345FROM centos:7RUN yum -y install vimRUN yum -y install wgetRUN yum -y install net-toolsCMD [&quot;&#x2F;bin&#x2F;bash&quot;] 1[root@localhost test1]# docker build -t centos-new . 2）即使镜像层里的操作一样，也必须是在同一层才可以使用dockerfile的缓存特性 如果制作镜像过程中，不想使用缓存可以加–no-cache选项 3）如果前面的曾发生改变，即使后边的层操作和顺序一样，也不能使用缓存特性 Dockerfile常用指令： 1）FROM：构建镜像基于哪个镜像 例如：FROM:centos:7 2）MAINTAINER：镜像维护者姓名或邮箱 例如：MAINTAINER admin 3）RUN：构建镜像时运行的shell命令 例如： RUN [“yum”,”install”,”httpd”] RUN yum -y install httpd 4）CMD：运行容器时执行的shell命令 例如： CMD [“/bin/bash”] 5）EXPOSE：声明容器的服务端口 例如：EXPOSE 80 443 6）ENV：设置容器环境变量 例如： ENV MYSQL_ROOT_PASSWORD 123.com 7）ADD：拷贝文件或目录到镜像，如果时URL或压缩包会自动下载或解压 ADD &lt;源文件&gt;… &lt;目标目录&gt; ADD [“源文件”…”目标目录”] 8）COPY：拷贝文件或目录到镜像容器内，跟ADD相似，但不具备自动下载或解压功能 9）ENTRYPOINT：运行容器时执行的shell命令 例如： ENTRYPOINT [“/bin/bash”,”-c”,”command”] ENTRYPOINT /bin/bash -c ‘command’ 10）VOLUME：指定容器挂在点到宿主机自动生成的目录或其他容器 例如： VOLUME [“/va/lib/mysql”] 11）USER：为RUN、CMD、和ENTRYPOINT执行命令指定运行用户 12）WORKDIR：为RUN、CMD、ENTRYPOINT、COPY和ADD设置工作目录，意思为切换目录 例如： WORKDIR：/var/lib/mysql 13）HEALTHCHECK：健康检查 14）ARG：构建时指定的一些参数 例如： FROM centos:7 ARG user USER $user 注意： 1、RUN在building时运行，可以写多条 2、CMD和ENTRYPOINT在运行container时，只能写一条，如果写多条，最后一条生效 3、CMD在run时可以被COMMAND覆盖，ENTRYPOINT不会被不会被COMMAND覆盖，但可以指定–entrypoint覆盖 4、如果在Dockerfile里需要往镜像内导入文件，则此文件必须在dockerfile所在目录或子目录下 小实验： 写一个dockerfile，基于cenyos:7镜像，部署安装NGINX服务 1234567891011121314151617[root@localhost ~]# mkdir web[root@localhost ~]# mv nginx-1.14.0.tar.gz web&#x2F;[root@localhost ~]# cd web&#x2F;[root@localhost web]# vim DockerfileFROM centos:7RUN yum -y install gcc pcre pcre-devel openssl openssl-devel zlib zlib-develCOPY nginx-1.14.0.tar.gz &#x2F;RUN tar -zxf nginx-1.14.0.tar.gz -C &#x2F;usr&#x2F;srcRUN useradd -M -s &#x2F;sbin&#x2F;nologin nginxWORKDIR &#x2F;usr&#x2F;src&#x2F;nginx-1.14.0RUN .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --user&#x3D;nginx --group&#x3D;nginxRUN make &amp;&amp; make installRUN ln -s &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;* &#x2F;usr&#x2F;local&#x2F;sbinRUN nginx -tRUN nginxEXPOSE 80[root@localhost web]# docker build -t test-web . &#x2F;&#x2F;如果Dockerfile在其他路径需要加-f参数来指定Dockerfile文件路径 //如果想要保证容器运行之后，nginx服务就直接开启，不必手动开启，我们可以在命令最后加上：nginx -g “daemon off;”选项 1[root@localhost web]# docker run -itd --name testweb_2 test-web:latest nginx -g &quot;daemon off;&quot; //查看容器的IP： 1[root@localhost web]# docker inspect testweb_2","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/tags/Docker/"}]},{"title":"Docker的基本操作命令","slug":"Docker的基本操作命令","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.240Z","comments":true,"path":"Docker的基本操作命令.html","link":"","permalink":"http://pdxblog.top/Docker%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4.html","excerpt":"","text":"Docker的基本操作命令：//查找镜像： 1[root@localhost ~]# docker search mysql &#x2F;&#x2F;默认在docker hub公共仓库进行查找 //拉取镜像，下载镜像: 1[root@localhost ~]# docker pull busybox //导出镜像到本地： 1[root@localhost ~]# docker save -o busybox.tar busybox:latest （docker save &gt; busybox.tar busybox:latest） //查看本地镜像： 1[root@localhost ~]# docker images （docker image ls） PS：虽然我们查看到镜像标签位latest（最新的），但并不表示它一定是最新的，而且镜像如果没有写标签，默认是以latest为标签 //删除镜像： 1[root@localhost ~]# docker rmi busybox:latest //根据本地镜像包导入镜像： 1[root@localhost ~]# docker load -i busybox.tar （docker load &lt; busybox.tar ） //查看容器–正在运行的： 1[root@localhost ~]# docker ps //查看所有的容器： 1[root@localhost ~]# docker ps -a //删除容器： 1[root@localhost ~]# docker rm centos [CONTAINER ID&#x2F;容器名称] //停止容器运行： 1[root@localhost ~]# docker stop centos //启动容器: 1[root@localhost ~]# docker start centos PS:开启容器后记得去验证一下容器是否开启 //强制删除容器： 1[root@localhost ~]# docker rm centos -f //强制删除所有容器（生产环境严禁使用）： 1[root@localhost ~]# docker ps -a -q | xargs docker rm -f -——————————————————————————————— 123[root@localhost ~]# docker ps -a -q | xargs docker start -f &#x2F;&#x2F;开启所有容器[root@localhost ~]# docker ps -a -q | xargs docker stop -f &#x2F;&#x2F;关闭所有容器 //重启一个容器： 1[root@localhost ~]# docker restart test2 //运行一个容器： 1[root@localhost ~]# docker run -it --name test1 centos:7 -i：交互 -t：伪终端 -d（daemon）：后台运行 –name：给容器命名 –restart=always：始终保持运行（随着docker开启而运行） 1[root@localhost ~]# docker create -it --name test3 centos:7 &#x2F;&#x2F;不常用 //进入一个容器： 123[root@localhost ~]# docker exec -it test2 &#x2F;bin&#x2F;bash[root@localhost ~]# docker attach test2 区别： exec进入的方式需要添加-i，-t选项，后面还需要给容器一个shell环境，但attach就不需要这么麻烦 exec进入的方式：如果exit退出，容器仍然保持运行 attach：如果执行exit退出，容器会被关闭，如果想要保持容器不被关闭，可以使用键盘：ctrl+p ctrl+q可以实现 本质上区别： exec进入的方法，会产生新的进程 attach进入的方法，不会产生新的进程 Docker的基本操作逻辑： ) 小实验： 基于centos:7镜像运行一个容器，并且在这个容器内部署nginx服务 1）下载镜像： 1[root@localhost ~]# docker pull centos:7 2）运行容器： 1[root@localhost ~]# docker run -itd --name webapp --restart&#x3D;always centos:7 3）进入容器，开始部署nginx服务：//将nginx包导入到容器内： 1[root@localhost ~]# docker cp nginx-1.14.0.tar.gz 12345678910111213[root@localhost ~]# docker exec -it webapp &#x2F;bin&#x2F;bash[root@01b870908942 ~]# tar zxf nginx-1.14.0.tar.gz [root@01b870908942 ~]# cd nginx-1.14.0[root@01b870908942 nginx-1.14.0]# yum -y install gcc pcre pcre-devel openssl openssl-devevl zlib zlib-devel[root@01b870908942 nginx-1.14.0]# useradd -s &#x2F;sbin&#x2F;nologin nginx[root@01b870908942 nginx-1.14.0]# .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --user&#x3D;nginx --group&#x3D;nginx[root@01b870908942 nginx-1.14.0]# make &amp;&amp; make install[root@01b870908942 nginx-1.14.0]# ln -s &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;* &#x2F;usr&#x2F;local&#x2F;sbin&#x2F;[root@01b870908942 nginx-1.14.0]# nginx[root@01b870908942 nginx-1.14.0]# cd &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;html&#x2F;[root@01b870908942 html]# echo This is a resrweb in container &gt; index.html[root@01b870908942 html]# curl 127.0.0.1This is a resrweb in container //把容器制作成镜像：（可移植性） 1[root@localhost ~]# docker commit webapp myweb:12-10","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/tags/Docker/"}]},{"title":"Docker的底层原理","slug":"Docker的底层原理","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.240Z","comments":true,"path":"Docker的底层原理.html","link":"","permalink":"http://pdxblog.top/Docker%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86.html","excerpt":"","text":"Docker底层原理如果虚拟机内服务对内核版本有要求，这个服务就不太适合用docker来实现了 Busybox：欺骗层 解耦：解除耦合、解除冲突 耦合：冲突现象 run—–&gt;Centos系统（nginx、web） 对于docker host来说这个系统仅仅是一个进程 Namespace（名称空间）： 用来隔离容器 1234[root@localhost ns]# pwd&#x2F;proc&#x2F;2971&#x2F;ns[root@localhost ns]# lsipc mnt net pid user uts ipc：共享内存、消息列队 mnt：挂载点、文件系统 net：网络栈 pid：进程编号 user：用户、组 uts：主机名、域名 //namespace六项隔离，实现了容器与宿主机、容器与容器之间的隔离 Cgroup（控制组）： 资源的限制 12[root@d9d679199f74 cgroup]# pwd&#x2F;sys&#x2F;fs&#x2F;cgroup 1[root@localhost cpu]# cat tasks PS：task这个文件内的数字，记录的是进程编号。PID 四大功能： 1）资源限制：cgroup可以对进程组使用的资源总额进行限制 2）优先级分配：通过分配的cpu时间片数量以及硬盘IO带宽大小，实际上相当于控制了进程运行的优先级别 3）资源统计：cgroup可以统计西系统资源使用量，比如cpu使用时间，内存使用量等，用于按量计费。同时还支持挂起功能，也就是说用过cgroup把所有的资源限制起来，对资源都不能使用，注意并不算是说我们的程序不能使用了，只是不能使用资源，处于挂起等待状态 4）进程控制：可以对进程组执行挂起、恢复等操作 内存限额： 容器内存包括两个部分：物理内存和swap 可以通过参数控制容器内存的使用量： -m或者–memory：设置内存的使用限额 –memory-swap：设置内存+swap的使用限额 举个例子： 如果运行一个容器，并且限制该容器最多使用200M内存和100M的swap 123[root@localhost ~]# docker run -it -m 200M --memory-swap 300M centos:7[root@5bc0e71faba3 memory]# pwd&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory //内存使用限制 12[root@5bc0e71faba3 memory]# cat memory.limit_in_bytes 209715200（字节） //内存+swap限制 12[root@5bc0e71faba3 memory]# cat memory.memsw.limit_in_bytes314572800（字节） 对比一个没有限制的容器，我们会发现，如果运行容器之后不限制内存的话，意味着没有限制 CPU使用： 通过-c或者–cpu-shares设置容器使用cpu的权重，如果 不设置默认为1024 举个例子： //没有限制：1024 1234[root@localhost ~]# docker run -it --name containerA centos:7[root@e2d88b8f8b87 &#x2F;]# cd &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu[root@e2d88b8f8b87 cpu]# cat cpu.shares 1024 //限制CPU使用权重为512： 1234[root@localhost ~]# docker run -it --name containerB -c 512 centos:7[root@f8165e07c8d7 &#x2F;]# cd &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu[root@f8165e07c8d7 cpu]# cat cpu.shares 512 容器的Block IO（磁盘的读写）： docker中可以通过设置权重，限制bps和iops的方式控制容器读写磁盘的IO bps：每秒的读写的数据量（byte per second） iops：每秒IO的次数 （io per second） 默认情况下，所有容器都能够平等的读写磁盘，也可以通过–blkio-weight参数改变容器的blockIO的优先级 –device-read-bps：显示读取某个设备的bps –device-write-bps：显示写入某个设备的bps –device-read-iops：显示读取某个设备的iops –device-write-iops：显示写入某个设备的iops //限制testA这个容器，写入/dev/sda这块磁盘的bps为30MB 1[root@localhost ~]# docker run -it --name testA --device-write-bps &#x2F;dev&#x2F;sda:30MB centos:7 //从/dev/zero输入，然后输出到test.out文件中，每次大小为1M，总共为800次，oflg=direct用来指定directIO方式写文件，这样才会使–device-write-bps生效 1[root@0e659ca3e85d &#x2F;]# time dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;test.out bs&#x3D;1M count&#x3D;800 oflag&#x3D;direct","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/tags/Docker/"}]},{"title":"Docker的监控","slug":"Docker的监控","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.256Z","comments":true,"path":"Docker的监控.html","link":"","permalink":"http://pdxblog.top/Docker%E7%9A%84%E7%9B%91%E6%8E%A7.html","excerpt":"","text":"Docker的监控docker自带的监控命令 docker top / stats / logs sysdig 12345678910[root@localhost ~]# docker load &lt; sysdig.tar[root@localhost ~]# docker load &lt; scope.1.12.tar[root@localhost ~]# docker run -it --rm --name sysdig \\&gt; --privileged&#x3D;true \\&gt; --volume&#x3D;&#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;host&#x2F;var&#x2F;run&#x2F;docker.sock \\&gt; --volume&#x3D;&#x2F;dev:&#x2F;host&#x2F;dev \\&gt; --volume&#x3D;&#x2F;proc:&#x2F;host&#x2F;proc:ro \\&gt; --volume&#x3D;&#x2F;boot:&#x2F;host&#x2F;boot:ro \\&gt; --volume&#x3D;&#x2F;lib&#x2F;modules:&#x2F;host&#x2F;lib&#x2F;modules:ro \\&gt; --volume&#x3D;&#x2F;usr:&#x2F;host&#x2F;usr:ro sysdig&#x2F;sysdig //下载失败后可以运行下边的命令，重新下载 1root@2fefbfde3db5:&#x2F;# sysdig-probe-loader //下载成功之后，可以运行sysdig命令 1root@2fefbfde3db5:&#x2F;# csysdig scope123[root@localhost ~]# curl -L git.io&#x2F;scope -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scope[root@localhost ~]# chmod a+x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scope[root@localhost ~]# scope launch //访问本机的4040端口 ) //监控两台dockerhost docker01 192.168.1.70 docker02 192.168.1.50 //docker02上也需要同样的操作 123[root@docker02 ~]# curl -L git.io&#x2F;scope -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scope[root@docker02 ~]# chmod a+x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scope[root@docker02 ~]# docker load &lt; scope.1.12.tar 12[root@docker01 ~]# scope launch 192.168.1.70 192.168.1.50[root@docker02 ~]# scope launch 192.168.1.50 192.168.1.70 )","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/tags/Docker/"}]},{"title":"Docker的私有仓库","slug":"Docker的私有仓库","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.256Z","comments":true,"path":"Docker的私有仓库.html","link":"","permalink":"http://pdxblog.top/Docker%E7%9A%84%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93.html","excerpt":"","text":"Registry用docker容器运行registry私有仓库 下载registry镜像： 1[root@localhost ~]# docker pull registry:2 &#x2F;&#x2F;2版本是使用go语言编写的，而registry是使用python写的 //运行私有仓库： 1[root@localhost ~]# docker run -itd --name registry --restart&#x3D;always -p 5000:5000 -v &#x2F;registry:&#x2F;var&#x2F;lib&#x2F;registry registry:2 -p：端口映射（宿主机端口：容器暴露的端口） -v：挂载目录（宿主机的目录：容器内的目录） 镜像重命名： 1[root@localhost ~]# docker tag test-web:latest 192.168.1.70:5000&#x2F;test 上传镜像到私有仓库： 123456[root@localhost ~]# vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service修改：指定私有仓库地址ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd --insecure-registry 192.168.1.70:5000[root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker[root@localhost ~]# docker push 192.168.1.70:5000&#x2F;test:latest 这里注意，既然是私有仓库，肯定是要考虑多台DockerHost共用的情况，如果有其他的DockerHost想要使用私有仓库，仅需要修改docker的配置文件，指定私有仓库的IP和端口即可。当然别忘了，更改过配置文件之后，daemon-reload ,restart docker服务 企业级私有仓库镜像Harbor下载一个docker-compse工具 //从GitHub上下载方法： 12[root@docker01 ~]# curl -L https:&#x2F;&#x2F;github.com&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.25.0&#x2F;docker-compose-&#96;uname -s&#96;-&#96;uname -m&#96; -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose[root@docker01 ~]# chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose 12[root@docker01 ~]# tar zxf docker-compose.tar.gz -C &#x2F;usr&#x2F;local&#x2F;bin[root@docker01 ~]# chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose //下载依赖包 1[root@docker01 ~]# yum -y install yum-utils device-mapper-persistent-data lvm2 //导入harbo离线安装包，并解压到/usr/local/下 1[root@docker01 ~]# tar zxf harbor-offline-installer-v1.7.4.tgz -C &#x2F;usr&#x2F;local&#x2F; //安装harbor 1234[root@docker01 ~]# cd &#x2F;usr&#x2F;local&#x2F;harbor&#x2F;[root@docker01 harbor]# vim harbor.cfghostname &#x3D; 192.168.1.70[root@docker01 harbor]# .&#x2F;install.sh //浏览器访问：192.168.1.70 用户名：admin 密码：Harbor12345 ) //修改docker配置文件，连接Harbor私有仓库 1234[root@docker01 ~]# vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.serviceExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd --insecure-registry 192.168.1.70[root@docker01 ~]# systemctl daemon-reload [root@docker01 ~]# systemctl restart docker //创建私有仓库 ) //登录仓库上传镜像 123[root@docker01 harbor]# docker login -u admin -p Harbor12345 192.168.1.70[root@docker01 harbor]# docker tag centos:7 192.168.1.70&#x2F;bdqn&#x2F;centos:7[root@docker01 harbor]# docker push 192.168.1.70&#x2F;bdqn&#x2F;centos:7 //从私有仓库下载镜像 1[root@docker03 ~]# docker pull 192.168.1.70&#x2F;bdqn&#x2F;centos:7","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/tags/Docker/"}]},{"title":"Docker部署LNMP环境","slug":"Docker部署LNMP环境","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.271Z","comments":true,"path":"Docker部署LNMP环境.html","link":"","permalink":"http://pdxblog.top/Docker%E9%83%A8%E7%BD%B2LNMP%E7%8E%AF%E5%A2%83.html","excerpt":"","text":"Docker部署LNMP环境172.16.10.0/24 Nginx：172.16.10.10 Mysql：172.16.10.20 PHP：172.16.10.30 网站的访问主目录：/wwwroot Nginx的配置文件：/docker 123456789101112[root@localhost ~]# docker run -itd --name test nginx:latest[root@localhost ~]# mkdir &#x2F;wwwroot[root@localhost ~]# mkdir &#x2F;docker[root@localhost ~]# docker cp test:&#x2F;etc&#x2F;nginx &#x2F;docker&#x2F;[root@localhost ~]# ls &#x2F;docker&#x2F;nginx[root@localhost ~]# docker cp test:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html &#x2F;wwwroot&#x2F;[root@localhost ~]# ls &#x2F;wwwroot&#x2F;html[root@localhost ~]# vim &#x2F;wwwroot&#x2F;html&#x2F;index.html[root@localhost ~]# cat &#x2F;wwwroot&#x2F;html&#x2F;index.html hello LNMP! 1）创建一个自定义网络 1[root@localhost ~]# docker network create -d bridge --subnet 172.16.10.0&#x2F;24 --gateway 172.16.10.1 lnmp 2）运行nginx容器 1[root@localhost ~]# docker run -itd --name nginx -v &#x2F;docker&#x2F;nginx&#x2F;:&#x2F;etc&#x2F;nginx -v &#x2F;wwwroot&#x2F;html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html -p 80:80 --network lnmp --ip 172.16.10.10 nginx:latest 3）运行mysql容器 123456789101112131415[root@localhost ~]# docker run --name mysql -e MYSQL_ROOT_PASSWORD&#x3D;123.com -d -p 3306:3306 --network lnmp --ip 172.16.10.20 mysql:5.7[root@localhost ~]# yum -y install mysql[root@localhost ~]# mysql -u root -p123.com -h 127.0.0.1 -P 3306MySQL [(none)]&gt; create database name;MySQL [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || name || performance_schema || sys |+--------------------+5 rows in set (0.00 sec) 4）运行php容器 1[root@localhost ~]# docker run -itd --name phpfpm -p 9000:9000 -v &#x2F;wwwroot&#x2F;html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html --network lnmp --ip 172.16.10.30 php:7.2-fpm //添加php测试页面： 123456[root@localhost html]# pwd&#x2F;wwwroot&#x2F;html[root@localhost html]# cat test.php &lt;?phpphpinfo();?&gt; 5）修改nginx配置文件，nginx和php连接 1234567891011121314[root@localhost ~]# cd &#x2F;docker&#x2F;nginx&#x2F;conf.d&#x2F;[root@localhost conf.d]# vim default.conf location &#x2F; &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; index index.html index.htm index.php; &#x2F;&#x2F;添加php解析&#x2F;&#x2F;打开此模块，并更改相应信息 location ~ \\.php$ &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; fastcgi_pass 172.16.10.30:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; //重启nginx 1[root@localhost conf.d]# docker restart nginx //到此，去浏览器验证，nginx服务和php服务界面 ) ) 说明nginx和php的来连接，没有问题，接下来是php和mysql的连接，在这我们使用一个phpMyAdmin的数据库管理工具 1234[root@localhost html]# pwd&#x2F;wwwroot&#x2F;html[root@localhost html]# unzip phpMyAdmin-4.9.1-all-languages.zip[root@localhost html]# mv phpMyAdmin-4.9.1-all-languages phpmyadmin //更改nginx的配置文件 123456789101112131415161718[root@localhost ~]# cd &#x2F;docker&#x2F;nginx&#x2F;conf.d&#x2F;[root@localhost conf.d]# pwd&#x2F;docker&#x2F;nginx&#x2F;conf.d[root@localhost conf.d]# vim default.conf&#x2F;&#x2F;在27行添加 location &#x2F;phpmyadmin &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; index index.html index.htm index.php; &#125;&#x2F;&#x2F;在43行添加 location ~ &#x2F;phpmyadmin&#x2F;(?&lt;after_ali&gt;(.*)\\.(php|php5)?$) &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; fastcgi_pass 172.16.10.30:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; //重启nginx 1[root@localhost conf.d]# docker restart nginx //验证php主界面 ) //需要我们对php镜像做出更改，添加php和mysql连接的模块 写一个Docker 123456789FROM php:7.2-fpmRUN apt-get update &amp;&amp; apt-get install -y \\ libfreetype6-dev \\ libjpeg62-turbo-dev \\ libpng-dev \\ &amp;&amp; docker-php-ext-install -j$(nproc) iconv \\ &amp;&amp; docker-php-ext-configure gd --with-freetype-dir&#x3D;&#x2F;usr&#x2F;include&#x2F; --with-jpeg-dir&#x3D;&#x2F;usr&#x2F;include&#x2F; \\ &amp;&amp; docker-php-ext-install -j$(nproc) gd \\ &amp;&amp; docker-php-ext-install mysqli pdo pdo_mysql 1[root@localhost ~]# docker build -t phpmysql . //删除之前的php容器，并用我们新制作的php镜像重新运行 123[root@localhost ~]# docker stop phpfpm [root@localhost ~]# docker rm phpfpm [root@localhost ~]# docker run -itd --name phpfpm -p 9000:9000 -v &#x2F;wwwroot&#x2F;html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html --network lnmp --ip 172.16.10.30 phpmysql:latest //修改phpmyadmin的配置文件，指定连接的数据库的IP，然后重启php容器 1234567[root@localhost ~]# cd &#x2F;wwwroot&#x2F;html&#x2F;phpmyadmin&#x2F;[root@localhost phpmyadmin]# pwd&#x2F;wwwroot&#x2F;html&#x2F;phpmyadmin[root@localhost phpmyadmin]# cp config.sample.inc.php config.inc.php[root@localhost phpmyadmin]# vim config.inc.php$cfg[&#39;Servers&#39;][$i][&#39;host&#39;] &#x3D; &#39;172.16.10.20&#39;; &#x2F;&#x2F;修改，指定数据库的IP地址[root@localhost ~]# docker restart phpfpm 用户名：root 密码：123.com ) //登录成功之后会看到我们之前创建的数据库 )","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/tags/Docker/"}]},{"title":"KVM基本操作命令","slug":"KVM基本操作命令","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.334Z","comments":true,"path":"KVM基本操作命令.html","link":"","permalink":"http://pdxblog.top/KVM%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4.html","excerpt":"","text":"基于操作命令1）查看虚拟机列表： 12[root@kvm ~]# virsh list &#x2F;&#x2F;查看正在运行的虚拟机[root@kvm ~]# virsh list --all &#x2F;&#x2F;查看所有虚拟机 //开机的虚拟机才有ID号，而且会随时变化 Id 名称 状态 test01 关闭 2）查看虚拟机的详细信息： 1234567891011121314[root@kvm ~]# virsh dominfo test01 &#x2F;&#x2F;dom全称domain，域的意思Id: -名称： test01UUID: 8ba94166-08dd-4805-962b-c99ed56869bcOS 类型： hvm状态： 关闭CPU： 1最大内存： 1048576 KiB使用的内存： 1048576 KiB持久（peisistent）： 是 &#x2F;&#x2F;数据的持久化自动启动（autostart）： 禁用 &#x2F;&#x2F;是否开机自启管理的保存： 否安全性模式： none安全性 DOI： 0 3）虚拟机域的开关机： 123[root@kvm ~]# virsh start test01 &#x2F;&#x2F;开机[root@kvm ~]# virsh shutdown test01 &#x2F;&#x2F;关机（shutdown：温柔的关机）[root@kvm ~]# virsh shutdown 2 &#x2F;&#x2F;2为ID号 //关机后再开机ID号也会变化 1[root@kvm ~]# virsh destroy test01 &#x2F;&#x2F;强制关机，类似于拔电源 4）导出配置： 1[root@kvm ~]# virsh dumpxml test01 &gt; test01.xml &#x2F;&#x2F;dump备份的意思 vmnet0：桥接 //好处：外网能够访问你的虚拟机vmnet1：主机vmnet8：NAT //缺点：外网访问不了你的虚拟机，好处：可以自己随意指定IP 一个完成的KVM域，生成之后会有两个文件： 1）磁盘文件：在部署之处已经指定 //用来记录它的信息 2）xml配置文件，默认在/etc/libvirt/qemu //qemu模拟硬件，类型为raw 5）删除虚拟机：//删除之前保证虚拟机是关闭状态 1[root@kvm ~]# virsh undefine test01 &#x2F;&#x2F;undefine取消定义 //xml配置文件也会被删除，但是磁盘文件不会被影响 6）根据配置文件恢复虚拟机： 1[root@kvm ~]# virsh define test01.xml &#x2F;&#x2F;define：定义 7）修改配置文件： 1[root@kvm qemu]# virsh edit test01 edit：自带语法检查功能（y：是、n：不、i：忽略、f：强制）vim：不会提示你语法错误 8）虚拟机重命名（7.2版本之前的不支持这条命令） 1[root@kvm ~]# virsh domrename test01 test1 &#x2F;&#x2F;重命名前关闭虚拟机 9）查看虚拟机对应的vnc端口 12[root@localhost ~]# virsh vncdisplay test01:0 :0等于5900:1=5901:2=5902 10)挂起虚拟机 12[root@localhost ~]# virsh suspend test01[root@localhost ~]# virsh resume test01 &#x2F;&#x2F;恢复挂起的虚拟机 11）开机自启 12[root@localhost ~]# virsh autostart test01[root@localhost autostart]# virsh autostart --disable test01 &#x2F;&#x2F;取消开机自启 12）console登录KVM域//在KVM域里添加 1234grubby --update-kernel&#x3D;ALL --args&#x3D;&quot;console&#x3D;ttyS0&quot;rebootvirsh console test01 &#x2F;&#x2F;使用xshell连接kvm退出 ctrl+]","categories":[{"name":"KVM","slug":"KVM","permalink":"http://pdxblog.top/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://pdxblog.top/tags/KVM/"}]},{"title":"KVM简介","slug":"KVM简介","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.318Z","comments":true,"path":"KVM简介.html","link":"","permalink":"http://pdxblog.top/KVM%E7%AE%80%E4%BB%8B.html","excerpt":"","text":"KVM简介：什么是云计算：云计算:配置各种资源的方式 云计算的分类：基础即服务Lass平台即服务Pass软件即服务Sass如果按照不同的部署方式：公有云、私有云、混合云 KVM介绍：虚拟化的不同的方式 实现虚拟化的技术：基于二进制翻译的全虚拟化：（会报错）解决思路：捕捉报错—-翻译—模拟（会增加服务器的开销） 半虚拟化（Xen）：更改内核，只能用到Linux系统上全虚拟化：KVM、VMware//所依赖的硬件全部准备好就行 KVM的概念：基于内核的虚拟机（Kernel-Based VIrtul mathine） 打开KVM的方式：virt-manager应用程序–系统工具–虚拟系统管理器 命令创建虚拟主机域： 1234[root@localhost iso]# virt-install --os-type&#x3D;linux --os-variant centos7.0--name test01 --ram 1024 --vcpus 1 --disk&#x3D;&#x2F;kvm-vm&#x2F;centos.raw,format&#x3D;raw,size&#x3D;10--location &#x2F;iso&#x2F;CentOS-7-x86_64-DVD-1611.iso --network network&#x3D;default --graphics vnc,listen&#x3D;0.0.0.0 --noautoconsole（不会占用终端） vnc连接KVM虚拟机默认的端口为：5900","categories":[{"name":"KVM","slug":"KVM","permalink":"http://pdxblog.top/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://pdxblog.top/tags/KVM/"}]},{"title":"KVM磁盘格式","slug":"KVM磁盘格式","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.318Z","comments":true,"path":"KVM磁盘格式.html","link":"","permalink":"http://pdxblog.top/KVM%E7%A3%81%E7%9B%98%E6%A0%BC%E5%BC%8F.html","excerpt":"","text":"磁盘格式：RAW：（裸格式） //占用空间较大，性能较好，但不支持虚拟机快照功能QCOW2：（copy on write） //占用空间较小，支持快照，性能比RAW稍差一些 创建磁盘：（默认是裸格式） 1[root@kvm disk]# qemu-img create 1234.raw 5G 查看磁盘信息： 1[root@kvm disk]# qemu-img info 1234.raw 创建指定格式磁盘： 1[root@kvm disk]# qemu-img create -f qcow2 bdqn.qcow2 5G 转换磁盘格式：qemu-img convert [-f fmt] [-O output_fmt] filename output_filename 1[root@kvm kvm-vm]# qemu-img convert -f raw -O qcow2 centos.raw centos.qcow2 //转换之后原来的磁盘还在 拍摄快照： 1[root@kvm ~]# virsh snapshot-create test01 查看快照信息： 1234[root@kvm ~]# virsh snapshot-list test01 名称 生成时间 状态------------------------------------------------------------ 1575254957 2019-12-02 10:49:17 +0800 running 时间戳：1970年1月1号（计算机C语言诞生了，Linux系统诞生了）32位系统：68年之后你的系统就不能使用了64位系统：使用时间没有限制 根据快照恢复系统： 1[root@kvm ~]# virsh snapshot-revert test01 1575254957 //拍摄的快照是占用磁盘空间的","categories":[{"name":"KVM","slug":"KVM","permalink":"http://pdxblog.top/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://pdxblog.top/tags/KVM/"}]},{"title":"KVM网络","slug":"KVM网络","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.318Z","comments":true,"path":"KVM网络.html","link":"","permalink":"http://pdxblog.top/KVM%E7%BD%91%E7%BB%9C.html","excerpt":"","text":"NAT模式：KVM默认的网络方式，如果想要应用这种模式，防火墙需要打开，因为需要用到iptables规则 //打开防火墙添加规则，打开5900端口 12345[root@localhost ~]# firewall-cmd --add-port&#x3D;5900&#x2F;tcp --permanent success[root@localhost ~]# firewall-cmd --reloadsuccess[root@localhost ~]# firewall-cmd --list-all //添加路由转发： 123[root@localhost ~]# echo net.ipv4.ip_forward &#x3D; 1 &gt;&gt; &#x2F;etc&#x2F;sysctl.conf [root@localhost ~]# sysctl -pnet.ipv4.ip_forward &#x3D; 1 总结：nat模式支持主机与虚拟机的互访，也支持虚拟机访问互联网，但不支持外网访问虚拟机域 桥接网络：1）创建虚拟桥接网卡br0 1234[root@localhost ~]# systemctl stop NetworkManager[root@localhost ~]# virsh iface-bridge ens33 br0 &#x2F;&#x2F;提示失败不用理会使用附加设备 br0 生成桥接 ens33 失败已启动桥接接口 br0 //查看配置文件会看到，ens33桥接到了br0 12345678[root@localhost network-scripts]# cat ifcfg-ens33DEVICE&#x3D;ens33ONBOOT&#x3D;yesBRIDGE&#x3D;&quot;br0&quot;[root@localhost network-scripts]# brctl showbridge name bridge id STP enabled interfacesbr0 8000.000c2901f11f yes ens33virbr0 8000.525400dc381b yes virbr0-nic 2）修改kvm虚拟机域的xml配置文件： 123&lt;interface type&#x3D;&#39;bridge&#39;&gt;&lt;mac address&#x3D;&#39;52:54:00:f8:a1:c9&#39;&#x2F;&gt;&lt;source bridge&#x3D;&#39;br0&#39;&#x2F;&gt; 3）开启虚拟机，配置IP，验证是否能够联通外网: 1[root@localhost ~]# vi &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth //IP和br0的IP要在同一网段","categories":[{"name":"KVM","slug":"KVM","permalink":"http://pdxblog.top/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://pdxblog.top/tags/KVM/"}]},{"title":"ReplicaSet、DaemonSet","slug":"ReplicaSet、DaemonSet","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.303Z","comments":true,"path":"ReplicaSet、DaemonSet.html","link":"","permalink":"http://pdxblog.top/ReplicaSet%E3%80%81DaemonSet.html","excerpt":"","text":"ReplicaSet RC：ReplicationoController（老一代的Pod控制器） RS：ReplicaSet（新一代的Pod控制器） 用于确保由其管理的控制的Pod对象副本，能够满足用户期望，多则删除，少则通过模板创建 deployment、rs、rc 特点： 确保Pod资源对象的数量精准 确保Pod健康运行 弹性伸缩 同样，它也可以通过yaml或json格式的资源清单来创建，其中spec字段一般嵌套一下字段 replicas：期望的Pod对象副本数量 selector：当前控制器匹配Pod对象副本的标签 template：Pod副本的模板 与RC相比而言，RS不仅支持基于等值的标签选择器，而且还支持集合的标签选择器 标签：解决同类型的资源对象越来越多，为了更好的管理，按照标签分组 常用标签分类： release（版本）：stable（稳定版）、canary（金丝雀）、beta（测试版） environment（环境变量）：dev（开发）、qa（测试）、production（生产） application（应用）：ui、as（application software应用软件）、pc、sc tier（架构层级）：frontend（前端）、backend（后端）、cache（缓存） partition（分区）：customerA（客户A）、customoerB（客户B） track（品控级别）：daily（每天）、weekly（每周） 标签要做到：见名知意 //通过–show-labels显示资源对象的标签 1[root@master ~]# kubectl get pod --show-labels //通过-l选项查看仅含有包含某个标签的资源 1[root@master ~]# kubectl get pod -l env //通过-L显示某个键对应的值 1[root@master ~]# kubectl get pod -L env //给Pod资源添加标签 1[root@master ~]# kubectl label pod label app&#x3D;pc //删除标签 1[root@master ~]# kubectl label pod label app- //修改标签 1[root@master ~]# kubectl label pod label env&#x3D;dev --overwrite 如果标签有多个，标签选择器选择其中一个，也可以关联成功，相反，如果选择器有多个，那标签必须完全满足条件，才可以关联成功 标签选择器：标签的查询过滤条件 基于等值关系的（equality-based）：”=”，”==”，”!=” =” 前面两个都是相等，最后是不等 基于集合关系（set-based）：in、notin、exists三种 例子： 123456selector: matchLables: app: nginx metchExpressions: - &#123;key: name,operator: In,values: [zhangsan,lisi]&#125; - &#123;key: age,operator: Exists,values:&#125; matchLabels：指定键值对来表示的标签选择器 matchExpressions：基于表达式来指定的标签选择器，选择器列表间为”逻辑与”关系；使用In或者Notin操作时，其values不强制要求为非控的字符串，而使用Exists或DosNotExist时，其values必须为空 ) 使用标签选择器的逻辑： 同时指定的多个选择器之间的逻辑关系为”与”操作 使用空值的标签选择器意味着每个资源对象都将被选择中 空的标签选择器无法选中任何资源 DaemonSet 它也是一种Pod控制器 使用场景：如果必须将Pod运行再固定的某个或某几个节点，且要优先其他Pod的启动，通常情况下，默认会每个节点都会运行，并且只能运行一个Pod，这种情况推荐使用DaemonSet资源对象 监控程序： 日志收集程序： 运行一个web服务，在每一个节点都运行一个Pod 1234567891011121314[root@master ~]# vim daemonset.yamlkind: DaemonSetapiVersion: extensions&#x2F;v1beta1metadata: name: test-dsspec: template: metadata: labels: name: test-ns spec: containers: - name: test-ns image: httpd:v1 RC、RS、Deployment、DaemonSet，Pod控制器。statfulSet（有状态）、Ingress。Pod RBAC：基于用户的认证授权机制","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"虚拟机的克隆","slug":"虚拟机的克隆","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.318Z","comments":true,"path":"虚拟机的克隆.html","link":"","permalink":"http://pdxblog.top/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%85%8B%E9%9A%86.html","excerpt":"","text":"克隆：克隆的两种方式：1、手动克隆（完整克隆）：test01———-&gt;test02：（将test01克隆为test02）1）复制xml配置文件： 1234[root@localhost ~]# cd &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;[root@localhost qemu]# cp test01.xml test02.xml或者[root@kvm ~]# virsh dumpxml test01 &gt; test02.xml 2）复制磁盘文件： 12[root@localhost qemu]# cd &#x2F;kvm-vm&#x2F;[root@localhost kvm-vm]# cp centos.raw test02.raw 3）修改配置文件并重新生成一个虚拟机： 12[root@localhost kvm-vm]# cd &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;[root@localhost qemu]# vim test02.xml a:name字段b:删除UUIDc:删除mac addressd:修改磁盘路径以及名称 1[root@localhost qemu]# virsh define test02.xml 链接克隆：//做一个链接的磁盘，然后第二个新的虚拟机更改xml配置文件，磁盘信息指定新的链接磁盘 1[root@localhost kvm-vm]# qemu-img create -f qcow2 -b centos.raw test02.qcow2 自动克隆（完整克隆）: 1[root@localhost ~]# virt-clone --auto-clone -o test02 -n test03 //-o:表示克隆谁，-n：指定名称","categories":[{"name":"KVM","slug":"KVM","permalink":"http://pdxblog.top/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://pdxblog.top/tags/KVM/"}]},{"title":"虚拟机的迁移","slug":"虚拟机的迁移","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.334Z","comments":true,"path":"虚拟机的迁移.html","link":"","permalink":"http://pdxblog.top/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E8%BF%81%E7%A7%BB.html","excerpt":"","text":"虚拟机的迁移： 冷迁移（静态迁移）： //服务器需要关闭kvm01：192.168.1.100kvm02：192.168.1.200 两台机器防火墙全部关闭，禁用selinux 12[root@localhost ~]# lsmod | grep kvm &#x2F;&#x2F;查看是否支持kvm[root@localhost ~]# systemctl status libvirtd &#x2F;&#x2F;查看libvirtd服务是否正常 //迁移和克隆差不多，都是需要对磁盘文件和xml配置文件进行操作 123[root@kvm01 ~]# scp &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;test01.xml root@192.168.1.200:&#x2F;etc&#x2F;libvirt&#x2F;qemu [root@kvm01 ~]# scp &#x2F;kvm-vm&#x2F;centos.raw root@192.168.1.200:&#x2F;kvm-vm&#x2F;[root@kvm02 ~]# virsh define &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;test01.xml 热迁移（动态迁移）：删除所有的KVM虚拟机kvm01：192.168.1.100kvm02：192.168.1.200NFS：192.168.1.129 1）在NFS服务器上面操作： 123456789[root@NFS ~]# yum -y install nfs-utils[root@NFS ~]# mkdir &#x2F;kvmshare &#x2F;&#x2F;创建共享文件夹[root@NFS ~]# vim &#x2F;etc&#x2F;exports &#x2F;&#x2F;编辑共享文件夹权限[root@NFS ~]# cat &#x2F;etc&#x2F;exports&#x2F;kvmshare *(rw,sync,no_root_squash)[root@NFS ~]# systemctl start rpcbind &#x2F;&#x2F;远程传输控制协议[root@NFS ~]# systemctl enable rpcbind[root@NFS ~]# systemctl start nfs-server[root@NFS ~]# systemctl enable nfs-server //确保两台KVM服务器能看到 12[root@kvm01 ~]# showmount -e 192.168.1.129[root@kvm02 ~]# showmount -e 192.168.1.129 2）KVM01上基于NFS服务创建虚拟机添加新的存储池：名称：nfsshare类型：netfs目标路径：/opt/nfsshare（本机挂在的目录，目录默认没有，但会自己创建）主机名:192.168.1.129（nfs-server IP address）源路径：/kvmshare（nfs-server上的共享目录） 验证nfs服务是否正常： 123[root@kvm01 ~]# touch &#x2F;opt&#x2F;nfsshare&#x2F;test[root@NFS ~]# ls &#x2F;kvmshare&#x2F;test 创建存储卷：名称：centos7最大容量：10G //存储池和存储卷完成之后，直接创建虚拟机，并最小化安装选择之前的创建的iso镜像以及刚才创建的存储池和存储卷 配置虚拟机使用bridge桥接网络，使其能够ping通外网，并且在这里我们执行一个ping百度的命令，并让他保持一直是ping着的状态，用来模拟迁移到kvm02上服务不中断： 12345678[root@kvm01 ~]# virsh destroy centos7.0[root@kvm01 ~]# systemctl stop NetworkManager[root@kvm01 ~]# virsh iface-bridge ens33 br0[root@kvm01 ~]# virsh edit centos7.0&lt;interface type&#x3D;&#39;bridge&#39;&gt;&lt;mac address&#x3D;&#39;52:54:00:12:80:97&#39;&#x2F;&gt;&lt;source bridge&#x3D;&#39;br0&#39;&#x2F;&gt;[root@kvm01 ~]# virsh start centos7.0 配置IP为DHCP自动获取 在KVM02上操作，创建存储池：名称：nfsshare类型：netfs目标路径：/opt/nfsshare（本机挂在的目录，目录默认没有，但会自己创建）主机名:192.168.1.129（nfs-server IP address）源路径：/kvmshare（nfs-server上的共享目录）创建完之后会看到之前在KVM01上创建的test文件和centos.qcow2的存储卷 在KVM01上连接KVM02：右上角—文件—添加连接—连接到远程主机—方法：ssh—用户名：root—-主机名：192.168.1.200（KVM02的IP）会提示安装openssh-askpass，直接在KVM01和KVm02上安装： 12[root@kvm01 ~]# yum -y install openssh-askpass[root@kvm02 ~]# yum -y install openssh-askpass //因为KVM01使用的是bridge br0网卡，所以我们需要在KVM02上创建同样的网卡br0，用来支持虚拟机 12[root@kvm02 ~]# systemctl stop NetworkManager[root@kvm02 ~]# virsh iface-bridge ens33 br0 接下来直接在virt-manager管理器中迁移就可以了，迁移完成之后，保证我么的ping命令是不中断的，就表示实验完成了右键centos7.0—迁移—地址：192.168.1.200（KVM02的IP）—高级选项—-勾选允许不可靠—-迁移如果出现错误解决办法：把KVM01和KVM02上挂载的目录给一个777的权限，保证双方root用户有权限调用目录 12[root@kvm01 ~]# chmod 777 &#x2F;opt&#x2F;[root@kvm02 ~]# chmod 777 &#x2F;opt&#x2F; 迁移完成后在KVM02上面查看ping命令是否中断","categories":[{"name":"KVM","slug":"KVM","permalink":"http://pdxblog.top/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://pdxblog.top/tags/KVM/"}]},{"title":"Deployment","slug":"Deployment","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:00:44.044Z","comments":true,"path":"Deployment.html","link":"","permalink":"http://pdxblog.top/Deployment.html","excerpt":"","text":"Deployment 12345678910111213141516apiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: replicas: 4 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v1 ports: - containerPort: 80 PS：注意，在Deployment资源对象中，可以添加Port字段，但此字段仅供用户查看，并不实际生效 service 12345678910111213kind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort selector: app: web ports: - protocol: TCP port: 80 &#x2F;&#x2F;clusterIP的端口 targetPort: 80 &#x2F;&#x2F;Pod的端口 nodePort: 30123 SNAT：Source NAT（源地址转换） DNAT：Destination NAT（目标地址转换） MASQ：动态的源地址转换 service实现的负载均衡：默认使用的是iptables规则 第二种方案：IPVS 回滚到指定版本 准备三个版本所使用的私有镜像，来模拟每次升级到不同的镜像 Deployment1.yaml 1234567891011121314151617[root@master ~]# vim deployment1.yamlapiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v1 ports: Deployment2.yaml 123456789101112131415161718[root@master ~]# vim deployment2.yamlapiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v2 ports: - containerPort: 80 Deployment3.yaml 123456789101112131415161718[root@master ~]# vim deployment3.yamlapiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v3 ports: - containerPort: 80 此处3个yaml文件，指定不同版本的镜像 //运行一个服务，并记录一个版本信息 1[root@master ~]# kubectl apply -f deployment1.yaml --record //查看有哪些版本信息 1[root@master ~]# kubectl rollout history deployment test-web //运行并升级Deployment资源，并记录版本信息 12[root@master ~]# kubectl apply -f deployment2.yaml --record[root@master ~]# kubectl apply -f deployment3.yaml --record //此时可以运行一个关联的Service资源去验证升级是否成功 123[root@master ~]# kubectl apply -f web-svc.yaml[root@master ~]# curl 10.96.179.50&lt;h1&gt;zhb | test-web | httpd | v3&lt;h1&gt; //回滚到指定版本 用label控制Pod的位置 //添加节点你标签 12[root@master ~]# kubectl label nodes node02 disk&#x3D;ssd[root@master ~]# kubectl get nodes --show-labels | grep node02 12345678910111213141516171819apiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: revisionHistoryLimit: 10 &#x2F;&#x2F;版本历史限制 replicas: 3 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v1 ports: - containerPort: 80 nodeSelector: &#x2F;&#x2F;添加节点选择器 disk: ssd &#x2F;&#x2F;和标签内容一致 12345[root@master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-web-d58c9f847-bhswj 1&#x2F;1 Running 0 28s 10.244.2.14 node02 &lt;none&gt; &lt;none&gt;test-web-d58c9f847-k58nj 1&#x2F;1 Running 0 28s 10.244.2.13 node02 &lt;none&gt; &lt;none&gt;test-web-d58c9f847-vt7r5 1&#x2F;1 Running 0 28s 10.244.2.15 node02 &lt;none&gt; &lt;none&gt; //删除节点标签 1[root@master ~]# kubectl label nodes node02 disk-","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"Docker三剑客之docker-compose+wordpress的博客搭建","slug":"Docker三剑客之docker-compose+wordpress的博客搭建","date":"2020-01-23T16:00:00.000Z","updated":"2020-02-03T02:45:12.557Z","comments":true,"path":"Docker三剑客之docker-compose+wordpress的博客搭建.html","link":"","permalink":"http://pdxblog.top/Docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-compose+wordpress%E7%9A%84%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA.html","excerpt":"","text":"Docker三剑客：docker machine：自动化部署多台dockerHostdocker-compose：它可以同时控制多个容器docker swarm：从单个的服务向集群的形式发展为什么要做集群：高可用、高性能、高并发：防止单点故障 Docker三剑客之docker-composedocker容器的编排工具： 解决相互有依赖关系的多个容器的管理 //验证已有docker-compose命令 12[root@localhost ~]# docker-compose -vdocker-compose version 1.25.0, build 0a186604 docker-compose的配置文件实例 通过识别一个docker-compose.yml的配置文件，去管理容器 //设置tab键的空格数量 123[root@localhost ~]# vim .vimrcset tabstop&#x3D;2[root@localhost ~]# source .vimrc 12345678910111213[root@localhost ~]# mkdir compose_test[root@localhost ~]# cd compose_test&#x2F;[root@localhost compose_test]# vim docker-compose.ymlversion: &quot;3&quot;services: nginx: container_name: web-nginx image: nginx restart: always ports: - 90:80 volumes: - .&#x2F;webserver:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html 第一个部分：version：指定格式的版本 第二部分：services：定义服务，（想要运行什么样的容器） //运行docker-compose规定的容器： PS：在执行这条命令的当前目录下，也需要有一个docker-compose.yml的配置文件，并且通常只有一个 12345[root@localhost compose_test]# docker-compose up -d[root@localhost compose_test]# cd webserver&#x2F;[root@localhost webserver]# echo 123456 &gt; index.html[root@localhost webserver]# curl 127.0.0.1:90123456 //停止运行 1[root@localhost compose_test]# docker-compose stop //重启 1[root@localhost compose_test]# docker-compose restart //如果在当前目录没有docker-compose.yml这个文件，可以通过-f来指定docker-compose.yml文件位置 1[root@localhost ~]# docker-compose -f compose_test&#x2F;docker-compose.yml start 并且，在运行container的过程中，还可以支持Dockerfile 1234567891011121314151617181920212223[root@localhost compose_test]# vim Dockerfile[root@localhost compose_test]# cat Dockerfile FROM nginxADD webserver &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;htm[root@localhost compose_test]# vim docker-compose.ymlversion: &quot;3&quot;services: nginx: build: . container_name: web-nginx image: new-nginx:v1.0 restart: always ports: - 90:80[root@localhost compose_test]# docker-compose stop[root@localhost compose_test]# docker-compose rm[root@localhost compose_test]# docker-compose up -d[root@localhost compose_test]# curl 127.0.0.1:90123456[root@localhost compose_test]# cd webserver&#x2F;[root@localhost webserver]# echo 654321 &gt; index.html [root@localhost webserver]# curl 127.0.0.1:90123456 搭建wordpress的博客12345678910111213141516171819202122232425[root@localhost ~]# mkdir wordpress[root@localhost ~]# docker load &lt; wordpress.tar[root@localhost ~]# cd wordpress&#x2F;[root@localhost wordpress]# vim docker-compose.ymlversion: &quot;3.1&quot;services: wordpress: image: wordpress restart: always ports: - 8080:80 environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: 123.com WORDPRESS_DB_NAME: wordpress db: image: mysql:5.7 restart: always environment: MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: 123.com MYSQL_ROOT_PASSWORD: 123.com[root@localhost wordpress]# docker-compose up -d //浏览器访问本机的8080端口：（192.168.1.70:8080） ) ) ) )","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/tags/Docker/"}]},{"title":"Docker实现服务发现","slug":"Docker实现服务发现","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.209Z","comments":true,"path":"Docker实现服务发现.html","link":"","permalink":"http://pdxblog.top/Docker%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0.html","excerpt":"","text":"Docker实现服务发现Docker + Consul + registrator实现服务发现 Consul：分布式、高可用的，服务发现和配置的工具，数据中心 Registrator：负责收集dockerhost上，容器服务的信息，并且发送给consul Consul-template：根据编辑好的模板生成新的nginx配置文件，并且负责加载nginx配置文件 实验环境 docker01 192.168.1.70 docker02 192.168.1.60 docker03 192.168.1.50 关闭防火墙和selinux，并修改主机名 1234[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# setenforce 0[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - 1）docker01上，启动consul服务 123[root@docker01 ~]# unzip consul_1.5.1_linux_amd64.zip[root@docker01 ~]# mv consul &#x2F;usr&#x2F;local&#x2F;bin&#x2F;[root@docker01 ~]# chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;consul //以二进制的方式部署consul，并启动，身份为leader 12345[root@docker01 ~]# consul agent -server -bootstrap \\&gt; -ui -data-dir&#x3D;&#x2F;var&#x2F;lib&#x2F;consul-data \\&gt; -bind&#x3D;192.168.1.70 \\&gt; -client&#x3D;0.0.0.0 \\&gt; -node&#x3D;master //这时这个命令会占用终端，可以使用nohup命令让它保持后台运行 1[root@docker01 ~]# nohup consul agent -server -bootstrap -ui -data-dir&#x3D;&#x2F;var&#x2F;lib&#x2F;consul-data -bind&#x3D;192.168.1.70 -client&#x3D;0.0.0.0 -node&#x3D;master &amp; PS: -bootstrap：加入这个选项时，一般都在server单节点的时候用，自选举为leader -ui：开启内部web界面 -data-dir：key/volume数据存储位置 -bind：指定开启服务的IP -client：指定访问的客户端 -node：只当集群内通信使用的名称，默认是用主机名命名的 PS：开启的端口 8300：集群节点 8301：集群内部的访问 8302：跨数据中心的通信 8500：web ui界面 8600：使用dns协议查看节点信息的端口 //查看conusl的信息 12[root@docker01 ~]# consul infoleader_addr &#x3D; 192.168.1.70:8300 &#x2F;&#x2F;这个对我们比较有用，其他的都是一些它的算法 //查看集群内成员的信息 123[root@docker01 ~]# consul membersNode Address Status Type Build Protocol DC Segmentmaster 192.168.1.70:8301 alive server 1.5.1 2 dc1 &lt;all&gt; 2）docker02、docker03，加入consul集群 这里我们采用容器的方式去运行consul服务 //docker02 12[root@docker02 ~]# docker load &lt; myprogrium-consul.tar[root@docker02 ~]# docker run -d --name consul -p 8301:8301 -p 8301:8301&#x2F;udp -p 8500:8500 -p 8600:8600 -p 8600:8600&#x2F;udp --restart always progrium&#x2F;consul:latest -join 192.168.1.70 -advertise 192.168.1.60 -client 0.0.0.0 -node&#x3D;node01 //docker03 12[root@docker03 ~]# docker load &lt; myprogrium-consul.tar[root@docker03 ~]# docker run -d --name consul -p 8301:8301 -p 8301:8301&#x2F;udp -p 8500:8500 -p 8600:8600 -p 8600:8600&#x2F;udp --restart always progrium&#x2F;consul:latest -join 192.168.1.70 -advertise 192.168.1.50 -client 0.0.0.0 -node&#x3D;node02 //在docker01上就能看到加入的信息 12342019&#x2F;12&#x2F;26 09:50:25 [INFO] serf: EventMemberJoin: node01 192.168.1.602019&#x2F;12&#x2F;26 09:50:25 [INFO] consul: member &#39;node01&#39; joined, marking health alive2019&#x2F;12&#x2F;26 09:53:06 [INFO] serf: EventMemberJoin: node02 192.168.1.502019&#x2F;12&#x2F;26 09:53:06 [INFO] consul: member &#39;node02&#39; joined, marking health alive 12345[root@docker01 ~]# consul membersNode Address Status Type Build Protocol DC Segmentmaster 192.168.1.70:8301 alive server 1.5.1 2 dc1 &lt;all&gt;node01 192.168.1.60:8301 alive client 0.5.2 2 dc1 &lt;default&gt;node02 192.168.1.50:8301 alive client 0.5.2 2 dc1 &lt;default&gt; //浏览器访问consul服务，验证集群信息 192.168.1.70:8500 ) 3)下载部署consul-template //在docker01上导入consul-template_0.19.5_linux_amd64.zip 123[root@docker01 ~]# unzip consul-template_0.19.5_linux_amd64.zip[root@docker01 ~]# mv consul-template &#x2F;usr&#x2F;local&#x2F;bin&#x2F;[root@docker01 ~]# chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;consul-template 4）docker02、docker03上部署registrator服务 registrator是一个能自动发现docker container提供的服务，并在后端服务注册中心注册服务或取消服务的工具，后端注册中心支持consul、etcd、skydns2、zookeeper等 //docker02 1234567[root@docker02 ~]# docker load &lt; myregistrator.tar[root@docker02 ~]# docker run -d \\&gt; --name registrator \\&gt; -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;tmp&#x2F;docker.sock \\&gt; --restart always \\&gt; gliderlabs&#x2F;registrator \\&gt; consul:&#x2F;&#x2F;192.168.1.60:8500 //运行一个nginx容器 123[root@docker02 ~]# docker run -d -P --name test nginx:latestb0665dcbd6c5 nginx:latest &quot;nginx -g &#39;daemon of…&quot; 10 seconds ago Up 9 seconds 0.0.0.0:32768-&gt;80&#x2F;tcp &#x2F;&#x2F;映射的端口为32768 //回到浏览器 ) ) //docker03 1234567[root@docker03 ~]# docker load &lt; myregistrator.tar[root@docker02 ~]# docker run -d \\&gt; --name registrator \\&gt; -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;tmp&#x2F;docker.sock \\&gt; --restart always \\&gt; gliderlabs&#x2F;registrator \\&gt; consul:&#x2F;&#x2F;192.168.1.50:8500 ) 5）docker01部署一个nginx服务 //依赖环境 [root@docker01 ~]# yum -y install zlib-devel openssl-devel pcre-devel 1234567891011121314[root@docker01 ~]# useradd -M -s &#x2F;sbin&#x2F;nologin nginx[root@docker01 ~]# tar zxf nginx-1.14.0.tar.gz [root@docker01 ~]# cd nginx-1.14.0&#x2F;[root@docker01 nginx-1.14.0]# .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx \\&gt; --user&#x3D;nginx --group&#x3D;nginx \\&gt; --with-http_stub_status_module \\&gt; --with-http_realip_module \\&gt; --with-pcre --with-http_ssl_module[root@docker01 nginx-1.14.0]# make &amp;&amp; make install[root@docker01 nginx-1.14.0]# ln -s &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;* &#x2F;usr&#x2F;local&#x2F;sbin&#x2F;[root@docker01 nginx-1.14.0]# nginx -tnginx: the configuration file &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf syntax is oknginx: configuration file &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf test is successful[root@docker01 nginx-1.14.0]# nginx PS： 这里nginx作为反向代理，代理后端docker02、docker03上nignx的容器服务，所以我们先去docker02、docker03上部署一些服务，为了方便等会看到负载的效果，所以我们运行完成容器之后，做一个主界面内容的区分 docker02：web01 web02 docker03：web03 web04 //docker02 12345678910111213141516[root@docker02 ~]# docker run -itd --name web01 -P nginx:latest [root@docker02 ~]# docker exec -it web01 &#x2F;bin&#x2F;bashroot@dac0cc15f3fe:&#x2F;# cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;root@dac0cc15f3fe:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# echo &quot;This web caontainer in dockek02-web01&quot; &gt; index.html root@dac0cc15f3fe:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# cat index.html This web caontainer in dockek02-web01[root@docker02 ~]# docker run -itd --name web02 -P nginx:latest [root@docker02 ~]# docker exec -it web02 &#x2F;bin&#x2F;bashroot@26d622553e5e:&#x2F;# cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;root@26d622553e5e:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# echo &quot;This web caontainer in dockek02-web02&quot; &gt; index.html root@26d622553e5e:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# cat index.html This web caontainer in dockek02-web02[root@docker02 ~]# curl 127.0.0.1:32769This web caontainer in dockek02-web01[root@docker02 ~]# curl 127.0.0.1:32770This web caontainer in dockek02-web02 //docker03 12345678910111213141516[root@docker03 ~]# docker run -itd --name web03 -P nginx:latest [root@docker03 ~]# docker exec -it web03 &#x2F;bin&#x2F;bashroot@a10f25a91edf:&#x2F;# cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;root@a10f25a91edf:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# echo &quot;This web caontainer in dockek03-web03&quot; &gt; index.html root@a10f25a91edf:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# cat index.html This web caontainer in dockek03-web03[root@docker03 ~]# docker run -itd --name web04 -P nginx:latest [root@docker03 ~]# docker exec -it web04 &#x2F;bin&#x2F;bashroot@6d30a445c9b8:&#x2F;# cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;root@6d30a445c9b8:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# echo &quot;his web caontainer in dockek03-web04&quot; &gt; index.html root@6d30a445c9b8:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# cat index.html This web caontainer in dockek03-web04[root@docker03 ~]# curl 127.0.0.1:32768This web caontainer in dockek03-web03[root@docker03 ~]# curl 127.0.0.1:32769This web caontainer in dockek03-web04 更改nginx的配置文件 12345678910111213141516171819202122232425[root@docker01 ~]# cd &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;[root@docker01 nginx]# mkdir consul[root@docker01 nginx]# cd consul&#x2F;[root@docker01 consul]# pwd&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul[root@docker01 consul]# vim nginx.ctmpl[root@docker01 consul]# cat nginx.ctmpl upstream httpd_backend &#123; &#123;&#123;range service &quot;nginx&quot;&#125;&#125; server &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;; &#123;&#123; end &#125;&#125;&#125;server &#123; listen 8000; server_name localhost; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;http_backend; &#125;&#125;[root@docker01 ~]# vim &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf&#x2F;&#x2F;在文件最后，也就是倒数第二行添加：include &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;*.conf;&#x2F;&#x2F;使nginx的主配置文件能够识别到新产生的配置文件[root@docker01 ~]# nginx -s reload //使用consul-template命令，根据模板生产的配置文件，并重新加载nginx的配置文件 1[root@docker01 consul]# consul-template -consul-addr 192.168.1.70:8500 -template &quot;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;nginx.ctmpl:&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;vhost.conf:&#x2F;usr&#x2F;local&#x2F;sbin&#x2F;nginx -s reload&quot; //这时这个命令会占用终端，可以使用nohup命令让它保持后台运行 1[root@docker01 consul]# nohup consul-template -consul-addr 192.168.1.70:8500 -template &quot;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;nginx.ctmpl:&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;vhost.conf:&#x2F;usr&#x2F;local&#x2F;sbin&#x2F;nginx -s reload&quot; &amp; //此时，应该能够看到，新生产的vhost.conf配置文件已经生效，访问本机的8000端口可以得到不同容器提供的服务 12345678910111213141516171819202122232425[root@docker01 ~]# cd &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;[root@docker01 consul]# lsnginx.ctmpl vhost.conf[root@docker01 consul]# cat vhost.confupstream httpd_backend &#123; server 192.168.1.60:32768; server 192.168.1.60:32769; server 192.168.1.60:32770; server 192.168.1.50:32768; server 192.168.1.50:32769; &#125;server &#123; listen 8000; server_name localhost; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;httpd_backend; &#125;&#125; 12345678[root@docker01 consul]# curl localhost:8000This web caontainer in dockek02-web01[root@docker01 consul]# curl localhost:8000This web caontainer in dockek02-web02[root@docker01 consul]# curl localhost:8000This web caontainer in dockek03-web03[root@docker01 consul]# curl localhost:8000This web caontainer in dockek03-web04 当然，这时不管是添加新的nginx的web容器，或是删除，生产的配置文件都会时时更新，这是我们在运行consul-template这条命令最后添加：/usr/local/sbin/nginx -s reload它的作用 //删除之前的test容器，查看vhost文件 123456789101112[root@docker02 ~]# docker rm -f test[root@docker01 consul]# cat vhost.conf upstream httpd_backend &#123; server 192.168.1.60:32769; server 192.168.1.60:32770; server 192.168.1.50:32768; server 192.168.1.50:32769;&#125; //在运行一个web05，查看vhost文件的变化 12345678910111213[root@docker02 ~]# docker run -itd --name web05 -P nginx:latest upstream httpd_backend &#123; server 192.168.1.60:32769; server 192.168.1.60:32770; server 192.168.1.60:32771; server 192.168.1.50:32768; server 192.168.1.50:32769;&#125; ) 1、docker01主机上以二进制包的方式部署consul服务并后台运行，其身份为leader 2、docker02、docker03以容器的方式运行consul服务，并加入到docker01的consul群集中 3、在主机docker02、docker03上后台运行registrator容器，使其自动发现docker容器提供的服务，并发送给consul 4、在docker01上部署Nginx，提供反向代理服务，docker02、docker03主机上基于Nginx镜像，各运行两个web容器，提供不同的网页文件，以便测试效果 5、在docker01上安装consul-template命令，将收集到的信息（registrator收集到容器的信息）写入template模板中，并且最终写入Nginx的配置文件中 6、至此，实现客户端通过访问Nginx反向代理服务器（docker01），获得docker02、docker03服务器上运行的Nginx容器提供的网页文件 Consul：分布式、高可用的，服务发现和配置的工具，数据中心 Registrator：负责收集dockerhost上，容器服务的信息，并且发送给consul registrator是一个能自动发现docker container提供的服务，并在后端服务注册中心注册服务或取消服务的工具，后端注册中心支持consul、etcd、skydns2、zookeeper等 Consul-template：根据编辑好的模板生成新的nginx配置文件，并且负责加载nginx配置文件","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/tags/Docker/"}]},{"title":"Docker数据持久化","slug":"Docker数据持久化","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.224Z","comments":true,"path":"Docker数据持久化.html","link":"","permalink":"http://pdxblog.top/Docker%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96.html","excerpt":"","text":"Docker数据持久化为什么要做数据持久化： 因为Docker容器本身就是一个进程，可能会因为某些原因，或某些错误导致进程被杀死，这样数据就会丢失。 Docker容器是有生命周期的，生命周期结束，进程也会被杀死，数据就会丢失，因此需要做数据持久化，保证数据不会丢失 Storage Driver数据存储 Centos7版本的Docker，Storage Driver为：Overlay2； backing filesystem：xfs Data VolumeBind mount持久化存储：本质上是DockerHost文件系统中的目录或文件，能够直接被Mount到容器的文件系统中，在运行容器时，可以通过-v实现 特点： Data Volume是目录或文件，不能是没有格式化的磁盘（块设备） 容器可以读写volume中的数据 volume数据可以永久保存，即使用它的容器已经被销毁 小实验： 运行一个nginx服务，做数据持久化 12345678910[root@docker01 ~]# mkdir html[root@docker01 ~]# cd html&#x2F;[root@docker01 html]# echo &quot;This is a testfile in dockerHost.&quot; &gt; index.html[root@docker01 html]# cat index.html This is a testfile in dockerHost.[root@docker01 ~]# docker run -itd --name testweb -v &#x2F;root&#x2F;html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html nginx:latest[root@docker01 ~]# docker inspect testweb&quot;Gateway&quot;: &quot;172.17.0.1&quot;,[root@docker01 ~]# curl 172.17.0.2This is a testfile in dockerHost. PS:DockerHost上需要挂在的源文件或目录，必须是已经存在的，否则，当做一个目录挂在到容器中 默认挂载到容器内的文件，容器是有读写权限，可以在运行容器时-v后边加”:ro”限制容器的写入权限 并且还可以挂在单独文件到容器内部，一般它的使用场景是：如果不想对整个目录进行覆盖，而只希望添加某个文件，就可以使用挂载单个文件 Docker Manager Volume1[root@docker01 ~]# docker run -itd --name t2 -P -v &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html nginx:latest 删除容器的操作，默认不会对dockerHost上的文件操作，如果想要在删除容器时把源文件也删除，可以在删除容器时添加-v选型（一般不推荐使用这种方式，因为文件有可能被其他容器使用） 容器与容器的数据共享：volume container：给其他容器提供volume存储卷的容器，并且可以提供bind mount，也可以提供docker manager volume //创建一个vc_data容器： 123[root@docker01 ~]# docker create --name vc_data \\&gt; -v ~&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\&gt; -v &#x2F;other&#x2F;useful&#x2F;tools busybox 容器的跨主机数据共享 docker01 dcoker02 docker03 httpd httpd nfs 要求： docker01和docker02的主目录是一样的 //docker03的操作： 1234567891011121314151617[root@docker03 ~]#yum -y install nfs-utils[root@docker03 ~]# mkdir &#x2F;datashare[root@docker03 ~]# vim &#x2F;etc&#x2F;exports[root@docker03 ~]# cat &#x2F;etc&#x2F;exports&#x2F;datashare *(rw,sync,no_root_squash)[root@docker03 ~]# systemctl start rpcbind[root@docker03 ~]# systemctl enable rpcbind[root@docker03 ~]# systemctl start nfs-server[root@docker03 ~]# systemctl enable nfs-server[root@docker03 ~]# vim &#x2F;datashare&#x2F;index.html[root@docker03 ~]# cat &#x2F;datashare&#x2F;index.html &lt;div id&#x3D;&quot;datetime&quot;&gt; &lt;script&gt; setInterval(&quot;document.getElementById(&#39;datetime&#39;).innerHTML&#x3D;new Date().toLocaleString();&quot;, 1000); &lt;&#x2F;script&gt;&lt;&#x2F;div&gt;bdqn-webshare //验证 123456[root@docker01 ~]# showmount -e 192.168.1.60Export list for 192.168.1.60:&#x2F;datashare *[root@docker02 ~]# showmount -e 192.168.1.60Export list for 192.168.1.60:&#x2F;datashare * //docker01的操作 12345678910[root@docker01 ~]# mkdir &#x2F;htdocs[root@docker01 ~]# mount -t nfs 192.168.1.60:&#x2F;datashare &#x2F;htdocs&#x2F;&#x2F;&#x2F;-t：指定类型（type）[root@docker01 ~]# cat &#x2F;htdocs&#x2F;index.html &lt;div id&#x3D;&quot;datetime&quot;&gt; &lt;script&gt; setInterval(&quot;document.getElementById(&#39;datetime&#39;).innerHTML&#x3D;new Date().toLocaleString();&quot;, 1000); &lt;&#x2F;script&gt;&lt;&#x2F;div&gt;bdqn-webshare //docker02的操作 123456789[root@docker02 ~]# mkdir &#x2F;htdocs[root@docker02 ~]# mount -t nfs 192.168.1.60:&#x2F;datashare &#x2F;htdocs&#x2F;[root@docker02 ~]# cat &#x2F;htdocs&#x2F;index.html &lt;div id&#x3D;&quot;datetime&quot;&gt; &lt;script&gt; setInterval(&quot;document.getElementById(&#39;datetime&#39;).innerHTML&#x3D;new Date().toLocaleString();&quot;, 1000); &lt;&#x2F;script&gt;&lt;&#x2F;div&gt;bdqn-webshare 这里先不考虑将代码写入镜像，先以这种方式，分别在docker01和docker02部署httpd服务 //docker01 12[root@docker01 ~]# docker run -itd --name bdqn-web1 -P -v &#x2F;htdocs&#x2F;:&#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs httpd:latestPS：查看端口映射0.0.0.0:32778-&gt;80&#x2F;tcp bdqn-web1 //docker02 12[root@docker02 ~]# docker run -itd --name bdqn-web2 -P -v &#x2F;htdocs&#x2F;:&#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs httpd:latestPS：查看端口映射0.0.0.0:32768-&gt;80&#x2F;tcp bdqn-web2 此时用浏览器访问，两个web服务的主界面是一样的，但如果NFS服务器上源文件丢失，则两个web服务都会异常 想办法将源数据写入镜像内，在基于镜像做一个vc_data容器，这里因为没有接触到docker-compose和docker swarm等docker编排工具，所以我们在docker01和docker02上手动创建镜像 123456789101112[root@docker01 ~]# cd &#x2F;htdocs&#x2F;[root@docker01 htdocs]# vim Dockerfile[root@docker01 htdocs]# cat Dockerfile FROM busyboxADD index.html &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs&#x2F;index.htmlVOLUME &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs[root@docker01 htdocs]# docker build -t back_data .[root@docker01 htdocs]# docker create --name back_container1 back_data:latest [root@docker01 htdocs]# docker run -itd --name web3 -P --volumes-from back_container1 httpd:latest [root@docker01 htdocs]# pwd&#x2F;htdocs[root@docker01 htdocs]# docker save &gt; back_data.tar back_data:latest //docker02上操作： 123456[root@docker02 ~]# cd &#x2F;htdocs&#x2F;[root@docker02 htdocs]# lsback_data.tar Dockerfile index.html[root@docker02 htdocs]# docker load &lt; back_data.tar [root@docker02 htdocs]# docker create --name back_container2 back_data:latest[root@docker02 htdocs]# docker run -itd --name web4 -P --volumes-from back_container2 httpd:latest 测试： 1[root@docker01 htdocs]# rm -rf index.html 通过浏览器访问，一开始运行的web1和web2容器，无法访问了。web3和web4还是可以访问的。 但是数据无法同步了","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/tags/Docker/"}]},{"title":"Docker跨主机网络方案之MacVlan","slug":"Docker跨主机网络方案之MacVlan","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.271Z","comments":true,"path":"Docker跨主机网络方案之MacVlan.html","link":"","permalink":"http://pdxblog.top/Docker%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%96%B9%E6%A1%88%E4%B9%8BMacVlan.html","excerpt":"","text":"Docker跨主机网络方案之MacVlan实验环境： docker01 192.168.1.70 docker02 192.168.1.50 关闭防火墙和禁用selinux，更该主机名： 1234567[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# systemctl disable firewalld[root@localhost ~]# setenforce 0[root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - macvlan的单网络通信1）打开网卡的混杂模式//需要在docker01和docker02上都进行操作 12[root@docker01 ~]# ip link set ens33 promisc on[root@docker01 ~]# ip link show ens33 2）在docker01上创建macvlan网络 1234在这里插入代码片[root@docker01 ~]# docker network create -d macvlan --subnet 172.22.16.0&#x2F;24 --gateway 172.22.16.1 -o parent&#x3D;ens33 mac_net1[root@docker01 ~]# docker network lsNETWORK ID NAME DRIVER SCOPEe6860af70e90 mac_net1 macvlan local PS:-o parent=绑定在哪张网卡之上3）基于创建的macvlan网络运行一个容器 1[root@docker01 ~]# docker run -itd --name bbox1 --ip 172.22.16.10 --network mac_net1 busybox 4）在docker02上创建macvlan网络，注意与docker01上的macvlan网络一摸一样 1[root@docker02 ~]# docker network create -d macvlan --subnet 172.22.16.0&#x2F;24 --gateway 172.22.16.1 -o parent&#x3D;ens33 mac_net1 5）在docker02上，基于创建的macvlan网络运行一个容器，验证与docker01上容器的通信 1[root@docker02 ~]# docker run -itd --name bbox2 --network mac_net1 --ip 172.22.16.20 busybox macvlan的多网络通信1）docker01和docker02验证内核模块8021q封装macvlan需要解决的问题：基于真实的ens33网卡，生产新的虚拟网卡 123[root@docker01 ~]# modinfo 8021q&#x2F;&#x2F;如果内核模块没有开启，运行下边命令导入一下[root@docker01 ~]# modprobe 8021q 2）基于ens33创建虚拟网卡//修改ens33网卡配置文件： 1234[root@docker01 ~]# cd &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;[root@docker01 network-scripts]# vim ifcfg-ens33&#x2F;&#x2F;修改：BOOTPROTO&#x3D;manual &#x2F;&#x2F;手动模式 //手动添加虚拟网卡配置文件 123456789101112[root@docker01 network-scripts]# cp -p ifcfg-ens33 ifcfg-ens33.10&#x2F;&#x2F;PS：-p 保留源文件或目录的属性[root@docker01 network-scripts]# vim ifcfg-ens33.10BOOTPROTO&#x3D;noneIPADDR&#x3D;192.168.10.10PREFIX&#x3D;24GATEWAY&#x3D;192.168.10.1NAME&#x3D;ens33.10DEVICE&#x3D;ens33.10ONBOOT&#x3D;yesVLAN&#x3D;yes&#x2F;&#x2F;PS：这里注意，IP要和ens33网段做一个区分，保证网关和网段IP的一致性，设备名称和配置文件的一致性，并且打开VLAN支持模式 //创建第二个虚拟网卡配置文件 12345678910[root@docker01 network-scripts]# cp -p ifcfg-ens33.10 ifcfg-ens33.20[root@docker01 network-scripts]# vim ifcfg-ens33.20BOOTPROTO&#x3D;noneIPADDR&#x3D;192.168.20.20PREFIX&#x3D;24GATEWAY&#x3D;192.168.20.1NAME&#x3D;ens33.20DEVICE&#x3D;ens33.20ONBOOT&#x3D;yesVLAN&#x3D;yes 3）docker01上的操作，启用创建的虚拟网卡 12[root@docker01 network-scripts]# ifup ifcfg-ens33.10[root@docker01 network-scripts]# ifup ifcfg-ens33.20 4)基于虚拟网卡，创建macvlan网络 12[root@docker01 ~]# docker network create -d macvlan --subnet 172.16.10.0&#x2F;24 --gateway 172.16.10.1 -o parent&#x3D;ens33.10 mac_net10[root@docker01 ~]# docker network create -d macvlan --subnet 172.16.20.0&#x2F;24 --gateway 172.16.20.1 -o parent&#x3D;ens33.20 mac_net20 5)基于创建的虚拟网卡，创建macvlan网络 12[root@docker02 ~]# docker network create -d macvlan --subnet 172.16.10.0&#x2F;24 --gateway 172.16.10.1 -o parent&#x3D;ens33.10 mac_net10[root@docker02 ~]# docker network create -d macvlan --subnet 172.16.20.0&#x2F;24 --gateway 172.16.20.1 -o parent&#x3D;ens33.20 mac_net20 6)docker02上也创建虚拟网卡，并启用 1234567891011121314151617181920212223[root@docker01 ~]# scp &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33.10 &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33.20 root@192.168.1.50:&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;[root@docker02 network-scripts]# vim ifcfg-ens33BOOTPROTO&#x3D;manual[root@docker02 network-scripts]# vim ifcfg-ens33.10BOOTPROTO&#x3D;noneIPADDR&#x3D;192.168.10.11PREFIX&#x3D;24GATEWAY&#x3D;192.168.10.1NAME&#x3D;ens33.10DEVICE&#x3D;ens33.10ONBOOT&#x3D;yesVLAN&#x3D;yes[root@docker02 network-scripts]# vim ifcfg-ens33.20BOOTPROTO&#x3D;noneIPADDR&#x3D;192.168.20.21PREFIX&#x3D;24GATEWAY&#x3D;192.168.20.1NAME&#x3D;ens33.20DEVICE&#x3D;ens33.20ONBOOT&#x3D;yesVLAN&#x3D;yes[root@docker02 network-scripts]# ifup ifcfg-ens33.10 [root@docker02 network-scripts]# ifup ifcfg-ens33.20 7)基于macvlan网络创建容器，并指定IP地址，不过这里要注意，运行的同期与网络对应的网段相符合，还需要注意IP地址的唯一性 123456&#x2F;&#x2F;docker01[root@docker01 ~]# docker run -itd --name bbox10 --network mac_net10 --ip 172.16.10.10 192.168.1.70:5000&#x2F;busybox:v1 [root@docker01 ~]# docker run -itd --name bbox20 --network mac_net20 --ip 172.16.20.20 192.168.1.70:5000&#x2F;busybox:v1&#x2F;&#x2F;docker02[root@docker02 ~]# docker run -itd --name bbox11 --network mac_net10 --ip 172.16.10.11 192.168.1.70:5000&#x2F;busybox:v1[root@docker02 ~]# docker run -itd --name bbox21 --network mac_net20 --ip 172.16.20.21 192.168.1.70:5000&#x2F;busybox:v1 8)将VMware虚拟机的网络改为桥接9)进入容器测试通信在docker01上进入容器bbox10和docker02上的bbox11进行通信在docker01上进入容器bbox20和docker02上的bbox21进行通信 12345678910[root@docker01 ~]# docker exec -it bbox10 &#x2F;bin&#x2F;sh&#x2F; # ping 172.16.10.11PING 172.16.10.11 (172.16.10.11): 56 data bytes64 bytes from 172.16.10.11: seq&#x3D;0 ttl&#x3D;64 time&#x3D;0.668 ms64 bytes from 172.16.10.11: seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.335 ms[root@docker01 ~]# docker exec -it bbox20 &#x2F;bin&#x2F;sh&#x2F; # ping 172.16.20.21PING 172.16.20.21 (172.16.20.21): 56 data bytes64 bytes from 172.16.20.21: seq&#x3D;0 ttl&#x3D;64 time&#x3D;0.584 ms64 bytes from 172.16.20.21: seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.365 ms","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/tags/Docker/"}]},{"title":"Docker网络","slug":"Docker网络","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.256Z","comments":true,"path":"Docker网络.html","link":"","permalink":"http://pdxblog.top/Docker%E7%BD%91%E7%BB%9C.html","excerpt":"","text":"Docker网络：原生网络12345[root@localhost ~]# docker network lsNETWORK ID NAME DRIVER SCOPEfcc280741b01 bridge bridge local9c09e5a698dc host host local03411a6d716c none null local None：什么都没有的网络 12[root@localhost ~]# docker run -itd --name none --network none busybox:latest[root@localhost ~]# docker exec -it none /bin/sh PS：用到None网络的容器，会发现它只有一个LoopBack回环的网络，没有Mac地址、IP等信息，意味着它不能跟外界通信，是被隔离起来的网络 使用场景： 隔离意味着安全，所以，此网络可以运行一些关于安全方面的验证码、校验码等服务 Bridge：桥接网络 1234[root@localhost ~]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.02428e69e324 no virbr0 8000.525400547d41 yes virbr0-nic docker0：在我们安装docker这个服务的时候，默认就会生产一张docker0的网卡，一般默认IP为172.17.0.1/16 12[root@localhost ~]# docker run -itd --name test1 busybox:latest [root@localhost ~]# docker exec -it test1 /bin/sh 容器默认使用的网络是docker0网络，docker0此时相当于一个路由器，基于此网络的容器，网段都是和docker0一致的 自定义网络自带了一个ContainerDNSserver功能（域名解析） bridge //创建一个bridge网络： 1[root@localhost ~]# docker network create -d bridge my_net //创建两个容器，应用自定义网络（my_net）： 12[root@localhost ~]# docker run -itd --name test3 --network my_net busybox:latest[root@localhost ~]# docker run -itd --name test4 --network my_net busybox:latest PS：自定义网络优点，它可以通过容器的名称通信 12[root@localhost ~]# docker exec -it test3 &#x2F;bin&#x2F;sh&#x2F; # ping test4 //创建一个自定义网络，并且指定网关和网段 1[root@localhost ~]# docker network create -d bridge --subnet 172.20.16.0&#x2F;24 --gateway 172.20.16.1 my_net2 //创建两个容器，应用自定义网络(my_net2)，并指定IP 12[root@localhost ~]# docker run -itd --name test5 --network my_net2 --ip 172.20.16.6 busybox:latest[root@localhost ~]# docker run -itd --name test6 --network my_net2 --ip 172.20.16.8 busybox:latest PS：如果想要给容器指定IP地址，那么自定义网络的时候，必须指定网关gateway和subnet网段选项 //实现不同网段之间的通信，在容器里再添加一块网卡： 1[root@localhost ~]# docker network connect my_net2 test4 ) 让外网能够访问容器的端口映射方法：1）手动指定端口映射关系： 1[root@localhost ~]# docker run -itd --name web1 -p 90:80 nginx:latest 2）从宿主机随机映射端口到容器： 1[root@localhost ~]# docker run -itd --name web2 -p 80 nginx:latest 3)从宿主机随机映射端口到容器，容器内所有暴露的端口，都会一一映射 1[root@localhost ~]# docker run -itd --name web4 -P nginx:latest join容器：container（共享网络协议栈）容器和容器之间 12[root@localhost ~]# docker run -itd --name web5 busybox:latest[root@localhost ~]# docker run -itd --name web6 --network container:web5 busybox:latest 123[root@localhost ~]# docker exec -it web6 &#x2F;bin&#x2F;sh&#x2F; # echo 123456 &gt; &#x2F;tmp&#x2F;index.html&#x2F; # httpd -h &#x2F;tmp&#x2F; 123[root@localhost ~]# docker exec -it web5 &#x2F;bin&#x2F;sh&#x2F; # wget -O - -q 127.0.0.1123456 //这时会发现，两个容器的IP地址一样 PS：这种方法的使用场景： 由于这种网络的特殊特性，一般在运行同一个服务，并且合格服务需要做监控，已经日志收集、或者网络监控的时候，可以选择这种网络 docker的跨主机网络解决方案 overlay（覆盖）的解决方案： 实验环境： docker01：192.168.1.70 docker02：192.168.1.60 docker03：192.168.1.50 暂时不考录防火墙和selinux安全问题 将3台dockerhost防火墙和selinux全部关闭，并分别更改主机名称 12345[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su -[root@docker01 ~]# systemctl stop firewalld[root@docker01 ~]# setenforce 0[root@docker01 ~]# systemctl disable firewalld 在docker01上操作： //运行consul服务：（数据中心–分布式的） 123[root@docker01 ~]# docker load &lt; myprogrium-consul.tar[root@docker01 ~]# docker run -d -p 8500:8500 -h consul --name consul \\ &gt; --restart always progrium&#x2F;consul -server -bootstrap PS:容器产生之后我们可以通过浏览器访问consul服务，验证consul服务是否正常，访问dockerHost加映射端口 ) 修改docker02和docker03的docker配置文件： //将IP和端口的映射关系，写入consul 123[root@docker02 ~]# vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service修改：ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd -H unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock -H tcp:&#x2F;&#x2F;0.0.0.0:2376 --cluster-store&#x3D;consul:&#x2F;&#x2F;192.168.1.70:8500 --cluster-advertise&#x3D;ens33:2376 PS:返回浏览器consul服务界面，找到KEY/VALUE—-&gt;Docker—-&gt;NODES，会看到刚刚加入的docker02和docker03的信息 ) ) ) 在docker02上创建一个自定义网络： 1234[root@docker02 ~]# docker network create -d overlay ov_net1[root@docker02 ~]# docker network lsNETWORK ID NAME DRIVER SCOPEfe92a0eff6a4 ov_net1 overlay global 在docker02上创建网络，我们可以看到它的SCOPE定义的时global（全局），意味着加入到consul这个服务的docker服务，都可以看到我们自定义的网络 同理如果是用此网络创建的容器，会有两张网卡，默认这张网卡是10.0.0.0网段，如果想要docker01也可以看到这个网络，那么也只需在docker01的docker配置文件添加相应内容即可 同理，因为是自定义网络，符合自定义网络的特性，可以直接通过docker容器的名称互相通信，当然也可以在自定义网络的时候，指定它的网段，那么使用此网络的容器也可以指定IP地址 )","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/tags/Docker/"}]},{"title":"Kubernetes集群部署","slug":"Kubernetes集群部署","date":"2020-01-23T16:00:00.000Z","updated":"2020-02-03T02:44:38.847Z","comments":true,"path":"Kubernetes集群部署.html","link":"","permalink":"http://pdxblog.top/Kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2.html","excerpt":"","text":"生产级别的容器编排系统 Kubernetes 是用于自动部署，扩展和管理容器化应用程序的开源系统 k8s 最基本的硬件要求 CPU:双核 Mem：2G 3台dockerhost时间必须同步 Kubeadm工具自动部署k8s集群 //给3台docker命名，禁用swap交换分区 12345678910111213[root@localhost ~]# hostnamectl set-hostname master[root@localhost ~]# su -[root@localhost ~]# hostnamectl set-hostname node01[root@localhost ~]# su -[root@localhost ~]# hostnamectl set-hostname node02[root@localhost ~]# su -[root@master ~]# swapoff -a &#x2F;&#x2F;临时禁用[root@master ~]# free total used free shared buff&#x2F;cache availableMem: 1867292 335448 908540 9256 623304 1290100Swap: 0 0 0&#x2F;&#x2F;永久禁用[root@master ~]# vim &#x2F;etc&#x2F;fstab &#x2F;&#x2F;注释掉swap那一行 //禁用selinux，防火墙，并关闭开机自启（三台都需要） 12345[root@master ~]# vim &#x2F;etc&#x2F;selinux&#x2F;configSELINUX&#x3D;disabled[root@master ~]# setenforce 0[root@master ~]# systemctl stop firewalld[root@master ~]# systemctl disable firewalld //编写hosts文件，设置域名解析 12345678[root@master ~]# vim &#x2F;etc&#x2F;hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.1.70 master192.168.1.50 node01192.168.1.40 node02[root@master ~]# scp &#x2F;etc&#x2F;hosts root@192.168.1.50:&#x2F;etc[root@master ~]# scp &#x2F;etc&#x2F;hosts root@192.168.1.40:&#x2F;etc //设置免密登录 123[root@master ~]# ssh-keygen -t rsa[root@master ~]# ssh-copy-id node01[root@master ~]# ssh-copy-id node02 //打开iptables的桥接功能，开启路由转发 1234567891011121314151617181920212223[root@master ~]# vim &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.confnet.bridge.bridge-nf-call-iptables &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1[root@master ~]# echo net.ipv4.ip_forward &#x3D; 1 &gt;&gt; &#x2F;etc&#x2F;sysctl.conf [root@master ~]# sysctl -pnet.ipv4.ip_forward &#x3D; 1[root@master ~]# sysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf &#x2F;&#x2F;如果这条命令不成功则需要添加一个模块[root@master ~]# modprobe br_netfilternet.bridge.bridge-nf-call-iptables &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1[root@master ~]# scp &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf node01:&#x2F;etc&#x2F;sysctl.d [root@master ~]# scp &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf node02:&#x2F;etc&#x2F;sysctl.d [root@master ~]# scp &#x2F;etc&#x2F;sysctl.conf node02:&#x2F;etc&#x2F; [root@master ~]# scp &#x2F;etc&#x2F;sysctl.conf node01:&#x2F;etc&#x2F;[root@node01 ~]# sysctl -pnet.ipv4.ip_forward &#x3D; 1[root@node01 ~]# sysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf net.bridge.bridge-nf-call-iptables &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1[root@node02 ~]# sysctl -pnet.ipv4.ip_forward &#x3D; 1[root@node02 ~]# sysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf net.bridge.bridge-nf-call-iptables &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1 //获取yum源 123456789101112[root@master ~]# cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo&gt; [kubernetes]&gt; name&#x3D;Kubernetes&gt; baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;&gt; enabled&#x3D;1&gt; gpgcheck&#x3D;1&gt; repo_gpgcheck&#x3D;1&gt; gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg&gt; EOF[root@master ~]# yum repolist[root@master ~]# yum makecache&#x2F;&#x2F;三台都需要这个yum源（node01，node02步骤省略） //安装以下三个组件kubectl：k8s客户端kubeadm：自动化快速部署k8s集群工具kubelet：客户端代理 1234[root@master ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0 kubectl-1.15.0-0&#x2F;&#x2F;node01、node02不需要安装kubectl[root@node01 ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0[root@node02 ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0 //加入开机自启（三台全部加入开机自启） 1[root@master ~]# systemctl enable kubelet //导入镜像 1234567891011121314[root@master ~]# mkdir images[root@master ~]# cd images&#x2F;[root@master images]# lscoredns-1-3-1.tar kube-apiserver-1-15.tar kube-proxy-1-15.tar myflannel-11-0.taretcd-3-3-10.tar kube-controller-1-15.tar kube-scheduler-1-15.tar pause-3-1.tar[root@master ~]# cat &gt; images.sh &lt;&lt;EOF&gt; #!&#x2F;bin&#x2F;bash&gt; for i in &#x2F;root&#x2F;images&#x2F;*&gt; do&gt; docker load &lt; $i&gt; done&gt; EOF[root@master ~]# chmod +x images.sh[root@master ~]# sh images.sh //初始化k8s集群 1234[root@master ~]# kubeadm init --kubernetes-version&#x3D;v1.15.0 \\&gt; --pod-network-cidr&#x3D;10.244.0.0&#x2F;16 \\&gt; --service-cidr&#x3D;10.96.0.0&#x2F;12 \\&gt; --ignore-preflight-errors&#x3D;Swap //如果初始化失败，需要重置k8s集群 1[root@master ~]# kubeadm reset //初始化完成后的操作 123[root@master ~]# mkdir -p $HOME&#x2F;.kube[root@master ~]# cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config[root@master ~]# chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config //查看节点信息情况 123[root@master ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONmaster NotReady master 10m v1.15.0 //部署flannel网络，（k8s版本必须是1.7版本以上） 1[root@master ~]# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;coreos&#x2F;flannel&#x2F;master&#x2F;Documentation&#x2F;kube-flannel.yml PS：这里执行不成功的话可能是网络的问题 //在node01、node02上提前导入镜像（不然在加入集群的时候，它会自动下载镜像） 12345[root@node02 ~]# mkdir images[root@node02 ~]# cd images&#x2F;[root@node02 images]# lskube-proxy-1-15.tar myflannel-11-0.tar pause-3-1.tardocker load &lt; kube-proxy-1-15.tar &amp;&amp; docker load &lt; myflannel-11-0.tar &amp;&amp; docker load &lt; pause-3-1.tar //node01、node02加入集群 1234567kubeadm join 192.168.1.70:6443 --token x85ks8.4x5qrhw87zct1vti \\ --discovery-token-ca-cert-hash sha256:227c69c29f16521a7dccb52104710b8cdd449aa0f7cb787affb62514fc8cc9eb[root@master ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONmaster Ready master 25m v1.15.0node01 Ready &lt;none&gt; 82s v1.15.0node02 Ready &lt;none&gt; 76s v1.15.0 //确保是running的状态 1234567891011121314[root@master ~]# kubectl get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-5c98db65d4-fr894 1&#x2F;1 Running 0 28mkube-system coredns-5c98db65d4-qkqh5 1&#x2F;1 Running 0 28mkube-system etcd-master 1&#x2F;1 Running 0 27mkube-system kube-apiserver-master 1&#x2F;1 Running 0 27mkube-system kube-controller-manager-master 1&#x2F;1 Running 0 27mkube-system kube-flannel-ds-amd64-rjnns 1&#x2F;1 Running 0 4m44skube-system kube-flannel-ds-amd64-tpkh5 1&#x2F;1 Running 0 4m50skube-system kube-flannel-ds-amd64-x425t 1&#x2F;1 Running 0 13mkube-system kube-proxy-4qsj2 1&#x2F;1 Running 0 4m44skube-system kube-proxy-gngnx 1&#x2F;1 Running 0 28mkube-system kube-proxy-shkw9 1&#x2F;1 Running 0 4m50skube-system kube-scheduler-master 1&#x2F;1 Running 0 27m //设置tab键的距离 123[root@master ~]# vim .vimrcset tabstop&#x3D;2[root@master ~]# source .vimrc //将kubectl命令加入tab自动补全 123[root@master ~]# source &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;bash_completion [root@master ~]# source &lt;(kubectl completion bash)[root@master ~]# echo &quot; source &lt;(kubectl completion bash)&quot; &gt;&gt; ~&#x2F;.bashrc","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"Pod资源对象+健康检查","slug":"Pod资源对象+健康检查","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.287Z","comments":true,"path":"Pod资源对象+健康检查.html","link":"","permalink":"http://pdxblog.top/Pod%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1+%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5.html","excerpt":"","text":"Deployment、Service、Pod是k8s最核心的3个资源对象Deployment： 最常见的无状态的控制器，支持应用的扩容缩容、滚动更新等操作 Service： 为弹性变动且存在生命周期的Pod对象提供了一个固定的访问接口，用户服务发现和服务访问 Pod： 是运行容器以及调度的最小单位，同一个Pod可以同时运行多个容器，这些容器共享NET、UTS、IPC，除此之外还有USER、PID、MOUNT ReplicationController：（rc） 用于确保每个Pod副本在任意时刻都能满足目标数量，简单点来说，它用于保证每个容器或容器组总是运行并且可以访问：老一代无状态的Pod控制器 ReplicaSet：（rs） 新一代无状态的Pod应用控制器，它与rc的不同之处在于支持的标签选择器不同，rc只支持等值选择器，rs还额外支持基于集合的选择器 StatefulSet： 用于管理有状态的持久化应用，如database服务程序，它与Deployment不同之处在于，它会为每一个Pod创建一个独有的持久性标识符，并确保每个Pod之间的顺序性 DamonSet： 确保每一个节点都运行了某个Pod的一个副本，新增的节点一样会被添加此类Pod，在节点移除时Pod会被收回 Job： 用于管理运行完成后即可终止的银应用，例如批量处理作业任务 volume: PV PVC ConfigMap: Secret： Role： ClusterRole： ClusterRoleBinding： Service account： Helm： Namespace：名称空间 默认的名称空间：Default //查看名称空间 123456[root@master ~]# kubectl get nsNAME STATUS AGEdefault Active 6d22hkube-node-lease Active 6d22hkube-public Active 6d22hkube-system Active 6d22h //查看名称空间详细信息 1[root@master ~]# kubectl describe ns default //创建名称空间 1[root@master ~]# kubectl create ns bdqn //使用yaml创建名称空间 123456[root@master ~]# vim test-ns.yamlapiVersion: v1kind: Namespacemetadata: name: test[root@master ~]# kubectl apply -f test-ns.yaml PS： namespace资源对象仅用于资源对象的隔离，并不能隔绝不同名称空间的Pod之间的通信，那是网络策略资源的功能 //删除名称空间： 12[root@master ~]# kubectl delete ns test[root@master ~]# kubectl delete -f test-ns.yaml //查看指定名称空间的资源可以使用–namespace或者-n选项 12[root@master ~]# kubectl get pod --namespace&#x3D;bdqn [root@master ~]# kubectl get pod -n bdqn Pod //通过yaml文件手动创建pod 12345678910111213141516[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-pod namespace: bdqn &#x2F;&#x2F;指定名称空间spec: containers: - name: test-app image: httpd:v1[root@master ~]# kubectl apply -f pod.yaml[root@master ~]# kubectl get pod -n bdqnNAME READY STATUS RESTARTS AGEtest-pod 1&#x2F;1 Running 0 19s&#x2F;&#x2F;删除[root@master ~]# kubectl delete pod -n bdqn test-pod Pod中镜像获取策略： Always： 镜像标签为“lastest”或镜像不存在时，总是从指定的仓库中获取镜像 ifNotPresent： 仅当本地镜像不存在时从目标仓库中下载 Never： 禁止从仓库下载镜像，即只是用本地镜像 123456789101112131415kind: PodapiVersion: v1metadata: name: test-pod namespace: bdqn labels: app: test-webspec: containers: - name: test-app image: httpd:v1 imagePullPolicy: IfNotPresent &#x2F;&#x2F;指定镜像获取策略 ports: - protocol: TCP containerPort: 90 &#x2F;&#x2F;手动创建的Pod，指定容器暴露的端口也不会生效 PS： 对于标签为“latest”或者这标签不存在，其默认镜像下载策略为“Always”,而对于其他标签的镜像，默认策略为“ifNotPresent” 容器的重启策略 Always： 但凡Pod对象终止就将其重启，此为默认设定 OnFailure： 仅在Pod对象出现错误时才将其重启 Never： 从不重启 Pod的默认健康检查 12345678910111213141516171819202122[root@master ~]# vim healcheck.yamlapiVersion: v1kind: Podmetadata: labels: test: healcheck name: healcheckspec: restartPolicy: OnFailure containers: - name: healthcheck image: busybox args: &#x2F;&#x2F;由镜像运行成容器的时候去做的事情 - &#x2F;bin&#x2F;sh - -c - sleep 20; exit 1[root@master ~]# vim healcheck.yaml[root@master ~]# kubectl get pod -w &#x2F;&#x2F;实时查看它的状态[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEhealthcheck 0&#x2F;1 Error 5 8m44s&#x2F;&#x2F;可以看到每过20s就会重启一次 LivenessProbe（活跃度探测） 1234567891011121314151617181920212223[root@master ~]# vim liveness.yamlkind: PodapiVersion: v1metadata: name: liveness labels: test: livenessspec: restartPolicy: OnFailure containers: - name: liveness image: busybox args: &#x2F;&#x2F;由镜像运行成容器的时候去做的事情 - &#x2F;bin&#x2F;sh - -c - touch &#x2F;tmp&#x2F;test; sleep 60; rm -rf &#x2F;tmp&#x2F;test; sleep 300 livenessProbe: &#x2F;&#x2F;活跃度探测 exec: command: - cat - &#x2F;tmp&#x2F;test initialDelaySeconds: 10 &#x2F;&#x2F;pod运行10秒后开始探测 periodSeconds: 5 &#x2F;&#x2F;每5秒探测一次 PS: Liveness活跃度探测，根据某个文件是否存在，来确认某个服务是否正常运行，如果存在则正常，否则，它会根据你设置的Pod的重启策略操作Pod Readiness（敏捷探测、就绪性探测） 1234567891011121314151617181920212223[root@master ~]# vim readiness.yaml kind: PodapiVersion: v1metadata: name: readiness labels: test: readinessspec: restartPolicy: OnFailure containers: - name: readiness image: busybox args: - &#x2F;bin&#x2F;sh - -c - touch &#x2F;tmp&#x2F;test; sleep 60; rm -rf &#x2F;tmp&#x2F;test; sleep 300 readinessProbe: exec: command: - cat - &#x2F;tmp&#x2F;test initialDelaySeconds: 10 periodSeconds: 5 PS：总结liveness和readiness探测 1）leveness和readiness是两种健康检查机制，如果不特意配置，k8s两种探测采取相同的默认行为，即通过判断容器启动进程的返回是否为零，来判断探测是否成功 2）两种探测配置方法完全一样，不同之处在于探测失败后的行为： liveness探测是根据Pod重启策略操作容器，大多数是重启容器 readinesss则是将容器设置为不可用，不接收Service转发的请求 3）两种探测方法可以独立存在，也可以同时使用，用liveness判断容器是否需要实现自愈；用readiness判断容器是否已经准备好对外提供服务 监控检测应用 在scale（扩容、缩容）中的应用 123456789101112131415161718192021222324252627282930313233343536373839404142[root@master ~]# vim hcscal.yamlkind: DeploymentapiVersion: extensions&#x2F;v1beta1metadata: name: webspec: replicas: 3 template: metadata: labels: run: web spec: containers: - name: web image: httpd ports: - containerPort: 80 readinessProbe: httpGet: scheme: HTTP path: &#x2F;healthy port: 80 initialDelaySeconds: 10 periodSeconds: 5---kind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort selector: run: web ports: - protocol: TCP port: 90 targetPort: 80 nodePort: 30321[root@master ~]# kubectl exec web-69d659f974-ktqbz touch &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs&#x2F;healthy[root@master ~]# kubectl describe svcEndpoints: 10.244.1.4:80 在更新过程中的使用 1234567891011121314151617181920212223242526[root@master ~]# vim app.v1.yamlkind: DeploymentapiVersion: extensions&#x2F;v1beta1metadata: name: appspec: replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - &#x2F;bin&#x2F;sh - -c - sleep 10; touch &#x2F;tmp&#x2F;healthy; sleep 3000 readinessProbe: exec: command: - cat - &#x2F;tmp&#x2F;healthy initialDelaySeconds: 10 periodSeconds: 5 //第一次升级 12345678[root@master ~]# kubectl apply -f app.v1.yaml --record[root@master ~]# cp app.v1.yaml app.v2.yaml [root@master ~]# vim app.v2.yaml&#x2F;&#x2F;修改 args: - &#x2F;bin&#x2F;sh - -c - sleep 3000 //第二次升级 123456789101112131415161718192021[root@master ~]# cp app.v1.yaml app.v3.yaml [root@master ~]# vim app.v3.yaml&#x2F;&#x2F;删除探测机制kind: DeploymentapiVersion: extensions&#x2F;v1beta1metadata: name: appspec: replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - &#x2F;bin&#x2F;sh - -c - sleep 3000 maxSurge：此参数控制滚动更新过程中，副本总数超过预期数的值，可以是整数，也可以是百分比，默认为1 maxUnavilable：不可用Pod的值，默认为1","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"Prometheus（普罗米修斯）","slug":"Prometheus（普罗米修斯）","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.287Z","comments":true,"path":"Prometheus（普罗米修斯）.html","link":"","permalink":"http://pdxblog.top/Prometheus%EF%BC%88%E6%99%AE%E7%BD%97%E7%B1%B3%E4%BF%AE%E6%96%AF%EF%BC%89.html","excerpt":"","text":"Prometheus（普罗米修斯）是一个系统和服务的监控平台。它可以自定义时间间隔从已配置的目标收集指标，评估规则表达式，显示结果，并在发现某些情况时触发警报 与其他监视系统相比，Prometheus的主要区别特征是： 一个多维数据模型（时间序列由指标名称定义和设置键/值尺寸） 一个灵活的查询语言来利用这一维度 不依赖于分布式存储；单服务器节点是自治的 时间序列收集通过HTTP 上的拉模型进行 通过中间网关支持推送时间序列 通过服务发现或静态配置发现目标 多种图形和仪表板支持模式 支持分层和水平联合 实验环境： docker01 192.168.1.70 NodeEXporter cAdvisor+Prometheus server+gragana docker02 192.168.1.60 NodeEXporter cAdvisor docker03 192.168.1.50 NodeEXporter cAdvisor 123456[root@localhost ~]# hostnamectl set-hostname docker1[root@localhost ~]# su -[root@localhost ~]# hostnamectl set-hostname docker2[root@localhost ~]# su -[root@localhost ~]# hostnamectl set-hostname docker3[root@localhost ~]# su - 全部关闭防火墙，禁用selinux 123[root@docker01 ~]# systemctl stop firewalld[root@docker01 ~]# systemctl disable firewalld[root@docker01 ~]# setenforce 0 需要部署的组件 Prometheus server（9090）：普罗米修斯的主机服务器 NodeEXporter（9100）：负责收集host硬件信息和操作系统信息 //谷歌开发的监控软件。收集数据，不太直观。有历史保留，方便后期做优化 cAdvisor（8080）：负责收集host上运行的容器信息 Grafana（3000）：负责展示普罗米修斯监控界面 //类似于kibana的图形界面，提供可视化web页面 1）三个节点，全部部署node-exporter和cadvisor //分别在三个节点上导入镜像 1[root@docker01 ~]# docker load &lt; node-exporter.tar &amp;&amp; docker load &lt; mycadvisor.tar //部署node-exporter，收集硬件和系统信息（三台都需要部署） 12345[root@docker01 ~]# docker run -d -p 9100:9100 -v &#x2F;proc:&#x2F;host&#x2F;proc \\&gt; -v &#x2F;sys:&#x2F;host&#x2F;sys -v &#x2F;:&#x2F;rootfs --net&#x3D;host \\&gt; prom&#x2F;node-exporter --path.procfs &#x2F;host&#x2F;proc \\&gt; --path.sysfs &#x2F;host&#x2F;sys \\&gt; --collector.filesystem.ignored-mount-points &quot;^&#x2F;(sys|proc|dev|host|etc)($|&#x2F;)&quot; PS：注意这里使用了–net=host，这样prometheus server可以直接与node-exporter通信 验证，打开浏览器验证结果（192.168.1.70:9100） ) ) //部署安装cAdvisor，收集节点容器信息（三台都需要部署） 1234[root@docker01 ~]# docker run -v &#x2F;:&#x2F;rootfs:ro -v &#x2F;var&#x2F;run:&#x2F;var&#x2F;run&#x2F;:rw \\&gt; -v &#x2F;sys:&#x2F;sys:ro -v &#x2F;var&#x2F;lib&#x2F;docker:&#x2F;var&#x2F;lib&#x2F;docker:ro \\&gt; -p 8080:8080 --detach&#x3D;true --name&#x3D;cadvisor \\&gt; --net&#x3D;host google&#x2F;cadvisor 打开浏览器验证（192.168.1.70:8080） ) 2）在docker01上部署prometheus server服务 在部署prometheus之前，我们需要对它的配置文件进行修改，所以我们先运行一个容器，将其配置文件拷贝出来 123456789[root@docker01 ~]# docker load &lt; prometheus.tar[root@docker01 ~]# docker run -d -p 9090:9090 --name prometheus --net&#x3D;host prom&#x2F;prometheus[root@docker01 ~]# docker exec -it prometheus &#x2F;bin&#x2F;sh&#x2F;prometheus $ cd &#x2F;etc&#x2F;prometheus&#x2F;&#x2F;etc&#x2F;prometheus $ lsconsole_libraries consoles prometheus.yml[root@docker01 ~]# docker cp prometheus:&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml .&#x2F;[root@docker01 ~]# vim prometheus.yml- targets: [&#39;localhost:9090&#39;,&#39;localhost:8080&#39;,&#39;localhost:9100&#39;,&#39;192.168.1.60:8080&#39;,&#39;192.168.1.60:9100&#39;,&#39;192.168.1.50:8080&#39;,&#39;192.168.1.50:9100&#39;] PS：这里制定了prometheus的监控项，包括它也会监控自己收集到的数据 1[root@docker01 ~]# docker rm -f prometheus //重新运行容器： 1[root@docker01 ~]# docker run -d -p 9090:9090 --name prometheus --net&#x3D;host -v &#x2F;root&#x2F;prometheus.yml:&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml prom&#x2F;prometheus //浏览器访问：192.168.1.70:9090 ) ) PS:这里能够看到我们各个监控项 3）在docker01上部署grafana服务，用来展示prometheus收集到的数据 1234[root@docker01 ~]# docker load &lt; grafana.tar[root@docker01 ~]# mkdir grafana-storage[root@docker01 ~]# chmod 777 -R grafana-storage&#x2F;[root@docker01 ~]# docker run -d -p 3000:3000 --name grafana -v &#x2F;root&#x2F;grafana-storage:&#x2F;var&#x2F;lib&#x2F;grafana -e &quot;GF_SECURITY_ADMIN_PASSWORD&#x3D;123.com&quot; grafana&#x2F;grafana 浏览器访问：192.168.1.70:3000 用户名：admin 密码：123.com ) //创建数据源 ) ) ) ) PS：看到这这提示，说明prometheus和grafana服务是正常连接的 此时，虽然grafana收集到了数据，但怎么显示它，仍然是个问题，grafana支持自定义显示信息，不过要自定义起来非常麻烦，不过好在，grafana官方为我们提供了一些模板，来供我们使用 Grafana官网：https://grafana.com ) //可以根据自己的喜好选择模板 ) 选中一款模板后，然后，我们又两种方式可以套用这个模板 第一种方式：通过JSON文件使用模板 ) 下载完成之后，回到grangana控制台 ) ) ) ) 第二种方式： 可以直接通过模板的ID号 ) ) ) ) ) 配置AlertManagerAlertManager：用来接收prometheus发送的报警信息，并且执行设置好的报警方式、报警内容 AlertManager.yml配置文件： global：全部配置，包括报警解决后的超时时间、SMTP相关配置、各种渠道通知的API地址等新消息 route：用来设置报警的分发策略 receivers：配置告警消息接收者信息 inhibit_rules：抑制规则配置，当存在于另一组匹配的报警时，抑制规则将禁用一组匹配的警报 //运行一个容器获取配置文件并进行配置 12345678910111213141516171819202122232425262728[root@docker01 ~]# docker run -d --name alertmanager -p 9093：993 prom&#x2F;alertmanager:latest[root@docker01 ~]# docker cp alertmanager:&#x2F;etc&#x2F;alertmanager&#x2F;alertmanager .&#x2F;[root@docker01 ~]# cat alertmanager.yml global: resolve_timeout: 5m smtp_from: &#39;2960824193@qq.com&#39; smtp_smarthost: &#39;smtp.qq.com:465&#39; smtp_auth_username: &#39;2960824193@qq.com&#39; smtp_auth_password: &#39;aseydtzejqfqdhai&#39; smtp_require_tls: false smtp_hello: &#39;qq.com&#39;route: group_by: [&#39;alertname&#39;] group_wait: 5s group_interval: 5s repeat_interval: 5m receiver: &#39;email&#39;receivers:- name: &#39;email&#39; email_configs: - to: &#39;2960824193@qq.com&#39; send_resolved: trueinhibit_rules: - source_match: severity: &#39;critical&#39; target_match: severity: &#39;warning&#39; equal: [&#39;alertname&#39;, &#39;dev&#39;, &#39;instance&#39;] //删除容器并重新运行 12[root@docker01 ~]# docker rm -f alertmanager[root@docker01 ~]# docker run -d --name alertmanager -p 9093:9093 -v &#x2F;root&#x2F;alertmanager.yml:&#x2F;etc&#x2F;alertmanager&#x2F;alertmanager.yml prom&#x2F;alertmanager:latest //浏览器访问：192.168.1.70:9093 ) Prometheus配置alertmanager报警规则 1234567891011121314151617181920212223[root@docker01 ~]# mkdir -p prometheus&#x2F;rules[root@docker01 ~]# cd prometheus&#x2F;rules&#x2F;[root@docker01 rules]# lsnode-up.rules[root@docker01 rules]# cat node-up.rules groups:- name: node-up rules: - alert: node-up expr: up&#123;job&#x3D;&quot;prometheus&quot;&#125; &#x3D;&#x3D; 0 &#x2F;&#x2F;这个job要和prometheus里的job名称一样 for: 15s labels: severity: 1 team: node annotations: summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 已停止运行超过 15s！&quot;-----------------------------------------------------------------------------------------------&#x2F;etc&#x2F;prometheus $ cat prometheus.yml.......- job_name: &#39;prometheus&#39;&#x2F;&#x2F;监控的内容是： - targets: [&#39;localhost:9090&#39;,&#39;localhost:8080&#39;,&#39;localhost:9100&#39;,&#39;192.168.1.60:9100&#39;,&#39;192.168.1.60:8080&#39;,&#39;192.168.1.50:9100&#39;,&#39;192.168.1.50:8080&#39;]....... //编辑prometheus的配置文件 12345678910[root@docker01 ~]# vim prometheus.yml# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: - 192.168.1.70:9093 &#x2F;&#x2F;目标为alertmanager容器rule_files: - &quot;&#x2F;usr&#x2F;local&#x2F;prometheus&#x2F;rules&#x2F;*.rules&quot; &#x2F;&#x2F;路径是容器内的路径 //删除prometheus容器，重新挂载配置文件 12[root@docker01 ~]# docker rm -f prometheus[root@docker01 ~]# docker run -d -p 9090:9090 --name prometheus --net&#x3D;host -v &#x2F;root&#x2F;prometheus.yml:&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml -v &#x2F;root&#x2F;prometheus&#x2F;rules&#x2F;node-up.rules:&#x2F;usr&#x2F;local&#x2F;prometheus&#x2F;rules&#x2F;node-up.rules prom&#x2F;prometheus //浏览器访问：192.168.1.70:9090 ) //随便关闭一个容器进行测试 ) AlertManager配置自定义邮件模板 1234567891011121314151617[root@docker01 ~]# mkdir prometheus&#x2F;alertmanager-tmpl[root@docker01 ~]# cd prometheus&#x2F;alertmanager-tmpl&#x2F;[root@docker01 alertmanager.tmpl]# cat email.tmpl &#123;&#123; define &quot;email.from&quot; &#125;&#125;2960824193@qq.com&#123;&#123; end &#125;&#125; &#x2F;&#x2F;哪个邮箱来发送信息&#123;&#123; define &quot;email.to&quot; &#125;&#125;2960824193@qq.com&#123;&#123; end &#125;&#125; &#x2F;&#x2F;发送到哪个邮箱&#123;&#123; define &quot;email.to.html&quot; &#125;&#125;&#123;&#123; range .Alerts &#125;&#125;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;start&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&lt;br&gt;告警程序: prometheus_alert&lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; 级&lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125;&lt;br&gt;故障主机: &#123;&#123; .Labels.instance &#125;&#125;&lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125;&lt;br&gt;触发时间: &#123;&#123; .StartsAt.Format &quot;2019-08-04 16:58:15&quot; &#125;&#125; &lt;br&gt;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;end&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125; //修改altermanager的配置文件 1234567[root@docker01 ~]# vim alertmanager.yml&#x2F;&#x2F;在第九行添加templates: - &#39;&#x2F;etc&#x2F;alertmanager-tmpl&#x2F;*.tmpl&#39; &#x2F;&#x2F;容器内的路径&#x2F;&#x2F;修改第20行，和第21行： - to: &#39;&#123;&#123; template &quot;email.to&quot; &#125;&#125;&#39; &#x2F;&#x2F;必须和email.tmpl中的&#123;&#123; define “email.to”&#125;&#125; 2960824193@qq.com&#123;&#123;end&#125;&#125;对应 html: &#39;&#123;&#123; template &quot;email.to.html&quot; . &#125;&#125;&#39; &#x2F;&#x2F;必须和email.tml中的&#123;&#123; define &quot;email.to.html&quot; &#125;&#125;名字对应 //删除alertmanager容器，重新运行并挂载 12[root@docker01 ~]# docker rm -f alertmanager[root@docker01 ~]# docker run -d --name alertmanager -p 9093:9093 -v &#x2F;root&#x2F;alertmanager.yml:&#x2F;etc&#x2F;alertmanager&#x2F;alertmanager.yml -v &#x2F;root&#x2F;prometheus&#x2F;alertmanager-tmpl&#x2F;:&#x2F;etc&#x2F;altermanager-tmpl prom&#x2F;alertmanager:latest //停止容器测试 )","categories":[{"name":"Docker","slug":"Docker","permalink":"http://pdxblog.top/categories/Docker/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"k8s架构、基本概念","slug":"k8s架构、基本概念","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.303Z","comments":true,"path":"k8s架构、基本概念.html","link":"","permalink":"http://pdxblog.top/k8s%E6%9E%B6%E6%9E%84%E3%80%81%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.html","excerpt":"","text":"k8s总架构：) Master节点：（默认不参加工作） kubectl：k8s是命令端，用来发送客户端的操作指令 k8s的原生组件：（部署k8s比必不可少的组件） API server：是k8s集群的前端接口，各种客户端工具以及k8s的其他组件可以通过它管理k8s集群的各种资源，它提供了HTTP/HTTPS RESTful API，即k8s API Scheduler：负责决定将Pod放在哪个Node上运行，在调度时，会充分考虑集群内的拓扑结构，当前各个节点的负载情况，以及对高可用、性能、数据和亲和性需求 Controller Manager：负责管理集群的各种资源，保证资源处于预期的状态，它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等等 Etcd：负责保存k8s集群的配置信息和各种资源的状态信息，当数据发生变化时，etcd会快速的通知k8s相关组件。第三方组件，意味着它有可替换方案，比如：Consul、zookeeper Pod：k8s集群的最小组成单位，一个Pod内，可以运行一个或多个容器，大多数情况下，一个Pod内只有一个Container容器 Flannel：是k8s集群网络解决方案，可以保证Pod的跨主机通信。第三方解决方案，也有替换方案 Node节点：kubelet：它是Node的agent（代理），当Scheduler确定某个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet，kubelet会根据这些信息创建和运行容器，并向Master报告运行状态 kube-proxy：负责将访问service的TCP/UDP数据流转发到后端容器，如果有多个副本，kube-pory会实现负载均衡 运行一个例子： //创建一个deployment资源对象。Pod控制器 1[root@master ~]# kubectl run test-web --image&#x3D;httpd --replicas&#x3D;2 分析各个组件的作用以及架构流程： kubectl发送部署请求到API server API server通知Controller Manager创建一个Deployment资源 Scheduler执行调度任务，将两个副本Pod分发到node01和node02上 node1和node02上的kubelet在各个节点上创建并运行Pod 补充： 应用的配置和当前的状态信息报错在etcd中，执行kubectl get pod时API server会从etc中读取这 些数据 flannel会为每个Pod分配一个IP，但此时没有创建Service资源，目前kube-pory还没有参与进来","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]},{"title":"创建资源的两种方式","slug":"创建资源的两种方式","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.303Z","comments":true,"path":"创建资源的两种方式.html","link":"","permalink":"http://pdxblog.top/%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F.html","excerpt":"","text":"创建资源的两种方式 用命令行的方式创建：//创建Pod控制器，deployments 1[root@master ~]# kubectl run web --image&#x3D;nginx --replicas&#x3D;5 //查看控制器情况 1[root@master ~]# kubectl get deployments. //查看资源详细信息 1[root@master ~]# kubectl describe deployments. web PS：查看某种资源对象，没有指名称空间，默认是在default名称空间，可以加上-n选项，查看指定名称空间 1[root@master ~]# kubectl get pod -n&#x3D;kube-system 注意：直接运行创建的Deployment资源对象，是经常使用的一个控制器类型，除了deployment，还有rc，rs等Pod控制器，Deployment是一个高级的Pod控制器 //创建Service资源类型 1[root@master ~]# kubectl expose deployment web --name&#x3D;web-svc --port&#x3D;80 --type&#x3D;NodePort PS：如果想要外网能够访问服务，可以暴露deployment资源，得到service资源，但svc资源的类型必须为NodePoet 映射端口范围：30000-32767 服务的扩容与缩容： 1[root@master ~]# kubectl scale deployment web --replicas&#x3D;8 //通过修改yuml文件进行扩容与缩容 1[root@master ~]# kubectl edit deployments. web 服务的升级与回滚: 1[root@master ~]# kubectl set image deployment web web&#x3D;nginx:1.15 //通过修改配置文件进行升级 1[root@master ~]# kubectl edit deployments. web //回滚 1[root@master ~]# kubectl rollout undo deployment web 配置清单（yml、yaml）： 常见yaml文件写法，以及字段的作用： 五个一级字段： apiVersion: api版本信息 kind: 资源对象的类别 metadata: 元数据。名称字段必须写 spec: 用户期望的状态 status: 资源现在处于什么样的状态 Deployment 12345678910111213141516[root@master ~]# vim web.yamlkind: DeploymentapiVersion: extensions&#x2F;v1beta1metadata: name: webspec: replicas: 2 template: metadata: labels: app: web_server spec: containers: - name: nginx image: nginx[root@master ~]# kubectl apply -f web.yaml service 123456789101112kind: ServiceapiVersion: v1metadata: name: web-svcspec: selector: &#x2F;&#x2F;标签选择器，和Deployment里的标签要一样 app: web_server ports: - protocol: TCP port: 80 targetPort: 80[root@master ~]# kubectl apply -f web-svc.yml 使用相同的标签和标签选择器，使两个资源对象相互关联 PS：（本质的意义：提供一个统一的接口） 创建的Service资源对象，默认的type为ClusterIP,意味着集群内任何节点都可以访问，它的作用是为后端真正提供服务的Pod提供一个统一的访问接口,如果想要外网访问服务，应该把type改为NodePort 12345678910111213kind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort &#x2F;&#x2F;指定类型，让外网来访问 selector: app: web_server ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30033 &#x2F;&#x2F;指定集群映射端口，范围是30000-32767","categories":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://pdxblog.top/tags/k8s/"}]}]}