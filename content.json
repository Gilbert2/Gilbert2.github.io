{"meta":{"title":"Maisyの博客","subtitle":"记录生活中的点点滴滴","description":"人的一生就是一个储蓄的过程，在奋斗的时候储存了希望；在耕耘的时候储存了一粒种子；在旅行的时候储存了风景；在微笑的时候储存了快乐。聪明的人善于储蓄，在漫长而短暂的人生旅途中，学会储蓄每一个闪光的瞬间，然后用它们酿成一杯美好的回忆，在四季的变幻与交替之间，散发浓香，珍藏一生","author":"Maisy","url":"https://pdxblog.top","root":"/"},"pages":[{"title":"派大星","date":"2020-01-25T13:20:10.000Z","updated":"2020-01-25T13:22:20.493Z","comments":true,"path":"about/index.html","permalink":"https://pdxblog.top/about/index.html","excerpt":"","text":"扎实的专业知识是我最大的财富；认真踏实是我做事的原则；不断超越创新是我追求的目标；"},{"title":"热门文章Top 10","date":"2018-10-29T16:54:50.000Z","updated":"2020-05-12T07:55:24.901Z","comments":false,"path":"top/index.html","permalink":"https://pdxblog.top/top/index.html","excerpt":"","text":"var APP_ID = ******; //输入个人LeanCloud账号AppID var APP_KEY = ******; //输入个人LeanCloud账号AppKey AV.init({ appId: APP_ID, appKey: APP_KEY }); var query = new AV.Query('Counter');//表名 query.descending('time'); //结果按阅读次数降序排序 query.limit(10); //最终只返回10条结果 query.find().then( response => { var content = response.reduce( (accum, {attributes}) => { accum += `热度 ${attributes.time} ℃${attributes.title}` return accum; },\"\") document.querySelector(\"#post-rank\").innerHTML = content; }) .catch( error => { console.log(error); }); #post-rank { text-align: center; } #post-rank .prefix { color: #ff4d4f; }"},{"title":"categories","date":"2020-01-24T14:29:44.000Z","updated":"2020-01-24T14:30:26.053Z","comments":true,"path":"categories/index.html","permalink":"https://pdxblog.top/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-01-25T12:57:30.000Z","updated":"2020-01-25T12:58:43.644Z","comments":true,"path":"tags/index.html","permalink":"https://pdxblog.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Python网络管理","slug":"Python网络管理","date":"2020-05-14T16:00:00.000Z","updated":"2020-05-15T06:29:57.848Z","comments":true,"path":"Python网络管理.html","link":"","permalink":"https://pdxblog.top/Python%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86.html","excerpt":"","text":"网络网络可以将多台主机进行连接，使得网络中的主机可以相互通信。在网络通信中，使用最广泛的通信协议是TCP/IP协议簇，因此，Python也提供了相应的应用程序接口（API), 使得工程师可以在Python程序中创建网络连接、进行网络通信。计算机之间可以相互通信以后，就开始涉及网络安全问题。现如今网络情况复杂安全环境恶劣。 2017年5月12日起，全球范围内爆发基于Windows网络共享协议进行攻击传播的蠕虫恶意代码，这是不法分子通过改造之前泄露的NSA黑客武器库中“永恒之蓝”攻击程序发起的网络攻击事件。五个小时内，包括英国、俄罗斯、整个欧洲以及中国国内多个高校校内网、大型企业内网和政府机构专网中招，被勒索支付高额赎金才能解密恢复文件，对重要数据造成严重损失。被袭击的设备被锁定，并索要300美元比特币赎金。要求尽快支付勒索赎金，否则将删除文件，甚至提出半年后如果还没支付的穷人可以参加免费解锁的活动。原来以为这只是个小范围的恶作剧式的勒索软件，没想到该勒索软件大面积爆发，许多高校学生中招，愈演愈烈。 Python是一门应用领域非常广泛的语言，除了在科学计算、大数据处理、自动化运维等领域广泛应用以外，在计算机网络领域中使用也非常广泛。这主要得益于Python语言的开发效率高、入门门槛低、功能强大等优点。工程师可以使用Python语言管理网络，计算机黑客可以使用Python语言或者Python语言编写的安全工具进行渗透测试、网络分析、安全防范等。在这章中，我们将介绍Python在网络方面的应用，包括网络通信、网络管理和网络安全。我们首先介绍如何使用Python语言列出网络上所有活跃的主机；然后介绍一个 端口扫描工具；接着介绍如何使用IPy方便地进行IP地址管理；随后，介绍了一个DNS工具包；最后，我们介绍了一个非常强大的网络嗅探工具。 一、列出网络上所有活跃的主机在这一小节中，我们将会学习如何在shell脚本中调用ping命令得到网络上活跃的主机列表，随后，我们使用Python语言改造这个程序，以此支持并发的判断。 1、使用ping命令判断主机是否活跃ping命令是所有用户都应该了解的最基础的网络命令，ping命令可以探测主机到主机之间是否能够通信，如果不能ping到某台主机，则表明不能和这台主机进行通信。ping命令最常使用的场景是验证网络上两台主机的连通性以及找出网络上活跃的主机。为了检査网络上两台主机之间的连通性，ping命令使用互联网控制协议（ICMP)中的 ECHO_REQUEST数据报，网络设备收到该数据报后会做出回应。ping命令可以通过网络设备的回复得知两台主机的连通性以及主机之间的网络延迟。ping命令的使用非常简单，直接使用主机名、域名或IP地址作为参数调用ping命令即可。如下所示： 123456[root@bogon ~]# ping 192.168.1.10PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=0.023 ms64 bytes from 192.168.1.10: icmp_seq=2 ttl=64 time=0.034 ms64 bytes from 192.168.1.10: icmp_seq=3 ttl=64 time=0.033 ms64 bytes from 192.168.1.10: icmp_seq=4 ttl=64 time=0.039 ms ping命令会连续发送包，并将结果打印到屏幕终端上。如果主机不可达，ping将会显示“Destination Host Unreachable”的错误信息。如下所示： 123456[root@bogon ~]# ping www.baidu.comPING www.a.shifen.com (220.181.38.150) 56(84) bytes of data.From bogon (192.168.1.10) icmp_seq=1 Destination Host UnreachableFrom bogon (192.168.1.10) icmp_seq=2 Destination Host UnreachableFrom bogon (192.168.1.10) icmp_seq=3 Destination Host UnreachableFrom bogon (192.168.1.10) icmp_seq=4 Destination Host Unreachable 除了检查网络上两台主机之间的连通性外，ping命令还可以粗略地估计主机之间的网络延迟情况。在ping命令的输出结果中，time字段的取值表示网络上两台主机之间的往返时间，它是分组从源主机到B的主机一个来回的时间，单位是毫秒。我们可以通过这个时间粗略估计网络的速度以及监控网络状态。例如，有这样一个使用ping命令解决线上问题的案例。当时的情况是应用程序使用我们提供的数据库服务，在每个整点时都会出现应用程序建立数据库连接失败的情况。通过前期排査，可以确定的是应用的请求已成功发出，数据库的压力并不是特别大，数据库连接也没有满。因此，问题很有可能出在网络上面。为此，我们增加了一个ping延迟监控。通过监控发现，在每个整点时ping的网络延迟变大，甚至大到了不可接受的程度。有了这个线索以后，接着排查网络问题。通过分析定位，发现是因为宿主机上有定时任务，导致每个整点宿主机的cpu压力增加，从而引发了前面所说的建立数据库连接失败的错误。默认情况下，ping命令会不停地发送ECHO_REQUEST数据报并等待回复，到按下Ctrl+C为止。我们可以用选项-c限制所发送的ECHO_REQUEST数据报数量。用法如下： 12345678[root@bogon ~]# ping -c 2 192.168.1.10PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=0.030 ms64 bytes from 192.168.1.10: icmp_seq=2 ttl=64 time=0.047 ms--- 192.168.1.10 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1000msrtt min/avg/max/mdev = 0.030/0.038/0.047/0.010 ms 在这个例子中，ping命令发送了 2个ECHO_REQUEST数据报就停止发送，这个功能对于脚本中检查网络的连通性非常有用。ping命令将结果打印到屏幕终端，我们通过ping命令的输出结果判断主机是否可达, 这种方式虽然直观，但是不便于程序进行处理。在程序中可以通过ping命令的返回码判断主机足否可达，当主机活跃时，ping命令的返回码为0，当主机不可达时，ping命令的返回码非0。有了上面的基础以后，要判断网络上活跃的主机就非常容易了。我们只需要ping每一台主机，然后通过ping命令的返回值判断主机是否活跃。下面这段Shell程序就是用来判断网络中的主机是否可达： 123456789for ip in 'cat ips.txt'do if ping $ip -c 2 &amp; &gt; /dev/null then echo \"$ip is alive\" else echo \"$ip is unreachable\" fidone 在这个例子中，我们首先将主机地址以每行一个地址的形式保存到ips.txt文件中，然后通过cat命令读取ips.txt文件中的内容，使用for循环迭代ips.txt中保存的主机。为了减少视觉杂讯，使用输出重定向的方式将ping命令的结果输出到/dev/null中，以此避免信息在终端上打印。为了简化起见，我们直接在if语句中调用ping命令，Shell脚本能够根据ping命令的返回码判断命令执行成功还是失败。 2、使用Python判断主机是否活跃前面的Shell脚本中，虽然所有的IP地址都是彼此独立，但是，我们的程序依然是顺序调用ping命令进行主机探活。由于每执行一次ping命令都要经历一段时间延迟（或者接收回应，或者等待回应超时)，所以，当我们要检査大量主机是否处于活跃状态时需要很长的时间。对于这种情况可以考虑并发地判断主机是否活跃。Shell脚本可以非常快速地解决简单的任务，但是，对于比较复杂的任务，Shell脚本就无能为力。如这里的并发判断主机是否活跃的需求。对于这种情况，可以使用Python语言编写并发的程序，以此加快程序的执行。如下所示： 1234567891011121314151617181920212223242526272829#/usr/bin/python#_*_ coding:utf-8 _*_from __future__ import print_functionimport subprocessimport threadingdef is_reacheable(ip): if (subprocess.call(['ping', '-c', '1', ip])): print('&#123;0&#125; is alive'.format(ip)) else: print('&#123;0&#125; is unreacheable'.format(ip))def main(): with open('ips.txt') as f: lines = f.readlines(); threads = [] for line in lines: thr = threading.Thread(target=is_reacheable, args=(line,)) thr.start() threads.append(thr) for thr in threads: thr.join()if __name__ == \"__main__\": main() 在这个例子中，我们首先打开ips.txt文件，并通过File对象的readlines函数将所有IP 地址读入内存。读入内存以后，IP地址保存在一个列表中，列表的每一项正好是一个地址。 对于每一个IP地址都创建一个线程。由于线程之间不共享任何数据，因此，不需要进行并发控制，这也使得整个程序变得简单。 在Python 中要判断两台主机是否可达有两种不同的方法，一种是在Python程序中调用ping命令实 现，另一种是使用socket编程发送ICMP数据报。为了简单起见，在我们这里的例子中使用前一种方法。为了调用系统中的ping命令，我们使用了subprocess模块。我们的Python程序最终也是调用ping命令判断主机是否活跃，从思路上来说，和前面的Shell脚本是一样的。区别就在于Python程序使用并发加快了程序的执行效率。在我的测试环境中，测试了36个IP地址，其中，有12个IP地址不可达，需要等待网络超时才能返回，另外24个IP地址可达。使用Linux自带的time命令进行粗略计时，Shell脚本的执行时间是1分48秒，Python程序的执行时间是10秒。两个程序执行时间的差异，根据网络规模和网络环境将会显著不同。这里要表达的是，使用Python语言只需要很少的代码就能够将一个程序改造成并发的程序，通过并发来大幅提升程序的效率。 3、使用生产者消费者模型减少线程的数量在前面的例子中，我们为每一个IP地址创建一个线程，这在IP地址较少的时候还算可行，但在IP地址较多时就会暴露出各种问题（如频繁的上下文切换）。因此，我们需要限制线程的数量。 列出网络上所有活跃主机的问题，其实是一个简单的生产者和消费者的问题。生产者 和消费者问题是多线程并发中一个非常经典的问题，该问题描述如下： 有一个或多个生产者在生产商品，这些商品将提供给若干个消费者去消费。为了使生产者和消费者能并发执行，在两者之间设置一个缓冲期，生产者将它生产的商品放入缓冲中，消费者可以从缓冲区中取走商品进行消费。生产者只需要关心这个缓冲区是否已满， 如果未满则向缓冲区中放入商品，如果已满，则需要等待。同理，消费者只需要关心緩冲区中是否存在商品，如果存在商品则进行消费，如果缓冲区为空，则需要等待。 生产者和消费者模型的好处是，生产者不需要关心有多少消费者、消费者何时消费、 以怎样的速度进行消费。消费者也不需要关心生产者，这就实现了程序模块的解耦。我们这里的问题比生产者和消费者模型还要简单，只需要一次性将所有的IP地址读入到内存中，然后交由多个线程去处理。也就是说，我们一开始就生产好了所有的商品，只需要消费者消费完这些商品即可。如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445#/usr/bin/python#_*_ coding:utf-8 _*_from __future__ import print_functionimport subprocessimport threadingfrom sqlalchemy.util.queue import Empty, Queuedef call_ping(ip): if (subprocess.call(['ping', '-c', '1', ip])): print('&#123;0&#125; is alive'.format(ip)) else: print('&#123;0&#125; is unreacheable'.format(ip))def is_reacheable(q): try: while True: ip = q.get_nowait() call_ping(ip) except Empty: passdef main(): q = Queue() with open('ips.txt') as f: for line in f: q.put(line) threads = [] for i in range(10): thr = threading.Thread(target=is_reacheable, args=(q,)) thr.start() threads.append(thr) for thr in threads: thr.join()if __name__ == \"__main__\": main() 在这个例子中创建了10个线程作为消费者线程，我们可以修改range函数的参数控制线程的个数。此外，我们还用到了一个新的数据结构，即Queue。引入Queue足因为多个消费者之间存在并发访问的问题，即多个消费者可能同时从缓冲区中获取商品。为了解决并发问题，我们使用了 Python标准库的Queue。Queue是标准库中线程安全的队列（FIFO) 实现，提供了一个适用于多线程编程的先进先出的数据结构，非常适合用于生产者和消费者线程之间的数据传递。在这段程序中，我们首先将所有1P地址读入内存并放入Queue中，消费者不断从 Queue中获取商品。需要注意的是，如果我们使用Queue的get方法，当Queue中没有商品时，线程将会阻塞等待直到有新的商品为止。而在这个例子中不需要消费者阻塞等待， 因此，使用了Queue的get_nowait方法。该方法在冇商品时直接返回商品，没有商品时抛出Empty异常。消费者线程不断从Queue中获取IP地址，获取到IP地址以后调用call_ ping函数判断主机是否可达，直到没有商品以后退出线程。 二、端口扫描仅仅知道网络上的主机是否可达还不够，很多情况下，我们需要的是一个端口扫描器。使用端口扫描器吋以进行安全检测与攻击防范。例如，在2017年5月12日，全球范围内爆发了基于Windows网络共享协议的永恒之蓝（Wannacry)勒索蠕虫。仅仅五个小时，包 括美国、中国、俄罗斯以及整个欧洲在内的100多个国家都不问程度地遭受永恒之蓝病毒攻击，尤其是高校、大型企业内网和政府机构专网，被攻击的电脑被勒索支付高额赎金才能解密恢复文件，对重要数据造成严重损失。永恒之蓝利用Windows系统的445端口进行蠕虫攻击，部分运营商已经在主干网络上封禁了 445端口，但是教育网以及大量企业内网并没有此限制，从而导致了永恒之蓝勒索蠕虫的泛滥。 所以作为工程师，一方面需要在日常维护养成良好的习惯，如配置防火墙、进行网络隔离、关闭不必要的服务、及时更新补丁；另一方面可以掌握一些安全相关的工具，在日常中进行安全防范，在紧急悄况下进行安全检测。在这一小节，我们将介绍如何使用Python进行端口扫描。有了端口扫描器，我们可以快速了解主机打开了哪些不必要的端口，以便及时消灭安全隐患。在这一小节中，我们将使用Python语言编写一个端口扫描器，然后介绍大名鼎鼎的端 口扫描工具nmap，最后，通过python-nmap在Python代码中调用nmap进行端口扫描。 1、使用Python编写端口扫描工具在Linux下，可以使用ping命令要判断一台主机是否可达，而判断一个端口是否打开可以使用telnet命令。我们可以模仿前面小节中并行ping的例子，在Python代码中调用 telnet命令判断一个端口是否打开。但是telnet命令存在一个问题，当我们telnet—个不可达的端口时，telnet需要很久才能够超时返回，并且telnet命令没有参数控制超时时间。 此外，如果Python标准库中有相应的模块，应该尽可能地使用Python的标准库，而不是在 Python代码中执行Linux命令。这一方面能够增加代码的可读性、可维护性，另一方面也能够保证程序跨平台运行。为了使用Python编写端口扫描器，我们需要简单了解socket模块。socket模块为操作系统的socket连接提供了一个Python接口。有了 socket模块，我们可以完成任何使用 socket的任务。socket模块提供了一个工厂函数socket，socket函数会返冋一个socket对象。我们可以给socket函数传递参数，以此创建不同网络协议和网络类塑的socket对象。默认情况下，socket函数会返回一个使用TCP协议的socket对象。如下所示： 123456789101112131415In [1]: import socketIn [2]: s = socket.socket()In [3]: s.connect(('47.100.98.242',80))In [4]: s.send(\"GET/HTTP/1.0\".encode())Out[4]: 12In [5]: print(s.recv(200))b'HTTP/1.1 400 Bad Request\\r\\nServer: nginx\\r\\nDate: Sat, 29 Feb 2020 15:44:51 GMT\\r\\nContent-Type: text/html\\r\\nContent-Length: 150\\r\\nConnection: close\\r\\n\\r\\n&lt;html&gt;\\r\\n&lt;head&gt;&lt;title&gt;400 Bad Request&lt;/title&gt;&lt;/head&gt;\\r\\n&lt;b'In [6]: s.close() 在这个例子中，socket工厂函数以默认参数AF_INET和SOCK_STREAM创建了一个 名为s的socket对象，该对象可以在进程间进行TCP通信。创建完对象以后，我们使用connect函数连接到远程服务器的80端口，并发送一个HTTP请求到远程服务器，发送完 毕之后，接收服务器响应的前200个宇节。最后，调用socket对象的close方法关闭连接。在这个例子中，我们用到了 socket工厂函数、socket的connect方法、send方法、recv 方法和close方法，这也是socket中最常使用的一些方法。 接下来，我们就看一下如何使用简单的socket接口编写一个端口扫描器。如下所示： 1234567891011121314151617181920212223242526#/usr/bin/python#_*_ coding:utf-8 _*_from __future__ import print_functionfrom socket import *def conn_scan(host, port): conn = socket(AF_INET, SOCK_STREAM) try: conn.connect((host, port)) print(host, port, 'is avaliable') except Exception as e: print(host, port, 'is not avaliable') finally: conn.close()def main(): host = '47.100.98.242' for port in range(80, 5000): conn_scan(host, port)if __name__ == '__main__': main() 在这个端口扫描的例子中，conn_scan用来判断端口是否可用。该函数尝试建立与目标主机和端口的连接，如果成功，打印一个端口开放的消息，否则，打印一个端口关闭的消息。除广使用socket套接字编程的方式判断端口是否可用以外，还可以使用Python标准库的telnet模块。该模块中包含了一个Telnet类，该类的对象表示一个telnet的连接。创建一 个Telnet对象并不会建立到远程主机的连接，需要显式地使用open方法建立连接。open方法接受三个参数，分别是主机名、端口号和超时时间。如下所示： 123456789101112131415161718192021222324252627#/usr/bin/python#_*_ coding:utf-8 _*_from __future__ import print_functionfrom socket import *import telnetlibdef conn_scan(host, port): t = telnetlib.Telnet() try: t.open(host,port,timeout=1) print(host, port, 'is avaliable') except Exception as e: print(host, port, 'is not avaliable') finally: t.close()def main(): host = '47.100.98.242' for port in range(20, 5000): conn_scan(host, port)if __name__ == '__main__': main() 对于上面这段程序，我们可以参考多线程的ping程序，以及使用生产者和消费者模型的ping程序，将这段程序扩展成多主机和多线程的端口扫描器。与ping程序不同的是，端U扫描需要用到两个参数，即主机地址和端口号。当我们有了主机的列表和端口号的列表以后，如何能够快速地得到所有主机与端口号的组合呢？对于这个问题，有多种不同的方法。其中比较方便的是使用列表推导。如下所示： 123456In [1]: l1 = ('a','b','c')In [2]: l2 = (22,80)In [3]: list([(x,y) for x in l1 for y in l2])Out[3]: [('a', 22), ('a', 80), ('b', 22), ('b', 80), ('c', 22), ('c', 80)] 使用列表推导虽然比较方便，但是，这个列表推导表达式本身比较复杂。因此，我们可以考虑使用itertools模块中的product函数。Python标准库的itertools模块提供了一组非常常用的函数，读者很有必要了解hertooU模块中提供的函数。在itertools模块中有一个名为product的函数，该函数用来返回多个可迭代对象的笛卡尔积。注意，product比前面的列表推导表达式更加通用，它可以返回多个可迭代对象的笛长尔积。这里的例子只需要计算两个可迭代对象的笛卡尔积。如下所示： 1234In [4]: from itertools import productIn [5]: list(product(l1,l2))Out[5]: [('a', 22), ('a', 80), ('b', 22), ('b', 80), ('c', 22), ('c', 80)] 有了主机和端口的组合以后，我们可以参照生产者和消费者模型的例子，开发一个多线程的端口扫描器。但是我们并没有必要这么做，因为除了使用多线程编程编写端口扫描器以外，还可以使用Python-nmap模块更加方便地进行端口扫描。 2、使用nmap扫描端口Python-nmap模块是对nmap命令的封装。nmap是知名的网络探测和安全扫描程序, 是Network Mapper的简称。nmap可以进行主机发现（Host Discovery)、端口扫描（Port Scanning)、版本侦测（Version Detection〉、操作系统侦测（Operating System Detection)，nmap是网络管理员必用的软件之一。nmap因为功能强大、跨平台、开源、文档丰富等诸多优点，在安全领域使用非常广泛。在使用之前，需要先安装nmap。如下所示： 1[root@bogon ~]# yum install nmap nmap的使用非常灵活，功能又很强大，因此nmap有很多命令行选项。使用nmap时， 首先需要确定要对哪些主机进行扫描，然后确定怎么进行扫描（如使用何种技术，对哪些端 口进行扫描）。nmap具有非常灵活的方式指定需要扫描的主机，我们可以使用nmap命令的-sL选项 来进行测试。-sL选项仅仅打印IP列表，不会进行任何操作。如下所示： 123456[root@bogon ~]# nmap -sL 47.100.98.242/80Starting Nmap 6.40 ( http://nmap.org ) at 2020-03-01 00:18 CSTIllegal netmask in \"47.100.98.242/80\". Assuming /32 (one host)Nmap scan report for 47.100.98.242Nmap done: 1 IP address (0 hosts up) scanned in 0.04 seconds nmap提供了非常灵活的方式来指定主机，包括同时指定多个IP、通过网段指定主机、通过通配符指定主机等。如下所示： 123456nmap -sL 47.100.98.242 14.215.177.39nmap -sL 47.100.98.*nmap -sL 47.100.98.242,243,245nmap -sL 47.100.98.242-250nmap -sL 47.100.98.* --exclude 47.100.98.242nmap -sL 47.100.98.242/30 除了上面指定主机的方式，我们也可以将IP地址保存到文本中，通过-iL选项读取文件中的IP地址。如下所示： 1nmap -iL ip.list （1）主机发现端口扫描是nmap的重点，除此之外，我们也可以使用nmap检查网络上所有在线的主机，实现类似前边小节中列出网络上所有活跃的主机的功能。使用-sP或-sn选项可以告诉nmap不要进行端口扫描，仅仅判断主机是否可达。如下所示： 1234567[root@bogon ~]# nmap -sP 47.100.98.*Starting Nmap 6.40 ( http://nmap.org ) at 2020-03-01 00:25 CSTNmap done: 256 IP addresses (0 hosts up) scanned in 206.44 seconds [root@bogon ~]# nmap -sn 47.100.98.*Starting Nmap 6.40 ( http://nmap.org ) at 2020-03-01 00:35 CSTNmap done: 256 IP addresses (0 hosts up) scanned in 205.38 seconds （2）端口扫描端口扫描是nmap最基本，也是最核心的功能，用于确定目标主机TCP/UDP端口的开放情况。不添加任何参数便是对主机进行端口扫描。默认情况下，nmap将会扫描1000个最常用的端口号。如下所示： 1234[root@bogon ~]# nmap 14.215.177.39Starting Nmap 6.40 ( http://nmap.org ) at 2020-03-01 00:42 CSTNote: Host seems down. If it is really up, but blocking our ping probes, try -PnNmap done: 1 IP address (0 hosts up) scanned in 3.09 seconds 在进行端口扫描时，nmap提供了大M的参数控制端口扫描。包括端口扫描协议、端口扫描类型、扫描的端口号。如下所示： 端口扫描协议：T (TCP)、U (UDP)、S (SCTP&gt;、P (IP); 端口扫描类型：-sS/sT/sA/sW/sM: TCP SYN/Connect()/ACK/Window/Maimon scans; 扫描的端口号：-p 80,443 -p 80-160 nmap中的端口扫描协议、扫描类型和端口号相关的选项，可以结合起来使用。如下所示： -p22; -p1-65535; -p U:53,111,137,T:21-25,80,139,8080,S:9 nmap通过探测将端口划分为6个状态，下表给出了每个状态的含义。 端口状态 状态含义 open 端口是开放的 closed 端口是关闭的 filtered 端口被防火墙IDS/IPS屏蔽，无法确认其状态 unfiltered 端口没有被屏蔽，但是否开放需要进一步确定 open|filtered 端口是开放的或被屏蔽 closed|filtered 端口是关闭的或被屏蔽 在进行端口扫描时，可以使用不同的端口扫描类型。常见的端口扫描类型如下： 123TCP SYNC SCAN:半开放扫描，这种类沏的扫描为发送一个SYN包，启动一个TCP会话，并等待响应的数据包。如果收到的是一个reset包，表明端口是关闭的; 如果收到的是一个SYNC/ACK包，则表示端口是打开的。TCP NULL SCAN: NULL扫描把TCP头中的所有标志位都设置为NULL。如果收到的是一个RST包，则表示相应的端口是关闭的。TCP FIN SCAN : TCP FIN扫描发送一个表示结束一个活跃的TCP连接的FIN包， 让对方关闭连接。如果收到了一个RST包，则表示相应的端口是关闭的。 TCPXMASSCAN: TCPXMAS扫描发送PSH、FIN、URG和TCP标志位被设置为1的数据包，如果收到一个RST包，则表示相砬端口是关闭的。 （3）版本侦测nmap在进行端口扫描时，还可以进行版本侦测。版本监测功能用于确定开放端口上运行的应用程序及版本信息。如下所示： 1nmap -sV 47.100.98.242 （4）操作系统监测操作系统侦测用于监测主机运行的操作系统类型及设备类型等信息。nmap拥有丰富的系统数据库，可以识别2600多种操作系统与设备类型。如下所示： 1nmap -sO 47.100.98.242 3、使用python-nmap进行端口扫描我们在上一小节中，花f较多的篇幅介绍nmap。Python的Python-nmap仅仅趋对nmap的封装，因此，要使用Python-nmap,必须先了解nmap。Python-nmap相对于nmap, 主要的改进在于对输出结果的处理。Python-nmap将nmap的输出结果保存到宇典之中，我们只需要通过Python的字典就可以获取到nmap的输出信息，不用像Shell脚本一样通过字符串处理和正则表达式来解析nmap的结果。Python-nmap将nmap的强大功能与Python语言优秀的表达能力进行了完美的结合，使用Python语言丰富的数据结构保存结果，以便后续继续进行处理，如使用Python-nmap生成相关的报告。Python-nmap是开源的库，因此，在使用之前需要手动进行安装。如下所示： 1pip3 install python-nmap Python-nmap的使用非常简单，我们只要创建一个PortScarmer对象，并调用对象的 scan方法就能够完成基本的nmap端口扫描。如下所示： 12345678910111213141516171819202122232425262728293031323334In [1]: import nmap In [2]: nm = nmap.PortScanner() In [3]: nm.scan('192.168.79.129','22-1000')Out[3]: &#123;'nmap': &#123;'command_line': 'nmap -oX - -p 22-1000 -sV 192.168.79.129', 'scaninfo': &#123;'tcp': &#123;'method': 'syn', 'services': '22-1000'&#125;&#125;, 'scanstats': &#123;'timestr': 'Mon Mar 2 16:31:17 2020', 'elapsed': '6.33', 'uphosts': '1', 'downhosts': '0', 'totalhosts': '1'&#125;&#125;, 'scan': &#123;'192.168.79.129': &#123;'hostnames': [&#123;'name': '192.168.79.129', 'type': 'PTR'&#125;], 'addresses': &#123;'ipv4': '192.168.79.129'&#125;, 'vendor': &#123;&#125;, 'status': &#123;'state': 'up', 'reason': 'localhost-response'&#125;, 'tcp': &#123;22: &#123;'state': 'open', 'reason': 'syn-ack', 'name': 'ssh', 'product': 'OpenSSH', 'version': '7.4', 'extrainfo': 'protocol 2.0', 'conf': '10', 'cpe': 'cpe:/a:openbsd:openssh:7.4'&#125;, 111: &#123;'state': 'open', 'reason': 'syn-ack', 'name': 'rpcbind', 'product': '', 'version': '2-4', 'extrainfo': 'RPC #100000', 'conf': '10', 'cpe': ''&#125;&#125;&#125;&#125;&#125; 当我们创建PortScanner对象时，Python-nmap会检査系统中是否已经安装了 nmap，如果没有安装，抛出PortScannerError异常。调用PortScanner对象的scan方法进行扫描以后就可以通过该类的其他方法获取本次扫描的信息。如命令行参数、主机列表、扫描的方法等。如下所示： 12345678In [4]: nm.command_line() Out[4]: 'nmap -oX - -p 22-1000 -sV 192.168.79.129'In [5]: nm.scaninfo() Out[5]: &#123;'tcp': &#123;'method': 'syn', 'services': '22-1000'&#125;&#125;In [6]: nm.all_hosts() Out[6]: ['192.168.79.129'] Python-nmap还提供了以主机地址为键，获取单台主机的详细信息。包括获取主机网络状态、所有的协议、所有打开的端口号，端口号对应的服务等。如下所示： 123456789101112131415161718192021222324252627282930In [7]: nm['192.168.79.129'].state()Out[7]: 'up'In [8]: nm['192.168.79.129'].all_protocols()Out[8]: ['tcp']In [9]: nm['192.168.79.129'].keys() Out[9]: dict_keys(['hostnames', 'addresses', 'vendor', 'status', 'tcp']) In [10]: nm['192.168.79.129']['tcp'][22]Out[10]: &#123;'state': 'open', 'reason': 'syn-ack', 'name': 'ssh', 'product': 'OpenSSH', 'version': '7.4', 'extrainfo': 'protocol 2.0', 'conf': '10', 'cpe': 'cpe:/a:openbsd:openssh:7.4'&#125;In [11]: nm['192.168.79.129']['tcp'][111]Out[11]: &#123;'state': 'open', 'reason': 'syn-ack', 'name': 'rpcbind', 'product': '', 'version': '2-4', 'extrainfo': 'RPC #100000', 'conf': '10', 'cpe': ''&#125; Python-nmap是对nmap的Python封装，因此我们也可以通过Python-nmap指定nmap命令的复杂选项。如下所示： 1nm.scan(hosts='192.168.79.129/24',arguments='-n -sP -PE -PA21,23,80,3389') 三、使用IPy进行IP管理在网络设计中，首先要做的就是规划IP地址。IP地址规划的好坏直接影响路由算法的效率，包括网络性能和扩展性。在IP地址规划中，需要进行大量的IP地址计算，包括网段、网络掩码、广播地址、子网数、IP类型等计算操作。在大量的计算操作中，如果没有一个好的工具，计算IP地址是一个很无趣有容易出错的事情。在Perl语言中，可以使用NET::IP模块，在Python语言中，可以使用开源的IPy模块进行操作。 1、IPy模块介绍IPy模块是一个处理IP地址的模块，它能够自动识别IP地址的版本、IP地址的类型。使用IPy模块，可以方便地进行IP地址的计算。 IPy模块是第三方的开源模块，因此，在使用之前需要进行安装。直接使用pip安装即可： 1pip3 install ipy 2、IPy模块的基本使用IPy模块有一个IP类，这个类几乎可以接受任何格式的IP地址和网段。如下所示： 123456789101112131415161718In [1]: import IPy In [2]:from IPy import IP In [3]: IP(0x7f000001) Out[3]: IP('127.0.0.1')In [4]: IP('127.0.0.1') Out[4]: IP('127.0.0.1')In [5]: IP('127.0.0.0/30') Out[5]: IP('127.0.0.0/30')In [6]: IP('1080:0:0:0:8:800:200C:417A')Out[6]: IP('1080::8:800:200c:417a')In [7]: IP('127.0.0.0-127.255.255.255')Out[7]: IP('127.0.0.0/8') IP类包含了许多的方法，用来进行灵活的IP地址操作。例如： （1）version:获取IP地址的版本12345678In [9]: IP('127.0.0.0-127.255.255.255') Out[9]: IP('127.0.0.0/8')In [10]: IP('10.0.0.0/8').version() Out[10]: 4In [11]: IP('::1').version() Out[11]: 6 （2）len:得到子网IP地址的个数12345In [12]: IP('127.0.0.0/30').len() Out[12]: 4 In [13]: IP('127.0.0.0/28').len() Out[13]: 16 （3）iptype:返回IP地址的类型12345In [14]: IP('127.0.0.1').iptype() Out[14]: 'LOOPBACK'In [15]: IP('8.8.8.8').iptype() Out[15]: 'PUBLIC' （4）int:返回IP地址的整数形式12In [16]: IP('8.8.8.8').int() Out[16]: 134744072 （5）strHex:返回IP地址的十六进制形式12In [17]: IP('8.8.8.8').strHex() Out[17]: '0x8080808' （6）strBin:返回IP地址的二进制形式12In [18]: IP('8.8.8.8').strBin()Out[18]: '00001000000010000000100000001000' 有一个方便的函数能够将IP转换为不同的格式，在工作环境中将会非常有用。例如，以数宇的形式在数据库中存储IP地址，在数据库中存储IP地址有两种形式，第一种是以变长字符串的形式将IP地址保存到数据库中，另一种是将IP地址转换为整数以后保存到数据库中。将IP地址转换为整数进行存储能够有效地节省存储空间，提高数据库的存储效率和访问速度。因此，在最佳实践中，我们一般将IP地址以数字的形式保存到数据库中。需要 IP地址时，再将数字形式的IP地址转换为字符串格式的IP地址。这个需求十分常见，因 此，MySQL提供了两个函数，分别用以将字符串形式的IP地址转换为数据格式的IP地址，以及将数字格式的IP地址转换为字符串形式的IP地址。如下所示： 123456789101112131415mysql&gt; select INET_ATON('10.166.224.14');+----------------------------+| INET_ATON('10.166.224.14') |+----------------------------+| 178708494 |+----------------------------+1 row in set (0.00 sec)mysql&gt; select INET_NTOA('178708494');+------------------------+| INET_NTOA('178708494') |+------------------------+| 10.166.224.14 |+------------------------+1 row in set (0.00 sec) 除了使用MySQL自带的函数以外，我们也可以使用IP类提供的int方法将字符串形式的IP地址转换为数字形式的IP地址。要将数字形式的IP地址转换会字符串形式的IP地址，可以直接使用数字的方式创建IP对象。如下所示： 12345In [9]: IP('178708494') Out[9]: IP('10.166.224.14')In [10]: '&#123;0&#125;'.format(IP(\"178708494\"))Out[11]: '10.166.224.14' 3、网段管理IP类的构造函数可以接受不同格式的IP地址，也可以接受网段。如下所示： 12345678910In [1]: from IPy import IP In [2]: IP('127.0.0.0/24') Out[2]: IP('127.0.0.0/24')In [3]: IP('127.0.0.0-127.255.255.255')Out[3]: IP('127.0.0.0/8')In [4]: IP('127.0.0.0/127.255.255.255')Out[4]: IP('127.0.0.0/31') 网段包含多个IP地址，我们可以直接使用len方法或者Python内置的len函数得到网段中IP地址的个数，也可以直接使用for循环迭代网段，以此遍历各个IP。如下所示： 12345678910111213141516171819202122232425262728293031323334353637In [1]: from IPy import IP In [2]: IP('127.0.0.0/24') Out[2]: IP('127.0.0.0/24')In [3]: IP('127.0.0.0-127.255.255.255')Out[3]: IP('127.0.0.0/8')In [4]: IP('127.0.0.0/127.255.255.255')Out[4]: IP('127.0.0.0/31')In [5]: ips = IP('10.166.224.144/28')In [6]: ips.len() Out[6]: 16In [7]: len(ips) Out[7]: 16In [8]: [ip for ip in ips] Out[8]: [IP('10.166.224.144'), IP('10.166.224.145'), IP('10.166.224.146'), IP('10.166.224.147'), IP('10.166.224.148'), IP('10.166.224.149'), IP('10.166.224.150'), IP('10.166.224.151'), IP('10.166.224.152'), IP('10.166.224.153'), IP('10.166.224.154'), IP('10.166.224.155'), IP('10.166.224.156'), IP('10.166.224.157'), IP('10.166.224.158'), IP('10.166.224.159')] IP类有一个名为strNormal的方法，该方法接受一个wantprefixlen参数，参数的合法取值为0~3，每一个取值代表一种网段的显示方式。如下所示： 1234567891011In [12]: ips.strNormal(0) Out[12]: '10.166.224.144'In [13]: ips.strNormal(1) Out[13]: '10.166.224.144/28'In [14]: ips.strNormal(2) Out[14]: '10.166.224.144/255.255.255.240'In [15]: ips.strNormal(3) Out[15]: '10.166.224.144-10.166.224.159' 通过IP类，我们也可以方便地判断一个IP是否属于一个网段，判断子网是否包含于另一个网段中，以及两个网段是否有重叠。如下所示： 12345678In [16]: '10.166.224.144' in IP('10.166.224.144/28')Out[16]: TrueIn [17]: IP('10.166.224.144/29') in IP('10.166.224.144/28')Out[17]: TrueIn [18]: IP('10.166.224.0/28').overlaps('10.166.224.144/28')Out[18]: 0 对于网段，我们可以方便地获取网络地址掩码以及网络的广播地址。如下所示： 12345In [22]: ips.netmask() Out[22]: IP('255.255.255.240')In [23]: ips.broadcast() Out[23]: IP('10.166.224.159') 四、使用dnspython解析DNS1、dnspython简介与安装dnspython是Python实现的一个DNS工具集，它支持几乎所有的记录类型，可以用于查询、传输并动态更新ZONE信息，同时支持TSIG（事务签名）验证消息和EDNS0（扩展DNS）。使用dnspython可以代替Linux命令行下的nslookup以及dig等工具。 dnspython是第三方的开源模块，因此，使用之前需要先进行安装： 1pip3 install dnspython 2、使用dnspython进行域名解析dnspython提供了丰富的API，其中，高层次的API根据名称和类型执行查询操作，低层次的API可以直接更新ZONE信息、消息、名称和记录。在所有的API中，最常使用的是域名查询。dnspython提供了一个DNS解析类resolver，使用它的query方法可以实现域名的查询功能。 1dns.resolver.query(qname,rdtype=1,rdclass=1,tcp=False,source=None,raise_on_no_answer=True,source_port=0) query方法各参数的含义如下： qname:査询的域名；rdtype:指定RR资源； 12345A:地址记录（Address),返回域名指向的IP地址；NS:域名服务器记录（Name Server)，返回保存下一级域名信息的服务器地址。该记录只能设罝为域名，不能设置为IP地址；MX:邮件记录（Mail exchange),返回接收电子邮件的服务器地址；CNAME:规范名称记录（Canonical Name)，别名记录，实现域名间的映射；PTR:逆向査询记录（Pointer Record),反向解析，与A记录相反，将IP地址转换为主机名。 rdclass:网络类型；tcp:指定査询是否启用TCP协议；source:査询源的地址；source_port:査询源的端口 ;raise_on_no_answer:指定査询无应答时是否触发异常，默认为True。 在使用dnspython查询DNS相关信息之前，我们先简单了解一下dig命令，以便对照查看Python程序的输出结果与dig命令的输出结果。dig的全称是domain information groper,它是一个灵活探测DNS的工具，可以执行DNS査找，并显示从查询的名称服务器返回的答案。由于dig命令灵活易用、输出明确， 因此，大多数DNS管理员都使用dig解决DNS问题。在我的主机上运行dig命令査找dnspython.org域名的信息。运行结果如下： 1234567891011121314151617181920212223242526272829[root@192 ~]# dig qiniu.lexizhi.com; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-72.el7 &lt;&lt;&gt;&gt; qiniu.lexizhi.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 35907;; flags: qr rd ra; QUERY: 1, ANSWER: 12, AUTHORITY: 0, ADDITIONAL: 0;; QUESTION SECTION:;qiniu.lexizhi.com. IN A;; ANSWER SECTION:qiniu.lexizhi.com. 5 IN CNAME www.lexizhi.com.qiniudns.com.www.lexizhi.com.qiniudns.com. 5 IN CNAME dt003.china.line.qiniudns.com.dt003.china.line.qiniudns.com. 5 IN CNAME tinychinacdnweb.qiniu.com.w.kunlunno.com.tinychinacdnweb.qiniu.com.w.kunlunno.com. 5 IN A 150.138.180.231tinychinacdnweb.qiniu.com.w.kunlunno.com. 5 IN A 150.138.180.234tinychinacdnweb.qiniu.com.w.kunlunno.com. 5 IN A 150.138.180.232tinychinacdnweb.qiniu.com.w.kunlunno.com. 5 IN A 150.138.180.233tinychinacdnweb.qiniu.com.w.kunlunno.com. 5 IN A 219.147.157.66tinychinacdnweb.qiniu.com.w.kunlunno.com. 5 IN A 150.138.180.228tinychinacdnweb.qiniu.com.w.kunlunno.com. 5 IN A 150.138.180.235tinychinacdnweb.qiniu.com.w.kunlunno.com. 5 IN A 150.138.180.229tinychinacdnweb.qiniu.com.w.kunlunno.com. 5 IN A 150.138.180.230;; Query time: 633 msec;; SERVER: 192.168.79.2#53(192.168.79.2);; WHEN: 一 3月 02 17:29:51 CST 2020;; MSG SIZE rcvd: 300 在Python代码中，可以使用dnspython查询A记录。如下所示： 123456from __future__ import print_functionimport dns.resolverdata = dns.resolver.query('www.lexizhi.com', 'A')for item in data: print(item) Python程序的输出结果如下： 147.100.98.242 使用dnspython实现NS记录，查询方法如下： 123456from __future__ import print_functionimport dns.resolverdata = dns.resolver.query('dnspython.org', 'NS')for item in data: print(item) Python程序查询NS记录的结果如下： 1234ns-343.awsdns-42.com.ns-518.awsdns-00.net.ns-1253.awsdns-28.org.ns-2020.awsdns-60.co.uk. 从输出结果来看，使用dig命令或dnspython模块都是一样的。如果在命令行操作，建议使用dig命令。如果要使用程序管理DNS或查询DNS的内容，则推荐使用dnspython模块。 五、网络嗅探器ScapyScapy是一个Python语言编写的工具，使用Scapy可以发送、嗅探、剖析和伪造网络数据报。Scapy涉及比较底层的网络协议，因此，不可避免地导致Scapy的接口复杂。虽然 Scapy的接口复杂，但整体思路却非常简单，就是发送数据报和接收数据报。在发送数据报时，Scapy提供了相关的辅助类来帮助我们构造数据报，在接收数据报时，Scapy也提供了相应的函数来帮助我们过滤和解析数据报。 在这一小节中，首先我们将介绍Scapy的功能和安装方式，然后介绍Scapy的基本使用，接着介绍如何使用Scapy发送数据报，并通过 —个DNS査询的例子演示Scapy发送数据报，最后介绍如何使用Scapy进行网络嗅探，并通过一个抓取敏感信息的例子来演示Scapy的网络嗅探。 1、Scapy简介与安装Scapy是一个强大的交互式数据报处理程序，它能够伪造或者解码大量的网络协议数据报，能够发送、捕捉、匹配请求和回复数据报。Scapy可以轻松处理大多数经典任务，如端口扫描、路由跟踪、探测、攻击或网络发现等。使用Scapy可以替代hping, arpspoof，arp-sk, arping，p0f等功能，甚至可以替代nmap, tcpdump和tshark的部分功能。此外，Scapy 还有很多其他工具没有的优秀特性，如发送无效数据帧、注入修改的802.11数据帧、在 WEP上解码加密通道（VOIP)、ARP缓存攻击（VLAN)等。Scapy是使用Python语言开发的丁.具，因此，我们可以直接使用pip安装： 1pip3 install scapy Scapy运行时要对网络接口进行控制，所以需要root权限。在这一小节的例子中， 我们都使用root用户来运行Scapy程序或与Scapy相关的Python程序。Scapy提供了非常丰富的功能，不同的功能依赖不同的软件。启动Scapy命令行工具时，Scapy会进行相应的检査并给出提示。如下所示： 1234567891011121314151617181920212223242526[root@192 ~]# scapyINFO: Can't import matplotlib. Won't be able to plot.INFO: Can't import PyX. Won't be able to use psdump() or pdfdump().WARNING: No route found for IPv6 destination :: (no default route?)INFO: Can't import python-cryptography v1.7+. Disabled WEP decryption/encryption. (Dot11)INFO: Can't import python-cryptography v1.7+. Disabled IPsec encryption/authentication. aSPY//YASa apyyyyCY//////////YCa | sY//////YSpcs scpCY//Pp | Welcome to Scapy ayp ayyyyyyySCP//Pp syY//C | Version 2.4.3 AYAsAYYYYYYYY///Ps cY//S | pCCCCY//p cSSps y//Y | https://github.com/secdev/scapy SPPPP///a pP///AC//Y | A//A cyP////C | Have fun! p///Ac sC///a | P////YCpc A//A | Craft packets before they craft scccccp///pSP///p p//Y | you. sY/////////y caa S//P | -- Socrate cayCyayP//Ya pY/Ya | sY/PsY////YCc aC//Yp sc sccaCY//PCypaapyCP//YSs spCPY//////YPSps ccaacs using IPython 7.12.0&gt;&gt;&gt; 例如，使用Scapy生成图形化的示意图需要安装matplotlib库，但没有安装matplotlib并不影响Scapy的基本使用。 2、Scapy的基本使用在本教程中，我们会介绍Scapy的一些基本用法，完整的使用方法可以参考Scapy的官方文档http://www.secdev.org/projects/scapy/doc/usage.html。 我们有两种方式运行Scapy，一种是直接启动Scapy进入一个交互式界面，另一种是在Python程序中调用Scapy提供的功能。与其他软件不同的是，Scapy的交互模式其实就是Python的交互模式。因此我们可以在Scapy的交互模式下导入Python的包，使用Python的语法，执行Python中的语句。如下所示： 1234&gt;&gt;&gt; import sys &gt;&gt;&gt; print(sys.version) 3.8.1 (default, Jan 14 2020, 10:59:16) [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] 既然知道了Scapy的交换模式是Python的交换模式这个事实，那我们就可以轻易地将Scapy交换模式中的代码放置在Python的源文件中，使用Python程序的方式进行软件开发。要在Python程序中使用Scapy的功能，只需要导入scapy.all模块即可。如下所示： 1234567891011121314&gt;&gt;&gt; import sys &gt;&gt;&gt; print(sys.version) 3.8.1 (default, Jan 14 2020, 10:59:16) [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)]&gt;&gt;&gt; from __future__ import print_function &gt;&gt;&gt; from scapy.all import * &gt;&gt;&gt; print(ls()) AH : AHAKMSuite : AKM suiteARP : ARPASN1P_INTEGER : NoneASN1P_OID : NoneASN1P_PRIVSEQ : NoneASN1_Packet : None 在Scapy的交互式工具中，我们可以通过ls()显示Scapy支持的所有协议、lsc()列出Scapy支持的所有命令、conf显示所有的配置信息、help(cmd)显示某一命令的使用帮助等。如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&gt;&gt;&gt; ls() AH : AHAKMSuite : AKM suiteARP : ARPASN1P_INTEGER : NoneASN1P_OID : NoneASN1P_PRIVSEQ : NoneASN1_Packet : None &gt;&gt;&gt; lsc() IPID_count : Identify IP id values classes in a list of packetsarpcachepoison : Poison target's cache with (your MAC,victim's IP) couplearping : Send ARP who-has requests to determine which hosts are uparpleak : Exploit ARP leak flaws, like NetBSD-SA2017-002.bind_layers : Bind 2 layers on some specific fields' values.bridge_and_sniff : Forward traffic between interfaces if1 and if2, sniff and returnchexdump : Build a per byte hexadecimal representationcomputeNIGroupAddr : Compute the NI group Address. Can take a FQDN as input parametercorrupt_bits : Flip a given percentage or number of bits from a stringcorrupt_bytes : Corrupt a given percentage or number of bytes from a stringdefrag : defrag(plist) -&gt; ([not fragmented], [defragmented],defragment : defragment(plist) -&gt; plist defragmented as much as possible dhcp_request : Send a DHCP discover request and return the answerdyndns_add : Send a DNS add message to a nameserver for \"name\" to have a new \"rdata\"dyndns_del : Send a DNS delete message to a nameserver for \"name\"etherleak : Exploit Etherleak flawexplore : Function used to discover the Scapy layers and protocols.fletcher16_checkbytes: Calculates the Fletcher-16 checkbytes returned as 2 byte binary-string.fletcher16_checksum : Calculates Fletcher-16 checksum of the given buffer.fragleak : --fragleak2 : --fragment : Fragment a big IP datagramfuzz : getmacbyip : Return MAC address corresponding to a given IP addressgetmacbyip6 : Returns the MAC address corresponding to an IPv6 addresshexdiff : Show differences between 2 binary strings&gt;&gt;&gt; help(sniff)Help on function sniff in module scapy.sendrecv:sniff(*args, **kwargs) Sniff packets and return a list of packets. Args: count: number of packets to capture. 0 means infinity. store: whether to store sniffed packets or discard them prn: function to apply to each packet. If something is returned, it is displayed. --Ex: prn = lambda x: x.summary() session: a session = a flow decoder used to handle stream of packets. e.g: IPSession (to defragment on-the-flow) or NetflowSession filter: BPF filter to apply. 【按q退出】 如果你对网络编程特别感兴趣，你一定会喜欢上Scapy。我们不但可以使用Scapy嗅探和发送数据报，甚至还可以使用Scapy学习计算机网络相关知识。例如，我们可以使用ls查看协议的详细格式。如下所示： 12345678910&gt;&gt;&gt; ls(ARP) hwtype : XShortField = (1)ptype : XShortEnumField = (2048)hwlen : FieldLenField = (None)plen : FieldLenField = (None)op : ShortEnumField = (1)hwsrc : MultipleTypeField = (None)psrc : MultipleTypeField = (None)hwdst : MultipleTypeField = (None)pdst : MultipleTypeField = (None) 3、使用Scapy发送数据报Scapy的数据报遵循了网络协议中经典的TCP/IP四层模型，即链路层、网络层、运输层和应用层。Scapy为每个层协议都提供了辅助类，我们要做的就是把这些类实例化并修改对象的取值，以此来构造数据报。每一层都可以通过类调用创建相应的数据报，如IP()、TCP()、UDP()等，不同层之间通过“/”来连接。如下所示： 123&gt;&gt;&gt; packet1 = IP(dst='10.166.244.14')&gt;&gt;&gt; packet2 = IP(dst='10.166.244.14')/TCP(dport=80)&gt;&gt;&gt; packet3 = IP(dst='10.166.244.14')/ICMP() display方法可以查看当前数据报的内容，即各个参数的取值情况。例如，下面就显示了我们构造的第一个数据报中各个字段的取值。 123456789101112131415&gt;&gt;&gt; packet1.display() ###[ IP ]### version= 4 ihl= None tos= 0x0 len= None id= 1 flags= frag= 0 ttl= 64 proto= hopopt chksum= None src= 192.168.79.129 dst= 10.166.244.14 \\options\\ 字段都有默认值，如果我们建立一个类的实例，没有传给它任何参数，那么它的参数取值就是默认值。如果传递了相应的参数，就使用用户传递的参数。如果使用del删除了某个参数，就恢复了默认值。如下所示： 12345678&gt;&gt;&gt; packet1.dst '10.166.244.14'&gt;&gt;&gt; packet1.ttl = 32 &gt;&gt;&gt; packet1.ttl 32&gt;&gt;&gt; del packet1.ttl &gt;&gt;&gt; packet1.ttl 64 如果我们没有提供相应的参数取值（user set fields)，参数将使用默认值（default fields）。之后，如果我们在这个类的上层进行操作 (比如IP的上面定义TCP)，那么，数据报的取值将由上层协议进行覆盖。 4、使用Scapy构造DNS查询请求下面以DNS解析为例，介绍如何使用Scapy构造数据报并发送请求。假设我们使用的 DNS服务器地址为8.8,8.8,现在，我们需要获取百度（&lt;www.baidu.com&gt; )的IP地址。 为了获取百度的IP地址，我们需要创建一个DNS的请求包。如下所示： 1&gt;&gt;&gt; dns=DNS(rd=1,qd=DNSQR(qname='www.baidu.com')) DNS是一个应用层协议，底层可以使用UDP或TCP协议。无论是TCP协议还是UDP 协议，都依赖IP协议进行网络报传输。因此，完整的DNS清求数据报如下所示： 12345&gt;&gt;&gt; packet = sr1(IP(dst='8.8.8.8')/UDP()/dns)Begin emission:...Finished sending 1 packets...*Received 6 packets, got 1 answers, remaining 0 packets 在这个例子中，我们使用sr1函数发送和接收数据报，sr1在三层发送数据报，并且接收第一个回复。收到回复后，我们可以使用show方法来查看数据报的详细内容。如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&gt;&gt;&gt; packet[DNS].show()###[ DNS ]### id= 0 qr= 1 opcode= QUERY aa= 0 tc= 0 rd= 1 ra= 1 z= 0 ad= 0 cd= 0 rcode= ok qdcount= 1 ancount= 3 nscount= 0 arcount= 0 \\qd\\ |###[ DNS Question Record ]### | qname= 'www.baidu.com.' | qtype= A | qclass= IN \\an\\ |###[ DNS Resource Record ]### | rrname= 'www.baidu.com.' | type= CNAME | rclass= IN | ttl= 332 | rdlen= None | rdata= 'www.a.shifen.com.' |###[ DNS Resource Record ]### | rrname= 'www.a.shifen.com.' | type= CNAME | rclass= IN | ttl= 93 | rdlen= None | rdata= 'www.wshifen.com.' |###[ DNS Resource Record ]### | rrname= 'www.wshifen.com.' | type= A | rclass= IN | ttl= 58 | rdlen= None | rdata= 103.235.46.39 ns= None ar= None DNS应答包里面包含了非常详细的信息。例如，在这个应答中我们可以看到，DNS支持递归查询（ra取值为1表示DNS服务器支持递归查询，ra取值为0表示不支持递归查询）。百度的域名解析给出了 3个结果（ancount取值为3）。在这个例子中，我们通过手动构造数据报的方式，正确发送了DNS请求，解析了百度的IP地址。在构造数据报时，如果有应用的数据，数据部分可以直接使用字符。如下所示： 12345678&gt;&gt;&gt; a=Ether()/IP(dst='www.slashdot.org')/TCP()/\"GET /index.html HTTP/1.0 \\n\\n\" &gt;&gt;&gt; hexdump(a)0000 00 50 56 FE 43 5E 00 0C 29 58 4A 96 08 00 45 00 .PV.C^..)XJ...E.0010 00 43 00 01 00 00 40 06 6C 12 C0 A8 4F 81 D8 69 .C....@.l...O..i0020 26 0F 00 14 00 50 00 00 00 00 00 00 00 00 50 02 &amp;....P........P.0030 20 00 AF 0F 00 00 47 45 54 20 2F 69 6E 64 65 78 .....GET /index0040 2E 68 74 6D 6C 20 48 54 54 50 2F 31 2E 30 20 0A .html HTTP/1.0 .0050 0A 如果我们安装了PyX，还可以直接将数据报dump成一个PostScript或PDF文件。 5、使用Scapy进行网络嗅探Scapy除了可以伪造数据报并接收响应结果以外，还可以用于数据报嗅探。对数据报进行嗅探的函数为sniff，sniff函数的详细使用方法如下： 1sniff(filter=\"\", iface=\"any\", prn=function, count=N) sniff函数的参数说明如下： filter:用来表示想要捕获数据报类型的过滤器，如只捕获ICMP数据报，则filter取值为“ICMP”，只捕获80端口的TCP数据报，则filter取值为“TCP and （port 80)”； iface:设置嗅探器所要嗅探的网卡，默认对所有网卡进行嗅探； pm:指定嗅探到符合过滤器条件的数据报时所调用的回调函数，这个回调函数只接受一个参数，即收到的数据报。 count:指定需要嗅探的数据报的个数。 123def pack_callback(packet): print(packet.show())sniff(prn=pack_calback, iface=\"ens33\", count=1) 下面是一个非常简单的sniff使用示例。在这个例子中，我们仅仅捕获三个ICMP的数据报，并且直接打印数据报的信息。前面说过，Scapy的交互模式就是Python的交互模式，因此，我们可以直接使用Python的库、语法和语句。sniff要求prn是一个回调函数，因此，我们传递给prn参数的是一个Lambda函数。如下所示： 1234&gt;&gt;&gt; a = sniff(filter=\"tcp\", prn=lambda x:x.summary(), count=3) Ether / IP / TCP 192.168.79.1:65134 &gt; 192.168.79.129:ssh PA / RawEther / IP / TCP 192.168.79.129:ssh &gt; 192.168.79.1:65134 PA / RawEther / IP / TCP 192.168.79.129:ssh &gt; 192.168.79.1:65134 PA / Raw","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"使用Python监控Linux系统","slug":"使用Python监控Linux系统","date":"2020-05-11T16:00:00.000Z","updated":"2020-05-12T09:10:20.608Z","comments":true,"path":"使用Python监控Linux系统.html","link":"","permalink":"https://pdxblog.top/%E4%BD%BF%E7%94%A8Python%E7%9B%91%E6%8E%A7Linux%E7%B3%BB%E7%BB%9F.html","excerpt":"","text":"使用Python监控Linux系统Linux下有许多使用Python语言编写的监控工具，如inotify-sync、dstat和glances。此外，如果要根据业务编写简单的监控脚本，很多工程师也会选择Python语言。Python语言是一门简单易学/语法清晰/表达能力强的编程语言，非常适合于编写监控程序的场景。使用Python语言编写监控程序具有以下几个优势： 1、Python语言开发效率高。Python语言有自己的优势与劣势，使用Python开发监控程序是一个充分发挥Python优势，避免Python劣势的领域。对于监控程序来说，能够利用Python语言开发效率高的优势尽快完成程序的编写工作。同时，监控程序也不要求性能，因此避免了Python语言性能不如C、C++和Java的劣势。 2、Python语言表达能力强。相信任何一个学习Linux的工程师都使用过shell脚本编写过监控程序。虽然Linux下有很多监控工具，也有很多文本处理程序，但是获取监控与解析结果是完全不同的工具。解析监控结果的程序不理解监控程序输出结果的具体含义。Python语言中有非常丰富的数据结构，可以用各种方式保存监控结果，以便后续处理。 3、利用第三方库开发监控程序。Python的标准库本身非常强大，被称为“连电池都包含在内”。对于一个问题，如果标准库没有提供相应的工具，那么也会有开源的项目来填补这个空白。监控程序正式这样一种情况，在Python语言中，具有非常成熟的第三方库帮助开发者简化监控程序的编写工作。 一、Python编写的监控工具我们将介绍两个Python语言编写的监控工具，分别是dstat和glances。 1、多功能系统资源统计工具dstatdstat是一个用Python语言实现的多功能系统资源统计工具，用来取代Linux下的vmstat、iostat、netstat、和ifstat等命令。并且，dstat克服了这些命令的限制，增加了额外的功能，以及更多的计数器与更好的灵活性。dstat可以在一个界面上展示非常全面的监控信息，因此，在系统监控、基准测试和故障排除等应用场景下特别有用。 我们可以使用dstat监控所有系统资源的使用情况，并且可以结合不同的场景定制监控的资源。例如，在同一时间段以相同的时间频率比较网络带宽与磁盘的吞吐率。 dstat将以列表的形式显示监控信息，并且使用不同的颜色进行输出，以可读性较强的单位展示监控数值。例如，对于字节数值，dstat自动根据数值的大小，以K、M、G等单位进行显示，避免了开发者使用其他命令时因为数值太大造成的困惑和错误。此外，使用dstat还可以非常方便地编写插件，用来收集默认情况下没有收到的监控信息。dstat是专门为人们实时查看监控信息设计的，因此默认将监控结果输出到屏幕终端。我们也可以将监控信息以CSV格式输出到文件，以便后续处理。 （1）dstat介绍作为一个多功能系统资源统计工具，dstat具有以下特性： 1234567891011121314结合了vmstat，iostat，ifstat，netstat以及更多的信息实时显示统计情况在分析和排障时可以通过启用监控项并排序模块化设计使用python编写的，更方便扩展现有的工作任务容易扩展和添加你的计数器（请为此做出贡献）包含的许多扩展插件充分说明了增加新的监控项目是很方便的可以分组统计块设备/网络设备，并给出总数可以显示每台设备的当前状态极准确的时间精度，即便是系统负荷较高也不会延迟显示显示准确地单位和和限制转换误差范围用不同的颜色显示不同的单位显示中间结果延时小于1秒支持输出CSV格式报表，并能导入到Gnumeric和Excel以生成图形 如果操作系统默认没有安装dstat，那么需要我们手动进行安装。如下所示： 1yum install dstat 查看dstat命令的帮助信息与支持选项，如下所示： 1dstat --help dstat命令的–version选项，处理显示dstat的版本以外，还会显示操作系统的版本、Python语言的版本、CPU的个数，以及dstat支持的插件列表等详细信息。如下所示： 123456789101112131415161718192021222324252627282930313233[root@192 ~]# dstat --versionDstat 0.7.2Written by Dag Wieers &lt;dag@wieers.com&gt;Homepage at http://dag.wieers.com/home-made/dstat/Platform posix/linux2Kernel 3.10.0-957.el7.x86_64Python 2.7.5 (default, Oct 30 2018, 23:45:53) [GCC 4.8.5 20150623 (Red Hat 4.8.5-36)]Terminal type: xterm (color support)Terminal size: 16 lines, 83 columnsProcessors: 1Pagesize: 4096Clock ticks per secs: 100internal: aio, cpu, cpu24, disk, disk24, disk24old, epoch, fs, int, int24, io, ipc, load, lock, mem, net, page, page24, proc, raw, socket, swap, swapold, sys, tcp, time, udp, unix, vm/usr/share/dstat: battery, battery-remain, cpufreq, dbus, disk-tps, disk-util, dstat, dstat-cpu, dstat-ctxt, dstat-mem, fan, freespace, gpfs, gpfs-ops, helloworld, innodb-buffer, innodb-io, innodb-ops, lustre, memcache-hits, mysql-io, mysql-keys, mysql5-cmds, mysql5-conn, mysql5-io, mysql5-keys, net-packets, nfs3, nfs3-ops, nfsd3, nfsd3-ops, ntp, postfix, power, proc-count, qmail, rpc, rpcd, sendmail, snooze, squid, test, thermal, top-bio, top-bio-adv, top-childwait, top-cpu, top-cpu-adv, top-cputime, top-cputime-avg, top-int, top-io, top-io-adv, top-latency, top-latency-avg, top-mem, top-oom, utmp, vm-memctl, vmk-hba, vmk-int, vmk-nic, vz-cpu, vz-io, vz-ubc, wifi[root@192 ~]# 除了使用dstat命令的–version选项查看dstat的详细信息获取可支持的插件以外，还可以使用dstat命令的–list选项获取dstat的插件列表。如下所示： 123456789101112[root@192 ~]# dstat --listinternal: aio, cpu, cpu24, disk, disk24, disk24old, epoch, fs, int, int24, io, ipc, load, lock, mem, net, page, page24, proc, raw, socket, swap, swapold, sys, tcp, time, udp, unix, vm/usr/share/dstat: battery, battery-remain, cpufreq, dbus, disk-tps, disk-util, dstat, dstat-cpu, dstat-ctxt, dstat-mem, fan, freespace, gpfs, gpfs-ops, helloworld, innodb-buffer, innodb-io, innodb-ops, lustre, memcache-hits, mysql-io, mysql-keys, mysql5-cmds, mysql5-conn, mysql5-io, mysql5-keys, net-packets, nfs3, nfs3-ops, nfsd3, nfsd3-ops, ntp, postfix, power, proc-count, qmail, rpc, rpcd, sendmail, snooze, squid, test, thermal, top-bio, top-bio-adv, top-childwait, top-cpu, top-cpu-adv, top-cputime, top-cputime-avg, top-int, top-io, top-io-adv, top-latency, top-latency-avg, top-mem, top-oom, utmp, vm-memctl, vmk-hba, vmk-int, vmk-nic, vz-cpu, vz-io, vz-ubc, wifi[root@192 ~]# 直接在终端输入dstat命令，dstat将以默认参数运行。默认情况下，dstat会收集CPU、磁盘、网络、交换页和系统消息，并以1秒钟1次的频率进行输出，直到我们按Ctrl+C结束。 （2）dstat常用选项如下所示，dstat会提示我们没有指定任何参数，因此使用-cdngy参数运行。 123456789101112131415[root@192 ~]# dstatYou did not select any stats, using -cdngy by default.----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--usr sys idl wai hiq siq| read writ| recv send| in out | int csw 9 3 88 0 0 0| 219k 60k| 0 0 |2682B 5427B| 163 1542 20 5 75 0 0 0| 0 0 | 66B 894B| 0 0 | 326 4218 19 6 75 0 0 0| 0 0 | 126B 410B| 0 0 | 330 4431 18 7 75 0 0 0| 0 0 | 66B 416B| 0 0 | 341 2998 20 6 74 0 0 0| 0 0 | 66B 350B| 0 0 | 340 4098 19 6 75 0 0 0| 0 0 | 66B 350B| 0 0 | 319 4010 20 5 75 0 0 0| 0 0 | 66B 350B| 0 0 | 326 4378 19 6 75 0 0 0| 0 0 | 66B 350B| 0 0 | 322 4407 20 5 75 0 0 0| 0 0 | 66B 350B| 0 0 | 332 4520 21 6 73 0 0 0|1140k 0 | 66B 350B| 0 0 | 345 3341 20 7 72 0 0 1| 0 0 | 66B 358B| 0 0 | 348 3821 常用选项如下： 直接跟数字，表示#秒收集一次数据，默认为一秒；dstat 5表示5秒更新一次 12345678910111213-c,--cpu 统计CPU状态，包括system, user, idle, wait, hardware interrupt, software interrupt等；-d, --disk 统计磁盘读写状态-D total,sda 统计指定磁盘或汇总信息-l, --load 统计系统负载情况，包括1分钟、5分钟、15分钟平均值-m, --mem 统计系统物理内存使用情况，包括used, buffers, cache, free-s, --swap 统计swap已使用和剩余量-n, --net 统计网络使用情况，包括接收和发送数据-N eth1,total 统计eth1接口汇总流量-r, --io 统计I/O请求，包括读写请求-p, --proc 统计进程信息，包括runnable、uninterruptible、new-y, --sys 统计系统信息，包括中断、上下文切换-t 显示统计时时间，对分析历史数据非常有用--fs 统计文件打开数和inodes数 除了前面介绍的与监控相关的参数以外，dstat还可以像vmstat和iostat一样使用参数控制报告的时间间隔，或者同事指定时间间隔与报告次数。 例如，下面的命令表示以默认的选项运行dstat，每2秒钟输出一条监控信息，并在输出10条监控信息以后退出dstat。如下所示： 12345678910111213141516[root@192 ~]# dstat 2 10You did not select any stats, using -cdngy by default.----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--usr sys idl wai hiq siq| read writ| recv send| in out | int csw 9 3 88 0 0 0| 218k 60k| 0 0 |2674B 5409B| 164 1550 20 6 74 0 0 0| 0 0 | 66B 594B| 0 0 | 326 3956 21 6 73 0 0 0| 0 147k| 66B 346B| 0 0 | 360 4114 20 5 76 0 0 0| 0 0 | 66B 346B| 0 0 | 320 4494 20 6 74 0 0 0| 0 0 | 96B 372B| 0 0 | 349 4144 20 5 75 0 0 0| 0 0 | 66B 342B| 0 0 | 331 4360 21 6 74 0 0 0| 0 0 | 66B 342B| 0 0 | 344 3607 19 6 75 0 0 0| 0 0 | 66B 342B| 0 0 | 334 4475 21 6 74 0 0 0| 0 0 | 66B 342B| 0 0 | 338 4580 20 7 73 0 0 1| 0 0 | 66B 342B| 0 0 | 340 4341 20 6 74 0 0 0| 0 0 | 66B 342B| 0 0 | 344 3899 [root@192 ~]# （3）dstat高级用法dstat的强大之处不仅仅是因为它聚合了多种工具的监控结果，还因为它能通过附带的插件实现一些更高级功能。如：找出磁盘中占用资源最高的进程和用户。 dstat -cdlmnpsyt 5 可以得到较全面的系统性能数据。 dstat的–top-(io|bio|cpu|cputime|cputime-avg|mem) 通过这几个选项，可以看到具体是哪个用户哪个进程占用了相关系统资源，对系统调优非常有效。如查看当前占用I/O、cpu、内存等最高的进程信息可以使用dstat –top-mem –top-io –top-cpu选项。以下示例演示了如何找出占用资源最多的进程。 12345678910[root@192 ~]# dstat --top-mem --top-io --top-cpu--most-expensive- ----most-expensive---- -most-expensive- memory process | i/o process | cpu process gnome-shell 115M|polkitd 203k 10k|polkitd 3.8gnome-shell 115M|polkitd 459k 23k|polkitd 10gnome-shell 115M|polkitd 625k 31k|polkitd 9.0gnome-shell 115M|polkitd 592k 29k|dbus-daemon 8.0gnome-shell 115M|polkitd 606k 30k|polkitd 9.0gnome-shell 115M|polkitd 525k 26k|polkitd 10gnome-shell 115M|polkitd 525k 26k|dbus-daemon 8.0 dstat的插件保存在/usr/share/dstat目录下，我们可以参考它们的实现，编写自己的插件。 （4）将结果输出到CSV文件dstat还可以将监控信息保存到CSV文件中，以便后续进行处理。通过–output选项指定监控数据输出的文件。如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142[root@192 ~]# dstat -a --output dstat_output.csv----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--usr sys idl wai hiq siq| read writ| recv send| in out | int csw 9 3 88 0 0 0| 217k 60k| 0 0 |2656B 5370B| 165 1568 19 8 50 22 0 0| 11M 50k| 66B 838B| 128k 0 | 428 3116 19 6 75 0 0 0| 0 0 | 66B 374B| 0 0 | 325 4514 25 7 25 43 0 0|5284k 0 | 66B 350B| 476k 0 | 448 4227 18 7 74 0 0 1| 32k 144k| 66B 366B| 52k 0 | 334 4524 31 11 6 52 0 0|1944k 4096B| 66B 374B| 868k 0 | 613 3846 19 6 69 6 0 0| 116k 4096B| 66B 374B| 116k 0 | 337 4367 20 6 74 0 0 0| 0 0 | 66B 374B| 0 0 | 331 4573 20 7 73 0 0 0| 0 0 | 66B 350B| 0 0 | 339 3787 18 6 76 0 0 0| 0 0 | 66B 350B| 0 0 | 317 4472 20 6 74 0 0 0| 0 0 | 66B 350B| 0 0 | 338 3675 20 5 75 0 0 0| 0 0 | 66B 350B| 0 0 | 324 4633 21 5 74 0 0 0| 0 0 | 66B 350B| 0 0 | 318 4597 19 6 75 0 0 0| 0 0 | 66B 350B| 0 0 | 333 4847 18 6 76 0 0 0| 0 0 | 66B 350B| 0 0 | 308 4742 ^C[root@192 ~]# cat dstat_output.csv \"Dstat 0.7.2 CSV output\"\"Author:\",\"Dag Wieers &lt;dag@wieers.com&gt;\",,,,\"URL:\",\"http://dag.wieers.com/home-made/dstat/\"\"Host:\",\"192.168.32.138\",,,,\"User:\",\"root\"\"Cmdline:\",\"dstat -a --output dstat_output.csv\",,,,\"Date:\",\"22 Feb 2020 00:22:08 CST\"\"total cpu usage\",,,,,,\"dsk/total\",,\"net/total\",,\"paging\",,\"system\",\"usr\",\"sys\",\"idl\",\"wai\",\"hiq\",\"siq\",\"read\",\"writ\",\"recv\",\"send\",\"in\",\"out\",\"int\",\"csw\"8.683,2.816,88.156,0.313,0.0,0.032,222019.623,61103.678,0.0,0.0,2655.691,5370.227,164.807,1568.44719.388,8.163,50.0,22.449,0.0,0.0,11702272.0,51200.0,66.0,838.0,131072.0,0.0,428.0,3116.019.192,6.061,74.747,0.0,0.0,0.0,0.0,0.0,66.0,374.0,0.0,0.0,325.0,4514.024.510,6.863,25.490,43.137,0.0,0.0,5410816.0,0.0,66.0,350.0,487424.0,0.0,448.0,4227.018.182,7.071,73.737,0.0,0.0,1.010,32768.0,147456.0,66.0,366.0,53248.0,0.0,334.0,4524.030.928,11.340,6.186,51.546,0.0,0.0,1990656.0,4096.0,66.0,374.0,888832.0,0.0,613.0,3846.019.192,6.061,68.687,6.061,0.0,0.0,118784.0,4096.0,66.0,374.0,118784.0,0.0,337.0,4367.019.802,5.941,74.257,0.0,0.0,0.0,0.0,0.0,66.0,374.0,0.0,0.0,331.0,4573.020.0,7.0,73.0,0.0,0.0,0.0,0.0,0.0,66.0,350.0,0.0,0.0,339.0,3787.018.367,6.122,75.510,0.0,0.0,0.0,0.0,0.0,66.0,350.0,0.0,0.0,317.0,4472.020.0,6.0,74.0,0.0,0.0,0.0,0.0,0.0,66.0,350.0,0.0,0.0,338.0,3675.020.202,5.051,74.747,0.0,0.0,0.0,0.0,0.0,66.0,350.0,0.0,0.0,324.0,4633.021.0,5.0,74.0,0.0,0.0,0.0,0.0,0.0,66.0,350.0,0.0,0.0,318.0,4597.019.192,6.061,74.747,0.0,0.0,0.0,0.0,0.0,66.0,350.0,0.0,0.0,333.0,4847.018.182,6.061,75.758,0.0,0.0,0.0,0.0,0.0,66.0,350.0,0.0,0.0,308.0,4742.0[root@192 ~]# 2、交互式监控工具glances在紧急情况下，工程师需要在尽可能短的时间内查看尽可能多的信息。此时，glances是一个不错的选择。glances的设计初衷就是在当前窗口中尽可能多地显示系统消息。 glances是一款使用Python语言开发、基于psutil的跨平台系统监控工具，在所有Linux命令行工具中，它与top命令最相似，都是命令行交互监控工具。但是，glances实现了比top命令更齐全的接口，提供了更加丰富的功能。 （1）glances提供的系统信息glances提供的系统信息如下所示： 12345678910CPU 使用率内存使用情况内核统计信息和运行队列信息磁盘 I/O 速度、传输和读/写比率文件系统中的可用空间磁盘适配器网络 I/O 速度、传输和读/写比率页面空间和页面速度消耗资源最多的进程计算机信息和系统资源 glances 工具可以在用户的终端上实时显示重要的系统信息，并动态地对其进行更新。这个高效的工具可以工作于任何终端屏幕。另外它并不会消耗大量的 CPU 资源，通常低于百分之二。glances 在屏幕上对数据进行显示，并且每隔2秒钟对其进行更新。您也可以自己将这个时间间隔更改为更长或更短的数值。 glances 工具还可以将相同的数据捕获到一个文件，便于以后对报告进行分析和绘制图形。输出文件可以是电子表格的格式 (.csv) 或者 html 格式。 （2）Linux下glances的安装在Linux系统中，可以使用yum命令或者pip命令安装glances。如下所示： 12yum install epel-releaseyum install glances glances的使用非常简单，直接输入glances命令便进入了一个类似top命令的交互式界面。在这个界面中，显示了比top更加全面，更加具有可读性的信息。如下所示： glances 工作界面的说明 :在图 1 的上部是 CPU 、Load（负载）、Mem（内存使用）、 Swap（交换分区）的使用情况。在图 1 的中上部是网络接口、Processes（进程）的使用情况。通常包括如下字段： 1234567891011VIRT: 虚拟内存大小RES: 进程占用的物理内存值%CPU：该进程占用的 CPU 使用率%MEM：该进程占用的物理内存和总内存的百分比PID: 进程 ID 号USER: 进程所有者的用户名TIME+: 该进程启动后占用的总的 CPU 时间IO_R 和 IO_W: 进程的读写 I/O 速率NAME: 进程名称NI: 进程优先级S: 进程状态，其中 S 表示休眠，R 表示正在运行，Z 表示僵死状态。 （3）glances的可读性对比可以发现，glances对屏幕的利用率比top明显高很多，信息量很大，有许多top所没有显示的数据。而且，glances的实时变动比top颜值高太多了。 Glances 会用一下几种颜色来代表状态，如下所示： 1234绿色：OK（一切正常）蓝色：CAREFUL（需要注意）紫色：WARNING（警告）红色：CRITICAL（严重） （4）glances常见命令glances是一个交互式的工具，因此我们也可以输入命令来控制glances的行为。glances常见的命令有： 123456789101112h：显示帮助信息q：离开程序退出c：按照 CPU 实时负载对系统进程进行排序m：按照内存使用状况对系统进程排序i：按照 I/O 使用状况对系统进程排序p：按照进程名称排序d：显示磁盘读写状况w：删除日志文件l：显示日志s：显示传感器信息f：显示系统信息1：轮流显示每个 CPU 内核的使用情况（次选项仅仅使用在多核 CPU 系统） glances还支持将采集的数据导入到其他服务中心，包括InfluxDB、Cassandra、CouchDB、OpenTSDB、Prometheus、StatsD、ElasticSearch、RabbitMQ/ActiveMQ、ZeroMQ、Kafaka和Riemann。 二、Python监控Linuxshell查看磁盘的监控信息，如下所示： 1234567[root@bogon proc]# cat /proc/diskstats 8 0 sda 85935 21845 10913707 101067 3119 81257 743486 15647 0 31410 109079 8 1 sda1 1822 0 12456 397 4 0 4096 74 0 457 462 8 2 sda2 84082 21845 10897907 100659 3115 81257 739390 15573 0 30950 108604 11 0 sr0 0 0 0 0 0 0 0 0 0 0 0 253 0 dm-0 80726 0 10688467 99971 2275 0 82606 10224 0 27927 110196 253 1 dm-1 25123 0 205184 7367 82098 0 656784 616558 0 5167 623924 编写一个Python脚本，监控磁盘信息，如下所示： 1234567891011121314151617181920212223242526#/usr/bin/python#_*_ coding:utf-8 _*_from __future__ import print_functionfrom collections import namedtupledisk = namedtuple('Disk','major_number minor_number device_name' ' read_count read_merged_count read_sections' ' time_spent_reading write_count write_merged_count' ' write_sections time_spent_write io_requests' ' time_spent_doing_io weighted_time_spent_dong_io')def get_disk_info(device): with open('/proc/diskstats') as f: for line in f: if line.split()[2] == device: return disk(*(line.split())) raise RuntimeError('设备(&#123;0&#125;)没找到。。。'.format(device))def main(): disk_info = get_disk_info('sda1') print(disk_info)if __name__ == '__main__': main() 三、使用开源库监控Linux在这一小节，我们将介绍一个在Python生态中广泛使用的开源项目，即psutil。随后，我们将使用psutil重构前一小节编写的监控程序。另外，还会简单介绍psutil提供的进程管理功能。 1、psutil介绍 psutil = process and system utilities psutil是一个开源且跨平台的库，其提供了便利的函数用来获取操作系统的信息，比如CPU，内存，磁盘，网络等。此外，psutil还可以用来进行进程管理，包括判断进程是否存在、获取进程列表、获取进程详细信息等。而且psutil还提供了许多命令行工具提供的功能，包括：ps，top，lsof，netstat，ifconfig， who，df，kill，free，nice，ionice，iostat，iotop，uptime，pidof，tty，taskset，pmap。 psutil是一个跨平台的库，支持Linux、Windows、OSX、FreeBSD、OpenBSD、NetBSD、Sun Solaris、AIX等操作系统。同时，psutil也支持32位与64位的系统架构，支持Python2.6到Python3.x之间的所有Python版本。 psutil具有简单易用、功能强大、跨平台等诸多优点，广泛应用于开源项目中，比较有名的有glances、Facebook的osquery、Google的grr等。psutil不但广泛应用于Python语言开发的开源项目中，还被移植到了其他编程语言中，如Go语言的gopsutil、C语言的cpslib、Rust语言的rust-psutil、Ruby语言的posixpsutil等。 psutil是一个第三方的开源项目，因此，需要先安装才能够使用。如果安装了Anaconda，psutil就已经可用了。否则，需要在命令行下通过pip安装： 1234567[root@localhost ~]# pip install psutilCollecting psutil Downloading psutil-5.7.0.tar.gz (449 kB) |████████████████████████████████| 449 kB 4.6 kB/s Installing collected packages: psutil Running setup.py install for psutil ... doneSuccessfully installed psutil-5.7.0 psutil包含了异常、类、功能函数和常量，其中功能函数用来获取系统的信息，如CPU、磁盘、内存、网络等。类用来实现进程的管理功能。 2、psutil提供的功能函数根据函数的功能，主要分为CPU、磁盘、内存、网络几类，下面将会总几个方面来介绍psutil提供的功能函数。在这一小节，我们也将学习如何使用psutil来简化使用shell脚本获取监控信息的程序，并获取CPU、内存、磁盘和网络等不同维度。 （1）CPU与CPU相关的功能函数如下： 函数 描述 psutil.cpu_count() cpu_count(,[logical]):默认返回逻辑CPU的个数,当设置logical的参数为False时，返回物理CPU的个数。 psutil.cpu_percent() cpu_percent(,[percpu],[interval])：返回CPU的利用率,percpu为True时显示所有物理核心的利用率,interval不为0时,则阻塞时显示interval执行的时间内的平均利用率 psutil.cpu_times() cpu_times(,[percpu])：以命名元组(namedtuple)的形式返回cpu的时间花费,percpu=True表示获取每个CPU的时间花费 psutil.cpu_times_percent() cpu_times_percent(,[percpu])：功能和cpu_times大致相同，看字面意思就能知道，该函数返回的是耗时比例。 psutil.cpu_stats() cpu_stats()以命名元组的形式返回CPU的统计信息，包括上下文切换，中断，软中断和系统调用次数。 psutil.cpu_freq() cpu_freq([percpu])：返回cpu频率 1）cpu_count 默认返回逻辑CPU的个数,当设置logical的参数为False时，返回物理CPU的个数。 1234567In [1]: import psutil In [2]: psutil.cpu_count() Out[2]: 2In [3]: psutil.cpu_count(logical&#x3D;False) Out[3]: 1 2）cpu_percent 返回CPU的利用率，percpu为True时显示所有物理核心的利用率，interval不为0时，则阻塞时显示interval执行的时间内的平均利用率。 12345678In [4]: psutil.cpu_percent() Out[4]: 1.5In [5]: psutil.cpu_percent(percpu=True) Out[5]: [1.3, 1.5]In [6]: psutil.cpu_percent(percpu=True,interval=2) Out[6]: [1.0, 0.0] 3）cpu_times以命名元组(namedtuple)的形式返回cpu的时间花费，percpu=True表示获取每个CPU的时间花费。 12345In [7]: psutil.cpu_times() Out[7]: scputimes(user=41.51, nice=2.05, system=35.36, idle=2096.05, iowait=5.45, irq=0.0, softirq=1.31, steal=0.0, guest=0.0, guest_nice=0.0)In [8]: psutil.cpu_times_percent() Out[8]: scputimes(user=0.3, nice=0.0, system=0.1, idle=99.5, iowait=0.0, irq=0.0, softirq=0.0, steal=0.0, guest=0.0, guest_nice=0.0) 4）cpu_stats以命名元组的形式返回CPU的统计信息，包括上下文切换，中断，软中断和系统调用次数。 12In [10]: psutil.cpu_stats() Out[10]: scpustats(ctx_switches=538642, interrupts=238329, soft_interrupts=273448, syscalls=0) 5）cpu_freq返回cpu频率。 12In [11]: psutil.cpu_freq() Out[11]: scpufreq(current=2394.464, min=0.0, max=0.0) （2）内存与内存相关的功能函数如下： 1）virtual_memory以命名元组的形式返回内存使用情况，包括总内存、可用内存、内存利用率、buffer和cache等。除了内存利用率，其它字段都以字节为单位返回。 1234In [1]: import psutil In [2]: psutil.virtual_memory() Out[2]: svmem(total=1019797504, available=95744000, percent=90.6, used=758079488, free=67502080, active=295485440, inactive=417394688, buffers=0, cached=194215936, shared=19103744, slab=92905472) 单位转换 1234567891011121314151617#/usr/bin/python#-*- conding:utf-8 _*_import psutildef bytes2human(n): symbols = ('K','M','G','T','P','E','Z','Y') prefix = &#123;&#125; for i,s in enumerate(symbols): prefix[s] = 1 &lt;&lt; (i + 1) * 10 for s in reversed(symbols): if n &gt;= prefix[s]: value = float(n) / prefix[s] return '%.1f%s' % (value,s) return '%sB' % nprint(bytes2human(psutil.virtual_memory().total)) 运行结果如下所示： 12[root@localhost ~]# python mem.py 972.6M 2）swap_memory以命名元组的形式返回swap/memory使用情况，包含swap中页的换入和换出。 12In [3]: psutil.swap_memory() Out[3]: sswap(total=2147479552, used=141819904, free=2005659648, percent=6.6, sin=24666112, sout=147292160) （3）磁盘与磁盘相关的功能如下： 函数 描述 psutil.disk_io_counters() disk_io_counters([perdisk])：以命名元组的形式返回磁盘io统计信息(汇总的)，包括读、写的次数，读、写的字节数等。 当perdisk的值为True，则分别列出单个磁盘的统计信息(字典：key为磁盘名称，value为统计的namedtuple)。 psutil.disk_partitions() disk_partitions([all=False])：以命名元组的形式返回所有已挂载的磁盘，包含磁盘名称，挂载点，文件系统类型等信息。 当all等于True时，返回包含/proc等特殊文件系统的挂载信息 psutil.disk_usage() disk_usage(path)：以命名元组的形式返回path所在磁盘的使用情况，包括磁盘的容量、已经使用的磁盘容量、磁盘的空间利用率等。 1）psutil.disk_io_counters以命名元组的形式返回磁盘io统计信息(汇总的)，包括读、写的次数，读、写的字节数等。 当perdisk的值为True，则分别列出单个磁盘的统计信息(字典：key为磁盘名称，value为统计的namedtuple)。有了disk_io_counters函数，省去了解析/proc/diskstats文件的烦恼。 12345678910111213In [1]: import psutil In [2]: psutil.disk_io_counters() Out[2]: sdiskio(read_count=86913, write_count=46560, read_bytes=5038501376, write_bytes=408987648, read_time=77974, write_time=79557, read_merged_count=5933, write_merged_count=35916, busy_time=42153)In [3]: psutil.disk_io_counters(perdisk=True) Out[3]: &#123;'sda': sdiskio(read_count=41472, write_count=5340, read_bytes=2524417024, write_bytes=205662720, read_time=38302, write_time=4484, read_merged_count=5933, write_merged_count=35916, busy_time=21074), 'sda1': sdiskio(read_count=1854, write_count=4, read_bytes=6441472, write_bytes=2097152, read_time=370, write_time=35, read_merged_count=0, write_merged_count=0, busy_time=396), 'sda2': sdiskio(read_count=39587, write_count=5337, read_bytes=2516263424, write_bytes=203570688, read_time=37925, write_time=4449, read_merged_count=5933, write_merged_count=35916, busy_time=20675), 'sr0': sdiskio(read_count=0, write_count=0, read_bytes=0, write_bytes=0, read_time=0, write_time=0, read_merged_count=0, write_merged_count=0, busy_time=0), 'dm-0': sdiskio(read_count=38566, write_count=5197, read_bytes=2483773952, write_bytes=55885312, read_time=37685, write_time=3546, read_merged_count=0, write_merged_count=0, busy_time=19410), 'dm-1': sdiskio(read_count=6875, write_count=36059, read_bytes=30310400, write_bytes=147697664, read_time=1987, write_time=71537, read_merged_count=0, write_merged_count=0, busy_time=1673)&#125; 2）psutil.disk_partitions以命名元组的形式返回所有已挂载的磁盘，包含磁盘名称，挂载点，文件系统类型等信息。当all等于True时，返回包含/proc等特殊文件系统的挂载信息。 123456789101112131415161718In [4]: psutil.disk_partitions() Out[4]: [sdiskpart(device='/dev/mapper/centos-root', mountpoint='/', fstype='xfs', opts='rw,seclabel,relatime,attr2,inode64,noquota'), sdiskpart(device='/dev/sda1', mountpoint='/boot', fstype='xfs', opts='rw,seclabel,relatime,attr2,inode64,noquota')]In [5]: [device for device in psutil.disk_partitions() if device.mountpoint == '/']Out[5]: [sdiskpart(device='/dev/mapper/centos-root', mountpoint='/', fstype='xfs', opts='rw,seclabel,relatime,attr2,inode64,noquota')]In [6]: def get_disk_via_mountpoint(point): ...: disk = [item for item in psutil.disk_partitions() if item.mountpoint == point] ...: return disk[0].device ...: In [7]: get_disk_via_mountpoint('/')Out[7]: '/dev/mapper/centos-root'In [8]: get_disk_via_mountpoint('/boot') Out[8]: '/dev/sda1' 3）psutil.disk_usage以命名元组的形式返回path所在磁盘的使用情况，包括磁盘的容量、已经使用的磁盘容量、磁盘的空间利用率等。 12345678In [9]: psutil.disk_usage('/') Out[9]: sdiskusage(total=18238930944, used=6775488512, free=11463442432, percent=37.1)In [10]: psutil.disk_usage('/').percentOut[10]: 37.2In [11]: type(psutil.disk_usage('/').percent)Out[11]: float （4）网络与网络相关的函数如下： 函数 详情 psutil.net_io_counter([pernic]) 以命名元组的形式返回当前系统中每块网卡的网络io统计信息，包括收发字节数，收发包的数量、出错的情况和删包情况。当pernic为True时，则列出所有网卡的统计信息。 psutil.net_connections([kind]) 以列表的形式返回每个网络连接的详细信息(namedtuple)。命名元组包含fd, family, type, laddr, raddr, status, pid等信息。kind表示过滤的连接类型，支持的值如下：(默认为inet) psutil.net_if_addrs() 以字典的形式返回网卡的配置信息，包括IP地址和mac地址、子网掩码和广播地址。 psutil.net_if_stats() 返回网卡的详细信息，包括是否启动、通信类型、传输速度与mtu。 psutil.users() 以命名元组的方式返回当前登陆用户的信息，包括用户名，登陆时间，终端，与主机信息 psutil.boot_time() 以时间戳的形式返回系统的启动时间 1）psutil.net_io_counter以命名元组的形式返回当前系统中每块网卡的网络io统计信息，包括收发字节数，收发包的数量、出错的情况和删包情况。当pernic为True时，则列出所有网卡的统计信息。使用net_io_counter函数与自己解析/proc/net/dev文件内容实现的功能相同。 123456789101112In [1]: import psutil In [2]: psutil.net_io_counters() Out[2]: snetio(bytes_sent=720405, bytes_recv=3661606, packets_sent=5520, packets_recv=14886, errin=0, errout=0, dropin=0, dropout=0)In [3]: psutil.net_io_counters(pernic=True) Out[3]: &#123;'ens37': snetio(bytes_sent=724145, bytes_recv=3365944, packets_sent=5538, packets_recv=10017, errin=0, errout=0, dropin=0, dropout=0), 'lo': snetio(bytes_sent=0, bytes_recv=0, packets_sent=0, packets_recv=0, errin=0, errout=0, dropin=0, dropout=0), 'virbr0-nic': snetio(bytes_sent=0, bytes_recv=0, packets_sent=0, packets_recv=0, errin=0, errout=0, dropin=0, dropout=0), 'virbr0': snetio(bytes_sent=0, bytes_recv=0, packets_sent=0, packets_recv=0, errin=0, errout=0, dropin=0, dropout=0), 'ens33': snetio(bytes_sent=0, bytes_recv=298202, packets_sent=0, packets_recv=4899, errin=0, errout=0, dropin=0, dropout=0)&#125; 2）net_connections以列表的形式返回每个网络连接的详细信息(namedtuple)，可以使用该函数查看网络连接状态，统计连接个数以及处于特定状态的网络连接个数。 123456789101112In [4]: psutil.net_connections() Out[4]: [sconn(fd=6, family=&lt;AddressFamily.AF_INET6: 10&gt;, type=&lt;SocketKind.SOCK_STREAM: 1&gt;, laddr=addr(ip='::', port=111), raddr=(), status='LISTEN', pid=6558), sconn(fd=7, family=&lt;AddressFamily.AF_INET6: 10&gt;, type=&lt;SocketKind.SOCK_DGRAM: 2&gt;, laddr=addr(ip='::', port=111), raddr=(), status='NONE', pid=6558), sconn(fd=8, family=&lt;AddressFamily.AF_INET6: 10&gt;, type=&lt;SocketKind.SOCK_STREAM: 1&gt;, laddr=addr(ip='::1', port=6010), raddr=(), status='LISTEN', pid=9047), sconn(fd=6, family=&lt;AddressFamily.AF_INET: 2&gt;, type=&lt;SocketKind.SOCK_STREAM: 1&gt;,......In [5]: conns = psutil.net_connections()In [6]: len([conn for conn in conns if conn.status == 'TIME_WAIT']) Out[6]: 0 3）net_if_addrs以字典的形式返回网卡的配置信息，包括IP地址和mac地址、子网掩码和广播地址。 12345678In [7]: psutil.net_if_addrs() Out[7]: &#123;'lo': [snicaddr(family=&lt;AddressFamily.AF_INET: 2&gt;, address='127.0.0.1', netmask='255.0.0.0', broadcast=None, ptp=None), snicaddr(family=&lt;AddressFamily.AF_INET6: 10&gt;, address='::1', netmask='ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff', broadcast=None, ptp=None), snicaddr(family=&lt;AddressFamily.AF_PACKET: 17&gt;, address='00:00:00:00:00:00', netmask=None, broadcast=None, ptp=None)], 'ens37': [snicaddr(family=&lt;AddressFamily.AF_INET: 2&gt;, address='192.168.1.131', netmask='255.255.255.255', broadcast='192.168.1.131', ptp=None), snicaddr(family=&lt;AddressFamily.AF_INET6: 10&gt;, address='240e:82:e03:7342:4378:7be3:558c:fc88', netmask='ffff:ffff:ffff:ffff::', broadcast=None, ptp=None)...... 4）psutil.net_if_stats返回网卡的详细信息，包括是否启动、通信类型、传输速度与mtu。 1234567In [8]: psutil.net_if_stats() Out[8]: &#123;'ens37': snicstats(isup=True, duplex=&lt;NicDuplex.NIC_DUPLEX_FULL: 2&gt;, speed=1000, mtu=1500), 'lo': snicstats(isup=True, duplex=&lt;NicDuplex.NIC_DUPLEX_UNKNOWN: 0&gt;, speed=0, mtu=65536), 'virbr0-nic': snicstats(isup=False, duplex=&lt;NicDuplex.NIC_DUPLEX_FULL: 2&gt;, speed=10, mtu=1500), 'virbr0': snicstats(isup=True, duplex=&lt;NicDuplex.NIC_DUPLEX_UNKNOWN: 0&gt;, speed=0, mtu=1500), 'ens33': snicstats(isup=True, duplex=&lt;NicDuplex.NIC_DUPLEX_FULL: 2&gt;, speed=1000, mtu=1500)&#125; （5）其他1）users以命名元组的方式返回当前登陆用户的信息，包括用户名，登陆时间，终端，与主机信息。 123456In [9]: psutil.users() Out[9]: [suser(name='root', terminal=':0', host='localhost', started=1582366080.0, pid=7991), suser(name='root', terminal='pts/0', host='localhost', started=1582366208.0, pid=8927), suser(name='root', terminal='pts/1', host='192.168.1.4', started=1582370816.0, pid=10099), suser(name='root', terminal='pts/3', host='192.168.1.4', started=1582369408.0, pid=9787)] 2）boot_time以时间戳的形式返回系统的启动时间。 12345In [10]: psutil.boot_time() Out[10]: 1582527367.0 In [11]: datetime.datetime.fromtimestamp(psutil.boot_time()).strftime('%Y-%m-%d %H:%M:%S') Out[11]: '2020-02-24 14:56:07' 3、综合案例：使用psutil实现监控程序4、psutil进程管理psutil还提供了作为进程管理的功能函数，包括获取进程列表，判断是否存在，以及进程管理的类封装。 函数 详情 psutil.Process() 对进程进行封装，可以使用该类的方法获取进行的详细信息，或者给进程发送信号。 psutil.pids() 以列表的形式返回当前正在运行的进程 psutil.pid_exists(1) 判断给点定的pid是否存在 psutil.process_iter() 迭代当前正在运行的进程，返回的是每个进程的Process对象 1）Process类对进程进行封装，可以使用该类的方法获取进行的详细信息，或者给进程发送信号。 123456In [1]: import psutil In [2]: init_process = psutil.Process()In [3]: init_process.cmdline() Out[3]: ['/usr/local/python38/bin/python3.8', '/usr/local/python38/bin/ipython'] Process类包含很多方法来获取进程的详细信息。下面是几个较常用的方法： 123456789name：获取进程的名称cmdline：获取启动进程的命令行参数create_time：获取进程的创建时间(时间戳格式)num_fds：进程打开的文件个数num_threads：进程的子进程个数is_running：判断进程是否正在运行send_signal：给进程发送信号，类似与os.kill等kill：发送SIGKILL信号结束进程terminate：发送SIGTEAM信号结束进程 2）pids以列表的形式返回当前正在运行的进程。 123456789In [1]: import psutil In [2]: init_process = psutil.Process()In [3]: init_process.cmdline() Out[3]: ['/usr/local/python38/bin/python3.8', '/usr/local/python38/bin/ipython']In [4]: psutil.pids()[:5] Out[4]: [1, 2, 3, 5, 7] 3）pid_exists判断给点定的pid是否存在。 12345In [5]: psutil.pid_exists(1) Out[5]: TrueIn [6]: psutil.pid_exists(10245) Out[6]: False 4）process_iter迭代当前正在运行的进程，返回的是每个进程的Process对象，而pids返回的是进程的列表。 四、使用Python监控MongoDB对于MongoDB数据库来说，获取监控的方法比较简单，因为MongoDB本身流返回给我们一个字典形式的数据。如下所示： 1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pymongo 1234567891011121314#/usr/bin/python#_*_ coding:utf-8 _*_from __future__ import print_functionimport pymongoclient = pymongo.MongoClient(host='127.0.0.1:27017')client.admin.authenticate('laoyu','laoyu')rs = client.admin.command('replSetGetStatus')print(\"set:\",rs['set'])print(\"myState:\",rs['myState'])print('num of members:',len(rs['members'])) 3）pid_exists判断给点定的pid是否存在。 12345In [5]: psutil.pid_exists(1) Out[5]: TrueIn [6]: psutil.pid_exists(10245) Out[6]: False 4）process_iter迭代当前正在运行的进程，返回的是每个进程的Process对象，而pids返回的是进程的列表。 四、使用Python监控MongoDB对于MongoDB数据库来说，获取监控的方法比较简单，因为MongoDB本身流返回给我们一个字典形式的数据。如下所示： 1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pymongo 1234567891011121314#/usr/bin/python#_*_ coding:utf-8 _*_from __future__ import print_functionimport pymongoclient = pymongo.MongoClient(host='127.0.0.1:27017')client.admin.authenticate('laoyu','laoyu')rs = client.admin.command('replSetGetStatus')print(\"set:\",rs['set'])print(\"myState:\",rs['myState'])print('num of members:',len(rs['members']))","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Python3+Django3开发简单的人员管理系统","slug":"Python3+Django3开发简单的人员管理系统","date":"2020-04-26T16:00:00.000Z","updated":"2020-04-27T14:43:16.438Z","comments":true,"path":"Python3+Django3开发简单的人员管理系统.html","link":"","permalink":"https://pdxblog.top/Python3+Django3%E5%BC%80%E5%8F%91%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E5%91%98%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F.html","excerpt":"","text":"Python3+Django3开发简单的人员管理系统1、创建工程和应用1.1 使用pycharm创建项目 1.2安装mysqlclient在设置里面找创建的项目点击右边的“+”号直接安装 2、应用配置2.1、修改项目配置文件（UserSystem/settings.py）1）注释csrf校验123456789MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', # 'django.middleware.csrf.CsrfViewMiddleware', # 注释此项 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',] 2）修改数据库的默认配置：sqlite3改为mysql1234567891011121314DATABASES = &#123; # 'default': &#123; # 'ENGINE': 'django.db.backends.sqlite3', # 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), # &#125;'default': &#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': 'userinfo', 'USER': 'root', 'PASSWORD': '123.com', 'HOST':'localhost', 'PORT':'3306', &#125;&#125; 3）修改语言和时区12345# LANGUAGE_CODE = 'en-us'LANGUAGE_CODE = 'zh-Hans'# TIME_ZONE = 'UTC'TIME_ZONE = 'Asia/Shanghai' 4）允许所有IP访问1ALLOWED_HOSTS = ['*'] 2.2、定义用户信息的数据模型：UserInfo/models.py12345678910111213from django.db import modelsclass User(models.Model): GENDER_CHOICES = ( ('男', '男'), ('女', '女'), ) name = models.CharField(max_length=30, unique=True, verbose_name='姓 名') birthday = models.DateField(blank=True, null=True, verbose_name='生 日') gender = models.CharField(max_length=30, choices=GENDER_CHOICES, verbose_name='性 别') account = models.IntegerField(default=0, verbose_name='工 号') age = models.IntegerField(default=18, verbose_name='年 龄') 2.3、初始化模型数据库兵生成数据库文件简言之：在Django 1.9及未来的版本种使用migrate代替原先的syscdb. 先在Mysql中创建数据库”userinfo” 执行下面代码：（直接再Pycharm里的”Terminal”终端执行 ） 12(venv) F:\\python1\\UserSystem&gt;python manage.py makemigrations(venv) F:\\python1\\UserSystem&gt;python ./manage.py migrate 2.4、显示注册信息修改默认标题（UserInfo/admin.py）123456789101112131415161718from django.contrib import adminfrom . models import Userclass HostAdmin(admin.ModelAdmin): list_display = [ 'name', 'age', 'birthday', 'gender', 'account', ] search_fields = ('name',)admin.site.register(User, HostAdmin)admin.AdminSite.site_header = '运维系统管理后台'admin.AdminSite.site_title = '运维系统' 2.5、添加应用的url访问（UserSystem/urls.py）123456from django.contrib import adminfrom django.urls import pathurlpatterns = [ path('admin/', admin.site.urls),] 3、启动Django服务3.1、命令启动1(venv) F:\\python1\\UserSystem&gt;python ./manage.py runserver 3.2、创建超级用户12345678910(venv) F:\\python1\\UserSystem&gt;python ./manage.py createsuperuser用户名: accp电子邮件地址:Password:Password (again):密码长度太短。密码必须包含至少 8 个字符。这个密码太常见了。密码只包含数字。Bypass password validation and create user anyway? [y/N]: ySuperuser created successfully. 3.3、浏览器访问登录：http://127.0.0.1:8000/admin 3.4、登录成功后即可添加对应的信息到系统中 3.5、前往数据库查看，用户信息是否保存","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Python对Linux系统的管理","slug":"Python对Linux系统的管理","date":"2020-04-26T16:00:00.000Z","updated":"2020-04-27T14:43:53.427Z","comments":true,"path":"Python对Linux系统的管理.html","link":"","permalink":"https://pdxblog.top/Python%E5%AF%B9Linux%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%AE%A1%E7%90%86.html","excerpt":"","text":"Python对Linux系统的管理一、OS模块常用功能1、os模块打开文件方法如下： os.open(filename, flag, [,mode]) flag参数说明： 1234os.O_CREAT # 创建文件os.O_RDONLY # 只读方式打开os.O_WRONLY # 只写方式打开os.O_RDWR # 读写方式打开 2、os模块对文件进行操作常用方法如下： 12345678# 读取文件os.read(fd, buffersize)# 写入文件os.write(fd, string)# 文件指针操作os.lseek(fd, pos, how)# 关闭文件os.close(fd) 代码演示： 文件创建和写入 123456789101112131415161718import os# 打开文件fd = os.open(\"abc.txt\", os.O_RDWR | os.O_CREAT)# 写入字符串str = \"Hello Python!\"ret = os.write(fd, bytes(str, 'UTF-8'))# 输入返回值print(\"写入的位数为: \")print(ret)print(\"写入成功\")# 关闭文件os.close(fd)print(\"关闭文件成功!!\") 文件读取 123456789101112import os# 打开文件fd = os.open(\"abc.txt\", os.O_RDWR)# 读取文本ret = os.read(fd, 6)print(ret)# 关闭文件os.close(fd)print(\"关闭文件成功!!\") 3、os模块管理文件和目录常用方法如下： os方法 说明 getcwd() 获取当前目录 listdir(path) 返回当前目录下所有文件组成的列表 chdir(path) 切换目录 rename(old, new) 修改文件或者目录名 mkdir(path [,mode]) 创建目录 makedirs(path [,mode]) 创建多级目录 rmdir(path) 删除目录（目录必须为空目录） removedirs(path) 删除多级目录（目录必须为空目录） remove(path) 删除文件 代码演示： 123456789101112131415# coding=utf-8import osprint(os.getcwd()) # pwdprint(os.listdir()) # lsos.chdir('/opt') # cd /optos.rename('abc.txt','test.txt') # mv abc.txt test.txtos.remove('read.py') # rm -f abc.txtos.mkdir('test') # mkdir dir1os.makedirs('demo/abc') # mkdir -p dir2/dir22os.rmdir('test') # 目录必须为空os.removedirs('demo') # 目录必须为空 4、os模块管理文件权限 os方法 说明 access(path, mode) 判断该文件权限：F_OK表示该路径存在；权限：R_OK，W_OK，X_OK chmod(path, mode) 修改文件权限：0o755 chown(path, uid, gid) 更改文件所有者，如果不修改可以设置为 -1 代码演示： 123456789101112131415161718192021222324import os# 测试路径是否存在：os.F_OKres = os.access('test.txt',os.F_OK)print(res)# 测试当前用户对该文件是否有读的权限res = os.access('test.txt',os.R_OK)print(res)# 测试当前用户对该文件是否有写的权限res = os.access('test.txt',os.W_OK)print(res)# 测试当前用户对该文件是否有执行的权限res = os.access('test.txt',os.X_OK)print(res)# 更改当前用户的权限os.chmod('test.txt',0o755)# 更改文件的所有者os.chown('test.txt', 1001, 1002) 5、os.path模块管理文件与路径（1）拆分路径 os.path方法 说明 os.path.split(path) 返回一个二元组，包含文件的路径和文件名 os.path.dirname(path) 返回文件的路径 os.path.basename(path) 返回文件名 os.path.splitext(path) 返回一个去掉文件扩展名的部分和扩展名的二元组 代码演示： 12345678910111213141516171819In [10]: os.getcwd()Out[10]: '/opt/os_demo'In [11]: os.listdir()Out[11]: ['os_access.py', 'test.txt']In [12]: path = '/opt/os_demo/test.txt' In [13]: os.path.split(path)Out[13]: ('/opt/os_demo', 'test.txt')In [14]: os.path.dirname(path)Out[14]: '/opt/os_demo'In [15]: os.path.basename(path)Out[15]: 'test.txt'In [16]: os.path.splitext(path) Out[16]: ('/opt/os_demo/test', '.txt') （2）构建路径 os.path方法 说明 os.path.expanduser(path) 展开用户的HOME目录，如，oracle os.path.abspath(path) 得到文件或路径的绝对路径 os.path.join(path) 根据不同的操作系统平台，使用不同的路径分隔符拼接路径 os.path.isabs(path) 检查一个路径是不是一个绝对路径 代码演示： 1234567891011121314151617181920212223242526In [19]: os.path.expanduser('~') Out[19]: '/root'In [20]: os.path.expanduser('~oracle') Out[20]: '/home/oracle'In [21]: os.path.expanduser('~accp')Out[21]: '/home/accp'In [22]: os.path.expanduser('~acp') Out[22]: '~acp' # 错误演示In [23]: os.path.abspath('.')Out[23]: '/opt/os_demo'In [24]: os.path.abspath('..')Out[24]: '/opt'In [25]: os.path.join('/opt/os_demo','test.txt')Out[25]: '/opt/os_demo/test.txt'In [26]: os.path.isabs('/opt/os_demo/') Out[26]: TrueIn [27]: os.path.isabs('.') Out[27]: False （3）获取文件属性 os.path方法 说明 os.path.getatime(path) 返回最近访问时间（浮点型秒数） os.path.getmtime(path) 返回最近文件修改时间 os.path.getctime(path) 返回文件 path 创建时间 os.path.getsize(path) 返回文件大小，如果文件不存在就返回错误 代码演示： 1234567891011In [33]: os.path.getatime(path)Out[33]: 1587547270.7306058 # 时间戳In [34]: os.path.getmtime(path)Out[34]: 1587547270.7306058In [35]: os.path.getctime(path)Out[35]: 1587548055.4721448In [36]: os.path.getsize(path)Out[36]: 0 （4）判断文件类型 os.path方法 说明 os.path.isfile(path) 判断路径是否为文件 os.path.isdir(path) 判断路径是否为目录 os.path.islink(path) 判断路径是否为链接 os.path.ismount(path) 判断路径是否为挂载点 代码演示： 1234567891011In [37]: os.path.isfile(path)Out[37]: TrueIn [38]: os.path.isdir(path)Out[38]: FalseIn [39]: os.path.islink(path)Out[39]: FalseIn [40]: os.path.ismount(path)Out[40]: False 6、os模块执行shell命令os.system()的作用： 123执行shell命令返回shell命令的返回值命令的输出会输出到标准输出 代码演示： os.system(‘cls’) 案例1：编写自动安装Python的脚本实现步骤： 123下载Python版本源码安装Python需要的依赖库编译安装Python 伪代码： 123451. 判断用户是不是root2. 如果是，等待用户输入Python版本3. 执行shell命令下载源码包4. 安装依赖开发包5. 编译安装Python 脚本内容如下（基于Python2）： auto_install_python.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# coding=utf-8import os# 判断用户是否是root用户if os.getuid() == 0: passelse: print '当前用户不是root用户！' SystemExit(1)# 安装Python依赖库cmd_module = 'yum -y install wget gcc zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel libffi-devel'res = os.system(cmd_module)if res != 0: print 'Python依赖库安装失败，请重新执行该脚本。' SystemExit(1)else: print 'python依赖库安装成功！'# 输入Python版本，下载Python源码包到本地目录# wget urlversion = raw_input('请输入Python版本：（3.6/3.8）')if version == '3.6': url = 'https://www.python.org/ftp/python/3.6.10/Python-3.6.10.tgz'else: url = 'https://www.python.org/ftp/python/3.8.1/Python-3.8.1.tgz'cmd = 'wget ' + urlres = os.system(cmd)if res != 0: print 'Python源码包下载失败！' SystemExit(1)else: print('======================&gt;&gt;&gt;Python源码包下载成功！')# 解压Python源码包# tar zxvf Python-3.6.10.tgzif version == '3.6': package_name = 'Python-3.6.10'else: package_name = 'Python-3.8.1'res = os.system('tar zxvf ' + package_name + '.tgz')if res != 0: print '解压失败。。。' SystemExit(1)else: print('#########################解压成功！#########################')# 配置语言os.system('export LANG=zh_CN.UTF-8')os.system('export LANGUAGE=zh_CN.UTF-8')# 切换到Python目录os.chdir(package_name)os.system('./configure --prefix=/usr/local/python3')res = os.system('make &amp;&amp; make install')if res !=0: print '源码编译失败。。。' SystemExit(1)else: print('####################Python安装成功，请进行验证！####################')# 修改用户环境变量os.system('echo \"export PYTHON3=/usr/local/python3\" &gt;&gt;~/.bash_profile')os.system('echo \"export PATH=$PYTHON3/bin:$PATH\" &gt;&gt;~/.bash_profile')os.system(\"source ~/.bash_profile\")os.system('cat ~/.bash_profile')print('####################用户环境变量已修改，请进行验证！####################')os.system('python3 --version') 7、os.walk函数遍历目录树os.walk() 方法遍历某个目录及其子目录，对于每一个目录，walk()函数返回一个三元组（dirpath、dirnames、filenames）。其中dirpath保存的是当前目录，dirnames是当前目录下的子目录列表，filenames是当前目录下的文件列表。 12345678910# coding=utf-8import osfor root, dirs, files in os.walk(\".\", topdown=False): for name in files: print(os.path.join(root, name)) for name in dirs: print(os.path.join(root, name)) os.walk() 方法是一个简单易用的文件、目录遍历器，可以帮助我们高效的处理文件、目录方面的事情。 案例2：打印最常用的10条Linux命令123456789101112import osfrom collections import Countercount &#x3D; Counter()with open(os.path.expanduser(&#39;~&#x2F;.bash_history&#39;)) as f: for line in f: cmd &#x3D; line.strip().split() if cmd: count[cmd[0]] +&#x3D;1print(count.most_common(10)) 二、使用ConfigParser类解析配置文件Python中有ConfigParser类，可以很方便的从配置文件中读取数据（如DB的配置，路径的配置），所以可以自己写一个函数，实现读取config配置。 1、配置文件的格式12节： [session]参数(键=值) name=value mysql配置文件部分内容如下： 1234567891011[client]port = 3306user = mysqlpassword = mysqlhost = 127.0.0.1[mysqld]basedir = /usrdatadir = /var/lib/mysqltmpdir = /tmpskip-external-locking 2、ConfigParser类的使用方法（1）创建configParser对象123In [1]: import configparser In [2]: cf = configparser.ConfigParser(allow_no_value=True) （2）读取配置文件内容12In [3]: cf.read('my.inf')Out[3]: ['my.ini'] （3）获取配置文件信息12345678sections: 返回一个包含所有章节的列表options：返回一个包含章节下所有选项的列表has_section：判断章节是否存在has_options：判断某个选项是否存在items：以元组的形式返回所有的选项get、getboolean、getint、getfloat：获取选项的值 方法测试： 1234567891011121314151617In [4]: cf.sections()Out[4]: ['client', 'mysqld']In [5]: cf.has_section('client')Out[5]: TrueIn [6]cf.options('client')Out[6]: ['port', 'user', 'password', 'host']In [7]: cf.has_option('client','user')Out[7]: TrueIn [8]: cf.get('client','port')Out[8]: '3306'In [9]: cf.getint('client','port')Out[9]: 3306 （4）修改配置文件常用方法： 12345remove_section：删除一个章节add_section：添加一个章节remove_option：删除一个选项set：添加一个选项write：将ConfigParser兑现中的数据保存到文件中 方法测试： 123456789101112131415161718192021In [11]: cf.remove_section('client')Out[11]: TrueIn [12]: cf.write(open('my.ini', 'w'))In [13]: cf.add_section('client')In [14]: cf.set('client','port','3306')In [15]: cf.set('client','user','mysql')In [16]: cf.set('client','password','mysql')In [17]: cf.set('client','host','127.0.0.1')In [18]: cf.write(open('my.ini','w'))In [19]: cf.remove_option('client','host')Out[19: TrueIn [20]: cf.write(open('my.ini','w')) 三、查找文件1、使用fnmatch找到特定文件fnmatch.fnmatch()函数一次只能处理一个文件 12345678910import osimport fnmatchfor item in os.listdir('.'): if os.path.isfile(item): # if fnmatch.fnmatch(item, '*.jpg'): if fnmatch.fnmatch(item, '[a-e]*'): # if fnmatch.fnmatch(item, '[a-g]?.txt'): # if fnmatch.fnmatch(item, '[!a-c]*'): print(item) fnmatch.filter()函数一次可以处理多个文件 123456import osimport fnmatchitems = os.listdir('.')files = fnmatch.filter(items, '[a-c]*')print(files) 2、使用glob找到特定文件标准库glob的作用相当于os.listdir()加上fnmatch。使用glob以后，不需要调用os.listdir()获取文件列表，直接通过模式匹配即可。如下所示： 1234import globfiles = glob.glob('*.jpg')print(files) 案例3：找到目录下最大（或最老）的10个文件四、高级文件处理接口shutil1、复制文件和文件夹12shutil.copy(file1,file2)shuti.copytree(dir1, dir2) 2、文件和文件夹的移动与重命名12shutil.move(file1, file2)shutil.move(file, dir) 3、删除目录12shutil.rmtree(dir)os.unlink(file) 五、文件内容管理1、目录和文件对比filecmp模块包含了比较目录和文件的操作。 目录结构如下，其中，a.txt和c.txt内容是一样的，a_copy.txt是a.txt的拷贝。 1234567891011121314├─dir1│ │ a.txt│ │ a_copy.txt│ │ b.txt│ │ c.txt│ └─subdir1│ sa.txt└─dir2 │ a.txt │ b.txt │ c.txt ├─subdir1 │ sb.txt └─subdir2 （1）比较两个文件使用filecmp模块的cmp函数比较两个文件是否相同，如果文件相同则返回True，否则返回False。 12345678910111213In [1]: import filecmpIn [2]: cd dir1E:\\git-project\\python_project\\cloud33\\Python常用模块\\compare\\dir1In [3]: filecmp.cmp('a.txt','b.txt')Out[3]: FalseIn [4]: filecmp.cmp('a.txt','c.txt')Out[4]: TrueIn [5]: filecmp.cmp('a.txt','a_copy.txt')Out[5]: True （2）比较多个文件filecmp目录下还有一个名为cmpfiles的函数，该函数用来同时比较两个不同的目录下的多个文件，并且返回一个三元组，分别包含相同的文件、不同的文件和无法比较的文件。示例如下： 12In [6]: filecmp.cmpfiles('dir1','dir2',['a.txt','b.txt','c.txt','a_copy.txt'])Out[6]: (['a.txt', 'b.txt', 'c.txt'], [], ['a_copy.txt']) cmpfiles函数同时用来比较两个目录下的文件，也可以使用该函数比较两个目录。但是，在比较两个目录时，需要通过参数指定可能的文件，因此比较繁琐。 （3）比较目录filecmp中还有一个名为dircmp的函数，用来比较两个目录。调用dircmp函数以后，会返回一个dircmp类的对象，该对象保存了诸多属性，我们可以通过查看这些属性获取目录之间的差异。如下所示： 123456789101112131415161718192021In [7]: d = filecmp.dircmp('dir1','dir2')In [8]: d.report()diff dir1 dir2Only in dir1 : ['a_copy.txt']Only in dir2 : ['subdir2']Identical files : ['c.txt']Differing files : ['a.txt', 'b.txt']Common subdirectories : ['subdir1']In [9]: d.left_listOut[9]: ['a.txt', 'a_copy.txt', 'b.txt', 'c.txt', 'subdir1']In [10]: d.right_listOut[10]: ['a.txt', 'b.txt', 'c.txt', 'subdir1', 'subdir2']In [11]: d.left_onlyOut[11]: ['a_copy.txt']In [12]: d.right_onlyOut[12]: ['subdir2'] 2、MD5校验和比较校验码是通过散列函数计算而成，是一种从任何数据中创建小的数字“指纹”的方法。散列函数把消息或数据压缩成摘要，使得数据量变小，便于进行比较。MD5是目前使用最广泛的散列算法。 MD5哈希一般用于检查文件的完整性，尤其常用于检查文件传输、磁盘错误或其他情况下文件的正确性。 Linux下计算一个文件的MD5校验码，如下所示： 12[root@192 demo]# md5sum a.txtd41d8cd98f00b204e9800998ecf8427e a.txt 在Python中计算文件的MD5校验码也非常简单，使用标准库hashlib模块即可。如下所示： 123456789101112131415import hashlibd = hashlib.md5()with open('b.txt') as f: for line in f: d.update(line.encode('utf-8'))print(d.hexdigest())# 或者可以这样（最常见的写法，常用于图片的命名）&gt;&gt;&gt; hashlib.md5(b'123').hexdigest()'202cb962ac59075b964b07152d234b70'# 也可以使用hash.new()这个一般方法，hashlib.new(name[, data])，name传入的是哈希加密算法的名称，如md5&gt;&gt;&gt; hashlib.new('md5', b'123').hexdigest()'202cb962ac59075b964b07152d234b70' 六、使用Python管理压缩包1、tarfile（1）读取tar包123456789import tarfilewith tarfile.open('cmake-3.17.0.tar.gz') as t: for member in t.getmembers(): print(member.name) with tarfile.open('cmake-3.17.0.tar.gz') as t:t.extractall()t.extract('cmake-3.17.0/Help','a') 常用方法说明： 1234getmembers():获取tar包中的文件列表member.name:获取tar包中文件的文件名extract(member, path):提取单个文件extractall(path, memebers):提取所有的文件 （2）创建tar包1234import tarfilewith tarfile.open('readme.tar', mode='w') as out: out.add('read.txt') （3）读取与创建压缩包1234567import tarfilewith tarfile.open('tarfile_add.tar',mode='r:gz') as out: passwith tarfile.open('tarfile_add.tar',mode='r:bz2') as out: pass 案例4：备份指定文件到压缩包中123456789101112131415161718192021222324252627import osimport fnmatchimport tarfileimport datetimedef is_file_math(filename, patterns): '''查找特定类型的文件''' for pattern in patterns: if fnmatch.fnmatch(filename, pattern): return True return Falsedef find_files(root, patterns=['*']): for root, dirnames, filenames in os.walk(root): for filename in filenames: if is_file_math(filename, patterns): yield os.path.join(root, filename)patterns = ['*.txt','*.md']now = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')filename = 'backup_all_file_&#123;0&#125;.tar.gz'.format(now)with tarfile.open(filename, 'w') as f: for item in find_files('.', patterns): f.add(item) 2、zipfile（1）读取zip文件123456import zipfiledemo_zip = zipfile.ZipFile('read.zip')print(demo_zip.namelist())demo_zip.extractall('1')demo_zip.extract('a.jpg','2') 常用方法说明： 123namelist():返回zip文件中包含的所有文件和文件夹的字符串列表extract(filename, path)：从zip文件中提取单个文件extractall(path)：从zip文件中提取所有文件 （2）创建zip文件12345import zipfilenewZip = zipfile.ZipFile('new.zip',mode='w')newZip.write('a.jpg')newZip.close() （3）Python命令行调用zipfilezipfile模块提供的命令行接口包含的选项： 1234-l:显示zip格式压缩包中的文件列表-e:提取zip格式的压缩包-c:创建zip格式的压缩包-t:验证文件是不是一个有效的zip格式压缩包 示例如下所示： 12345678910# 创建zip文件python -m zipfile -c new1.zip archive_tar.py# 查看zip文件列表python -m zipfile -l new1.zipFile Name Modified Sizearchive_tar.py 2020-04-26 16:32:54 239# 提取zip文件到指定目录python -m zipfile -e new1.zip new_dir 3、shutil创建和读取压缩包shutil支持的格式如下： 12345import shutilprint(shutil.get_archive_formats())[('bztar', \"bzip2'ed tar-file\"), ('gztar', \"gzip'ed tar-file\"), ('tar', 'uncompressed tar file'), ('xztar', \"xz'ed tar-file\"), ('zip', 'ZIP file')] （1）创建压缩包1234# 参数1：生成的压缩包文件名# 参数2：压缩包的格式# 参数3：压缩的目录shutil.make_archive('a.jpg','gztar', 'ddd') （2）解压123# 参数1：需要解压的压缩包# 参数2：解压的目录print(shutil.unpack_archive('a.jpg.tar.gz','jpg')) 七、Python执行外部命令1、subprocess模块简介这个模块用来创建和管理子进程。它提供了高层次的接口，用来替换os.system()、os.spawn*()、os.popen*()、os.popen2.()和commands.\\等模块和函数。 subprocess提供了一个名为Popen的类启动和设置子进程的参数，由于这个类比较复杂，subprosess还提供了若干便利的函数，这些函数都是对Popen类的封装。 2、subprocess模块的便利函数（1）callcall函数的定义如下 1subprocess.call(args, *, stdin=None, stdout=None, stderr=None, shell=False) 示例如下： 12345678910111213141516171819In [1]: import subprocess In [2]: subprocess.call(['ls','-l']) 总用量 8-rw-------. 1 root root 2049 8月 11 2019 anaconda-ks.cfg-rw-r--r--. 1 root root 2077 8月 11 2019 initial-setup-ks.cfgdrwxr-xr-x. 2 root root 21 4月 22 16:32 osdrwxr-xr-x. 2 root root 6 8月 11 2019 公共drwxr-xr-x. 2 root root 6 8月 11 2019 模板drwxr-xr-x. 2 root root 6 8月 11 2019 视频drwxr-xr-x. 2 root root 6 8月 11 2019 图片drwxr-xr-x. 2 root root 6 8月 11 2019 文档drwxr-xr-x. 2 root root 6 8月 11 2019 下载drwxr-xr-x. 2 root root 6 8月 11 2019 音乐drwxr-xr-x. 2 root root 6 8月 11 2019 桌面Out[2]: 0 In [3]: subprocess.call('exit 1', shell=True) Out[3]: 1 （2）check_callcheck_all函数的作用与call函数类似，区别在于异常情况下返回的形式不同。 对于call函数，工程师通过捕获call命令的返回值判断命令是否执行成功，如果成功则返回0，否则的话返回非0。对于check_call函数，如果执行成功，返回0，如果执行失败，抛出subprocess.CallProseccError异常，示例如下所示： 123456789101112131415161718192021222324252627282930In [6]: subprocess.check_call(['ls','-l']) 总用量 8-rw-------. 1 root root 2049 8月 11 2019 anaconda-ks.cfg-rw-r--r--. 1 root root 2077 8月 11 2019 initial-setup-ks.cfgdrwxr-xr-x. 2 root root 21 4月 22 16:32 osdrwxr-xr-x. 2 root root 6 8月 11 2019 公共drwxr-xr-x. 2 root root 6 8月 11 2019 模板drwxr-xr-x. 2 root root 6 8月 11 2019 视频drwxr-xr-x. 2 root root 6 8月 11 2019 图片drwxr-xr-x. 2 root root 6 8月 11 2019 文档drwxr-xr-x. 2 root root 6 8月 11 2019 下载drwxr-xr-x. 2 root root 6 8月 11 2019 音乐drwxr-xr-x. 2 root root 6 8月 11 2019 桌面Out[6]: 0In [7]: subprocess.check_call('lsljdl', shell=True) /bin/sh: lsljdl: 未找到命令---------------------------------------------------------------------------CalledProcessError Traceback (most recent call last)&lt;ipython-input-7-885ea94380a9&gt; in &lt;module&gt;----&gt; 1 subprocess.check_call('lsljdl', shell=True)/usr/local/python3/lib/python3.8/subprocess.py in check_call(*popenargs, **kwargs) 362 if cmd is None: 363 cmd = popenargs[0]--&gt; 364 raise CalledProcessError(retcode, cmd) 365 return 0 366 CalledProcessError: Command 'lsljdl' returned non-zero exit status 127. （3）check_output1234567891011121314151617181920212223242526272829303132333435363738394041In [8]: output = subprocess.check_output(['df','-h']) In [9]: print(output.decode()) 文件系统 容量 已用 可用 已用% 挂载点/dev/mapper/cl-root 37G 5.1G 32G 14% /devtmpfs 897M 0 897M 0% /devtmpfs 912M 84K 912M 1% /dev/shmtmpfs 912M 9.0M 903M 1% /runtmpfs 912M 0 912M 0% /sys/fs/cgroup/dev/sda1 1014M 173M 842M 18% /boottmpfs 183M 16K 183M 1% /run/user/42tmpfs 183M 0 183M 0% /run/user/0 In [10]: lines = output.decode().split('\\n') In [11]: lines Out[11]: ['文件系统 容量 已用 可用 已用% 挂载点', '/dev/mapper/cl-root 37G 5.1G 32G 14% /', 'devtmpfs 897M 0 897M 0% /dev', 'tmpfs 912M 84K 912M 1% /dev/shm', 'tmpfs 912M 9.0M 903M 1% /run', 'tmpfs 912M 0 912M 0% /sys/fs/cgroup', '/dev/sda1 1014M 173M 842M 18% /boot', 'tmpfs 183M 16K 183M 1% /run/user/42', 'tmpfs 183M 0 183M 0% /run/user/0', '']In [12]: for line in lines[1:-1]: ...: if line: ...: print(line.split()[-2]) ...: t14%0%1%1%0%18%1%0% 3、subprocess模块的Popen类1234567891011121314151617# coding=utf-8import subprocessdef execute_cmd(cmd): p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = p.communicate() if p.returncode != 0: return p.returncode, stderr return p.returncode, stdoutexecute_cmd('ls') 八、综合案例：使用Python部署MongoDB看一个综合案例，使用Python不俗MongoDB数据库。在这个例子中，将会用到各种与系统管理相关的标准库，包括os、os.path、shutil、tarfile和subprocess模块。 假设当前目录下存在一个MongoDB安装包，我们的Python程序需要将他解压到当前目录的mongo目录下，并且当前目录创建一个mongodata目录用老保存MongoDB的数据库文件。 在部署MongoDB数据库之前，当前目录下的文件结构如下： 123.├── auto_install_mongodb.py└── ./mongodb-linux-x86_64-rhel70-4.2.3.tgz 程序部署完成后，当前目录的文件结构大致如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748.├── ./auto_install_mongodb.py├── ./mongo│ ├── ./mongo/bin│ │ ├── ./mongo/bin/bsondump│ │ ├── ./mongo/bin/install_compass│ │ ├── ./mongo/bin/mongo│ │ ├── ./mongo/bin/mongod│ │ ├── ./mongo/bin/mongodump│ │ ├── ./mongo/bin/mongoexport│ │ ├── ./mongo/bin/mongofiles│ │ ├── ./mongo/bin/mongoimport│ │ ├── ./mongo/bin/mongoreplay│ │ ├── ./mongo/bin/mongorestore│ │ ├── ./mongo/bin/mongos│ │ ├── ./mongo/bin/mongostat│ │ └── ./mongo/bin/mongotop│ ├── ./mongo/LICENSE-Community.txt│ ├── ./mongo/MPL-2│ ├── ./mongo/README│ ├── ./mongo/THIRD-PARTY-NOTICES│ └── ./mongo/THIRD-PARTY-NOTICES.gotools├── ./mongodata│ ├── ./mongodata/collection-0-4813754152483353608.wt│ ├── ./mongodata/collection-2-4813754152483353608.wt│ ├── ./mongodata/collection-4-4813754152483353608.wt│ ├── ./mongodata/diagnostic.data│ │ ├── ./mongodata/diagnostic.data/metrics.2020-04-27T10-17-57Z-00000│ │ └── ./mongodata/diagnostic.data/metrics.interim│ ├── ./mongodata/index-1-4813754152483353608.wt│ ├── ./mongodata/index-3-4813754152483353608.wt│ ├── ./mongodata/index-5-4813754152483353608.wt│ ├── ./mongodata/index-6-4813754152483353608.wt│ ├── ./mongodata/journal│ │ ├── ./mongodata/journal/WiredTigerLog.0000000001│ │ ├── ./mongodata/journal/WiredTigerPreplog.0000000001│ │ └── ./mongodata/journal/WiredTigerPreplog.0000000002│ ├── ./mongodata/_mdb_catalog.wt│ ├── ./mongodata/mongod.lock│ ├── ./mongodata/mongod.log│ ├── ./mongodata/sizeStorer.wt│ ├── ./mongodata/storage.bson│ ├── ./mongodata/WiredTiger│ ├── ./mongodata/WiredTigerLAS.wt│ ├── ./mongodata/WiredTiger.lock│ ├── ./mongodata/WiredTiger.turtle│ └── ./mongodata/WiredTiger.wt└── ./mongodb-linux-x86_64-rhel70-4.2.3.tgz MongoDB下载地址如下： 1wget https://fastdl.mongodb.org/linux/mongodb-linux-s390x-rhel72-4.3.3.tgz MongoDB是当下最流行的文档数据库，具有很好的易用性。启动一个MongoDB数据库实例，只需要执行一下几条shell命令即可： 1234tar -zxf mongodb-linux-x86_64-rhel70-4.2.3.tgzmv mongodb-linux-x86_64-rhel70-4.2.3 mongomkdir mongodatamongo/bin/mongod --fork --logpath mongodata/mongod.log --dbpath mongodata 这里给出的shell命令，只是为了便于不熟悉MongoDB的人了解MongoDB数据库的启动过程，还有很多的情况没有考虑。例如，要将当前目录下的MongoDB安装包解压到当前目录下的mongo目录中，但是当前目录下已经存在一个名为mongo的目录，则会报错 下面的程序时使用Python部署MongoDB数据库的完整代码，这段程序综合应用了很多与系统管理相关的模块。如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# coding=utf-8import subprocessimport osimport shutilimport tarfiledef execute_cmd(cmd): '''执行shell命令''' p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE) stdout, stderr = p.communicate() if p.returncode != 0: return p.returncode, stderr return p.returncode, stdoutdef unpackage_mongo(package, package_dir): unpackage_dir = os.path.splitext(package)[0] if os.path.exists(unpackage_dir): shutil.rmtree(unpackage_dir) if os.path.exists(package_dir): shutil.rmtree(package_dir) # 解压 t = tarfile.open(package, 'r:gz') t.extractall('.') print('tar is ok.') # 重命名mongodb-linux-x86_64-rhel70-4.2.3为mongo shutil.move(unpackage_dir, 'mongo')def create_datadir(data_dir): if os.path.exists(data_dir): shutil.rmtree(data_dir) os.mkdir(data_dir)def format_mongod_commamd(package_dir, data_dir, logfile): mongod = os.path.join(package_dir, 'bin', 'mongod') mongod_format = \"\"\"&#123;0&#125; --fork --dbpath &#123;1&#125; --logpath &#123;2&#125;\"\"\" return mongod_format.format(mongod, data_dir, logfile)def start_mongod(cmd): returncode, out = execute_cmd(cmd) if returncode != 0: raise SystemExit('execute &#123;0&#125; error:&#123;1&#125;'.format(cmd, out.decode())) else: print('execute &#123;0&#125; sucessfully.'.format(cmd))def main(): package = 'mongodb-linux-x86_64-rhel70-4.2.3.tgz' cur_dir = os.path.abspath('.') package_dir = os.path.join(cur_dir, 'mongo') data_dir = os.path.join(cur_dir, 'mongodata') logfile = os.path.join(data_dir, 'mongod.log') if not os.path.exists(package): raise SystemExit('&#123;0&#125; not found.'.format(package)) unpackage_mongo(package, package_dir) create_datadir(data_dir) start_mongod(format_mongod_commamd(package_dir, data_dir, logfile))main() 代码说明： 123456789在这段过程中，我们首先在main函数中定义了几个变量，包括当前目录的路径，MongoDB二进制文件所在的路径、MongoDB数据目录所在的路径，以及MongoDB的日志问紧啊随后，我们判断MongoDB的安装包是否存在，如果不存在，则通过抛出SystemExit异常的方式结束程序在unpackage_mongo函数中，我们通过Python程序得到MongoDB安装包解压以后的目录。如果目录已经存在，则删除该目录。随后，我们使用tarfile解压MongoDB数据库，解压完成后，将命令重命名为mongo目录在create_datadir目录红，我们首先判断MongoDB数据库目录是否存在，如果存在，则删除该目录，随后在创建MongoDB数据库目录在start_mongod函数中，我们执行MongoDB数据库的启动命令启动MongoDB数据库。为了在Python代码中执行shell命令，我们使用了subprocess库，我们将subprocess库执行shell命令的瑞吉封装成execute_cmd函数，在执行shell命令时，直接调用该函数即可","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Python生态工具","slug":"Python生态工具","date":"2020-04-26T16:00:00.000Z","updated":"2020-04-27T14:45:09.576Z","comments":true,"path":"Python生态工具.html","link":"","permalink":"https://pdxblog.top/Python%E7%94%9F%E6%80%81%E5%B7%A5%E5%85%B7.html","excerpt":"","text":"Python生态工具一、Python内置小工具1.1、 1秒钟启动一个下载服务器在实际工作中，时常会有这样的一个需求：将文件传给其他同事。将文件传给同事本身并不是一个很繁 琐的工作，现在的聊天工具一般都支持文件传输。但是，如果需要传送的文件较多，操作起来就会比较 麻烦。此外，如果文件在远程的服务器上，则需要先将远程服务器的文件下载到本地，然后再通过聊天 工具传给同事。再或者，你并不是特别清楚要传哪几个文件给同事，所以，你们需要进行交流，而交流 的时间成本是比较高的，会降低办事效率。 此时，如果你知道Python内置了一个下载服务器就能够显著提升效率了。例如，你的同事要让你传的文 件位于某一个目录下，那么，你可以进入这个目录，然后执行下面的命令启动一个下载服务器： 1python -m SimpleHTTPServer 在Python 3中，由于对系统库进行了重新整理，因此，使用方式会有不同： 1python -m http.server 执行上面的命令就会在当前目录下启动一个文件下载服务器，默认打开8000端口。完成以后，只需要将 IP和端口告诉同事，让同事自己去操作即可，非常方便高效。 使用浏览器访问Python启动的下载服务器，可以看到一个类似于FTP下载的界面，这个时候单击文件下 载即可。通过这种方式传输文件，可以降低大家的沟通成本，提高文件传输的效率。 上面使用的Python语句，从工作原理来说，仅仅是启动了一个Python内置的Web服务器。如果当前目 录下存在一个名为index.html的文件，则默认显示该文件的内容。如果当前目录下不存在这样一个文 件，则默认显示当前目录下的文件列表，也就是大家看到的下载服务器。 1.2、字符串转换为JSONJSON是一种轻量级的数据交换格式，易于人类阅读和编写，同时也易于机器解析和生成。由于JSON的 诸多优点，已被广泛使用在各个系统中。JSON使用越广泛，需要将JSON字符串转换为JSON对象的需求 就越频繁。 例如，在工作过程中，我们的系统会调用底层服务的API。底层服务的API一般都是以JSON的格式返 回，为了便于问题追踪，我们会将API返回的JSON转换为字符串记录到日志文件中。当需要分析问题 时，就需要将日志文件中的JSON字符串拿出来进行分析。这个时候，需要将一个JSON字符串转换为 JSON对象，以提高日志的可读性。 这个需求十分常见，以至于使用搜索引擎搜索”JSON”，处于搜索结果的第一项便是“在线JSON格式化工 具”。除了打开浏览器，使用在线JSON格式化工具以外，我们也可以使用命令行终端的Python解释器来 解析JSON串，如下所示 123456789[root@oracle ~]# echo '&#123;\"address\": &#123;\"province\": \"zhejiang\", \"city\": \"hangzhou\"&#125;, \"name\": \"lmx\", \"sex\": \"male\"&#125;' | python -m json.tool &#123; \"address\": \"city\": \"hangzhou\", \"province\": \"zhejiang &#125;, \"name\": \"lmx\" \"sex\": \"male\"&#125; 使用命令行解释器解析JSON串非常方便，而且，为了便于阅读，该工具还会自动将转换的结果进行对 齐和格式化。如下所示： 12345678910[root@oracle ~]# echo '&#123;\"address\": &#123;\"province\": \"zhejiang\", \"city\":\"hangzhou\"&#125;, \"name\": \"lmx\", \"sex\": \"male\"&#125;' | python -m json.tool&#123; \"address\": &#123; \"city\": \"hangzhou\", \"province\": \"zhejiang\" &#125;, \"name\": \"lmx\", \"sex\": \"male\"&#125; 1.3、检查第三方库是否正常安装安装完Python的第三方库以后，如何确认这个库已经正确安装了呢？答案很简单，只需要尝试进行 import导入即可。如果导入没有任何错误，则认为安装成功；如果导入失败，则认为安装失败。 12345[root@oracle ~]# pythonPython 2.7.5 (default, Oct 30 2018, 23:45:53)[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux2Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.&gt;&gt;&gt; 验证Python的第三方库是否安装成功，本身也是一件很简单的事情，但是，如果我们使用脚本对大批量 的服务器进行自动部署，又应该如何验证第三方库安装成功了呢？肯定不能登录每一台服务器进行验 证。这个时候，我们可以使用Python解释器的-c参数快速地执行import语句，如下所示： 1234[root@oracle ~]# python -c \"import paramiko\"Traceback (most recent call last):File \"&lt;string&gt;\", line 1, in &lt;module&gt;ImportError: No module named paramiko 二、pip高级用法为了便于用户安装和管理第三方库和软件，越来越多的编程语言拥有自己的包管理工具，如nodejs的 npm，ruby的gem。Python也不例外，现在Python生态主流的包管理工具是pip 2.1、pip介绍pip是一个用来安装和管理Python包的工具，是easy_install的替代品，如果读者使用的是Python 2.7.9+或Python 3.4+版本的Python，则已经内置了pip，无须安装直接使用即可。如果系统中没有安装 pip，也可以手动安装 2.2、python3安装pip 方法1：python33安装完成后默认已经带有pip3 12345[root@oracle bin]# pip3 -Vpip 19.2.3 from /usr/local/python38/lib/python3.8/site-packages/pip (python3.8)[root@oracle bin]# pwd/usr/local/python38/bin 你可以用以下命令,创建软链接 1ln -s /usr/local/python38/bin/pip3 /usr/bin/pip3 方法2：使用以下方法重新安装pip插件 下载get-pip.py脚本 1wget https://bootstrap.pypa.io/3.2/get-pip.py 运行脚本 1python3 get-pip.py python3创建pip3索引 1ln -s /usr/python3.6.1/bin/pip /usr/bin/pip3 测试是否安装成功 1pip3 install requests pip之所以能够成为最流行的包管理工具，并不是因为它被Python官方作为默认的包管理器，而是因为 它自身的诸多优点。pip的优点有： 123456pip提供了丰富的功能，其竞争对手easy_install则只支持安装，没有提供卸载和显示已安装列表的功能；pip能够很好地支持虚拟环境；pip可以通过requirements.txt集中管理依赖；pip能够处理二进制格式(.whl)；pip是先下载后安装，如果安装失败，也会清理干净，不会留下一个中间状态。 如果用户没有将软件打包上传到pypi.python.org，则无法使用pip进行安装。对于这种情况，Python生 态也有标准的做法，例如，我们尝试从源码安装paramiko。需要注意的是，我们也可以通过pip安装 paramiko的，这里只是为了演示Python生态中源码安装： 123$ git clone https://github.com/paramiko/paramiko.git$ cd paramiko$ python setup.py install 2.3、给pip3重命名切换至家目录，通过.bashrc添加别名 1234567[root@oracle bin]# cd ~[root@localhost ~]# vim .bashrcalias pip=pip3[root@localhost ~]# source .bashrc[root@localhost ~]# pip -Vpip 19.2.3 from /usr/local/python38/lib/python3.8/site-packages/pip (python3.8) 2.4、pip3常用命令 子命令 解释说明 install 安装软件包 download 下载软件包 uninstall 卸载安装包 freeze 按照requirements格式输出安装包，可以到其他服务器上执行pip install -r requirements.txt直接安装软件 list 列出当前系统中的安装包 show 查看安装包的信息，包括版本、依赖、许可证、作者、主页等信息 check 检查安装包依赖是否完整 search 查找安装包 wheel 打包软件到wheel格式 hash 计算安装包的hash值 completion 生成命令补全配置 help 获取pip和子命令的帮助信息 2.5、加速pip安装的技巧如果大家使用Python的时间比较长的话，会发现Python安装的一个问题，即pypi.python.org不是特别 稳定，有时候会很慢，甚至处于完全不可用的状态。这个问题有什么好办法可以解决呢？根据笔者的经 验，至少有两种不同的方法。 1、使用豆瓣或阿里云的源加速软件安装访问pypi.python.org不稳定的主要原因是因为网络不稳定，如果我们从网络稳定的服务器下载安装 包，问题就迎刃而解了。我们国内目前有多个pypi镜像，推荐使用豆瓣的镜像源或阿里的镜像源。如果 要使用第三方的源，只需要在安装时，通过pip命令的-i选项指定镜像源即可。如下所示： 1pip install -i https://pypi.douban.com/simple/ flask 每次都要指定镜像源的地址比较麻烦，我们也可以修改pip的配置文件，将镜像源写入配置文件中。对 于Linux系统来说，需要创建～/.pip/pip.conf文件，然后在文件中保存如下内容： 123456[root@localhost ~]# mkdir .pip[root@localhost ~]# cd .pip[root@localhost .pip]# touch pip.conf[root@localhost .pip]# vim pip.conf[global]index-url = https://pypi.douban.com/simple/ 2、将软件下载到本地部署如果需要对大批量的服务器安装软件包，并且安装包比较多或者比较大，则可以考虑将软件包下载到本 地，然后从本地安装。这对于使用脚本部署大量的服务器非常有用，此外，对于服务器无法连接外网的 情况，也可以使用这种方法。如下所示： 12345# 下载到本地pip install --download='pwd' -r requirements.txt# 本地安装pip install --no-index -f file://'pwd' -r requirements.txt 使用这种方式，只需要下载一次，就可以多处安装，不用担心网络不稳定的问题。并且，pip能够自动 处理软件依赖问题。例如，我们通过这种方式下载Flask到当前目录下，则Flask的依赖click、 itsdangerous、Jinja2、MarkupSafe和Werkzeug也会被下载到本地，如下所示： 123456pip install --download='pwd' flask$ lsclick-6.7-py2.py3-none-any.whl itsdangerous-0.24.tar.gzMarkupSafe-0.23.tar.gz Flask-0.12-py2.py3-none-any.whlJinja2-2.9.5-py2.py3-none-any.whl Werkzeug-0.11.15-py2.py3-none-any.whl 三、Python变成辅助工具因为Python是一门动态类型语言，所以，Python程序不需要编译和链接就可以直接运行。Python程序 运行时是从上至下逐行执行，因此Python工程师可以进行交互式的编程，从而快速验证代码的运行结果 是否符合预期。同时，Python工程师也可以通过交互式编程的方式学习Python编程。也正是因为 Python交互式编程的诸多优点，所以，Python交互式编程使用非常广泛。 3.1、Python交互式编程要使用Python的交互式编程，最简单的方式是使用标准的Python Shell。在命令行直接输入python命 令便可进入Python Shell，如下所示： 12345678[root@localhost ~]# pythonPython 3.8.1 (default, Jan 14 2020, 12:20:36)[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linuxType \"help\", \"copyright\", \"credits\" or \"license\" for more information.&gt;&gt;&gt; a=3&gt;&gt;&gt; b=4&gt;&gt;&gt; a+b7 虽然标准的Python Shell也支持交互式编程，但是，它有很多不足，包括： 123456没有语法高亮；不支持Tab自动补全；没有自动缩进功能；不能保存历史记录；不能很好地与操作系统交互；无法导入外部文件中的程序。 虽然Python自带的交互式编程满足了功能性需求，但是在易用性上仍有诸多不足。IPython是增强型的 Python Shell，不但解决了上面提到的各种问题，而且提供了非常丰富的组件，可以方便地进行交互式 编程和数据分析。IPython功能丰富，不可避免地导致软件变得庞大复杂，因此，IPython 4.0对 IPython进行了拆分，分离成IPython Shell和jupyter两个组件，这两个组件现在需要分别安装。 按照行业惯例，IPython代指IPython Shell，是一个类似于Python Shell的交互式解释器；jupyter代指 IPython Notebook，是一个带图形界面的应用程序。接下来我们分别介绍IPython和jupyter的使用。 3.2、使用IPython交互编程IPython是一个第三方工具，因此，在使用之前需要先安装。可以直接使用操作系统的包管理工具或pip 进行安装。以下是在centos7上的安装方式： 1234567891011121314151617[root@localhost ~]# pip install ipythonLooking in indexes: https://pypi.douban.com/simple/Collecting ipython Downloadinghttps://pypi.doubanio.com/packages/1c/f3/c8be38ee117d02508bb8b9158eb41ca416f442a6e8e3b3159c2f2d14ed79/ipython-7.11.1-py3-none-any.whl (777kB) |████████████████████████████████| 778kB 934kB/s……省略部分输出信息Installing collected packages: six, ipython-genutils, decorator, traitlets,ptyprocess, pexpect, pickleshare, wcwidth, prompt-toolkit, pygments,backcall, parso, jedi, ipython Running setup.py install for backcall ... doneSuccessfully installed backcall-0.1.0 decorator-4.4.1 ipython-7.11.1ipython-genutils-0.2.0 jedi-0.15.2 parso-0.5.2 pexpect-4.7.0 pickleshare0.7.5 prompt-toolkit-3.0.2 ptyprocess-0.6.0 pygments-2.5.2 six-1.13.0traitlets-4.3.3 wcwidth-0.1.8 安装完成以后，在命令行终端输入ipython就进入了IPython交互式编程界面： 12345678910111213141516171819[root@localhost ~]# ipythonPython 3.8.1 (default, Jan 14 2020, 12:20:36)Type 'copyright', 'credits' or 'license' for more informationIPython 7.11.1 -- An enhanced Interactive Python. Type '?' for help.In [1]: sum=0In [2]: for i in range(5): ...: sum+=i ...: print(sum)10In [3]: import osIn [4]: os.getlogin()Out[4]: 'root'In [5]: 与标准的Python Shell一样，IPython的行显示了所使用的Python解释器版本以及当前的时间。第二行 是获取版权信息的方式，接着给出了IPython的版本。后是简短的使用说明，包括特征介绍、简短的 使用手册和如何获取帮助信息。表2-2给出了IPython提供的使用说明。 接下来我们将从五个不同的维度介绍IPython的使用，分别是： ①更好的编辑器； ②更方便地获取帮助信息； ③IPython提供的magic函数； ④IPython的保存历史功能； ⑤IPython与操作系统交 （1）更好的编辑器IPython非常强大，有各种高级功能。其中，有用也直观的便是作为交互式编程工具的编辑器功 能。简单来说，IPython相对于标准的Python Shell是一个更好的交互式编程的编辑器，因为它具有： 123456语法高亮； 自动缩进； Tab补全； 快速获取帮助信息； 搜索历史； 执行shell命令 如果只是描述IPython的特征，相信读者并没有完全的概念。这个时候可以坐在计算机旁，打开IPython 随便敲几行Python代码和标准的Python Shell进行比较，就能够直观感受到IPython的优点。 IPython与标准Python Shell的大区别在于，IPython会对命令提示符的每一行进行编号，编号以后能 够提高交互式编程的可读性。更重要的是，我们可以通过IPython提供的特殊函数对编号以后的代码进 行操作。此外，IPython支持语法高亮和自动缩进，相对于标准的Python Shell，是一个更好的编辑 器。如果在编写代码的过错中出现了错误需要删除时，标准的Python Shell无法进行很好的处理，只能 重新进行输入，而IPython则不存在这样的问题。 tab补全是一个特别有用的功能，IPython支持tab补全，而标准的Python Shell不支持。大家可以想象 一下，一个工程师近正在学习Python，他知道一个库里面有他想要的函数，但是，他并不能非常准确 地说出这个函数的名称。这个时候，如果没有tab补全，就只能一边打开Python官方的参考手册，一边 学习编程。有了tab补全以后，即使他对函数名称不是特别熟悉也没有关系，可以先通过tab补全列出当 前命名空间下的函数列表，然后根据函数名称选择自己需要的函数。IPython的补全功能非常强大，不 但可以补全用户的变量名、标准库的函数，在导入包时也可以进行补全。 这一小节，我们一直在强调IPython比标准的Python Shell更好用，拥有更多高级功能。如果读者接触 Python的时间不长，也许不能理解为什么需要使用交互式编程。交互式编程在当前会话退出以后就结束 了，并不满足计算机程序一次编写多次运行的特点。但是，在我们的日常工作中还是会经常用到交互式编程。 交互式编程不但可以快速验证代码执行结果，还可以帮助我们学习Python编程。Python工程师在编写 代码时，通常会使用编辑器和Python Shell组合的方式来完成程序的编写，例如，将代码从编辑器复制 到Python Shell以验证代码的正确性，然后将验证过的代码从Python Shell复制到编辑器中。 （2）使用IPython来解析MySQL的备份日志为了便于读者理解交互式编程的好处，我们这里演示一个使用Python交互式编程的例子。在这个例子 中，我们使用IPython来解析MySQL的备份日志。 一个典型的MySQL物理备份日志如下所示： 123456789101112131415170221 01:07:48 Executing UNLOCK TABLES170221 01:07:48 All tables unlockedStarting slave SQL thread170221 01:07:48 [00] Streaming ib_buffer_pool to &lt;STDOUT&gt;170221 01:07:48 [00] ...done170221 01:07:48 Backup created in directory '/home/lmx/log/backup'MySQL binlog position: filename 'mysql-bin.000003', position '507946128',GTID of the last change '5a81ea97-daf1-11e6-94c1-fa163ee35df3:1-3409440'MySQL slave binlog position: master host '10.173.33.35', filename 'mysqlbin.000002', position '524993060'170221 01:07:48 [00] Streaming backup-my.cnf170221 01:07:48 [00] ...done170221 01:07:48 [00] Streaming xtrabackup_info170221 01:07:48 [00] ...donextrabackup: Transaction log of lsn (3387315364) to (3451223966) was copied.170221 01:07:48 completed OK! 即使读者对MySQL不了解也没有关系，我们现在的需求是解析下面这一行日志，并获取日志中的host、 filename和position的值。虽然在日志中position的值包含在一对单引号内，但是，我们希望解析以后 position的值是一个整数。 1MySQL slave binlog position: master host '10.173.33.35', filename 'mysqlbin.000002', position '524993060' 在这个例子中，主要就是对字符串进行处理，并提取相应的值。这个问题当然不难，但是，如果不借助 交互式编程工具，需要工程师一次在代码中编写正确也不简单。如果工程师不知道交互式编程工具，就 只能在编辑器里面编写代码，然后运行。如果有错误再修改，直到获取正确的取值，整个过程将会非常 耗时。如果项目庞大，调试起来也会比较困难。这个时候就可以借助Python的交互式编程工具，先验证 代码的正确性，然后将验证过的代码从交互式编程工具复制到编辑器中。如下所示： 12345678910111213141516171819202122232425[root@localhost ~]# ipythonPython 3.8.1 (default, Jan 14 2020, 12:20:36)Type 'copyright', 'credits' or 'license' for more informationIPython 7.11.1 -- An enhanced Interactive Python. Type '?' for help.In [1]: line=\"MySQL slave binlog position: master host '10.173.33.35',filename 'mysql-bin.000002', position '524993060'\"In [2]: line.split(\"'\")Out[2]:['MySQL slave binlog position: master host ','10.173.33.35',', filename ','mysql-bin.000002',', position ','524993060','']In [3]: host=line.split(\"'\")[1]In [4]: filename=line.split(\"'\")[3]In [5]: position=line.split(\"'\")[5]In [6]: print(host,filename,position)10.173.33.35 mysql-bin.000002 524993060In [7]: type(position)Out[7]: strIn [8]: position=int(position)In [9]: print(host,filename,position)10.173.33.35 mysql-bin.000002 524993060 为了节省文章篇幅，我们没有进行错误的尝试，而是直接通过单引号来分解字符串。由于我们使用了交 互式编程，可以很方便地看到字符串分解以后的中间结果。正是有了这个中间结果，我们才知道，字符 串分解成列表以后，下标1对应的字符串是host的值，下标3对应的字符串是filename的值，下标5对应 的字符串是position的值。我们还可以通过交互式编程发现position是一个字符串。由于我们要求 position是一个整数，因此，需要在代码中将position强制转换为一个整数。 如果读者自己尝试从这一行字符串中获取有效的值，很可能一开始会尝试使用逗号或空格来分解字符 串。这两种方法都无法一次取出host、filename和position的值。如下所示： 12345678910111213141516171819In [10]: line.split(',')Out[10]:[\"MySQL slave binlog position: master host '10.173.33.35'\",\" filename 'mysql-bin.000002'\",\" position '524993060'\"]In [11]: line.split(' ')Out[11]:['MySQL','slave','binlog','position:','master','host',\"'10.173.33.35',\",'filename',\"'mysql-bin.000002',\",'position',\"'524993060'\"] 使用交互式编程，我们可以快速尝试不同的方案，先验证自己的想法是否正确，然后将代码拷贝到编辑 器中，组成我们的Python程序文件。通过这种方式，能够有效降低代码出错的概率，减少调试的时间， 从而提高工作效率。 （3）更好地获取帮助信息Python工程师不但可以通过交互式编程快速验证代码执行结果，还可以通过交互式编程的方式学习 Python编程。之所以说Python工程师可以通过交互式编程学习编程，是因为使用IPython能够方便地获 取到相应的帮助信息。如命名空间下的每个对象以及其定义和使用说明。虽然标准的Python Shell也可 以通过help函数获取到对象的帮助信息，但是，IPython提供了更加灵活的方式获取命名空间下的对象 列表，以及更加全面的帮助信息。 我们知道，在标准库的os模块下的path子模块中有很多操作文件、目录和路径的函数，也有很多 以”is”开始的判断类函数。这些判断类函数的作用非常明确，用以判断给定的对象是否为一个文件或一 个目录。我们可以使用通配符的方式获取该模块下的所有判断类函数，如下所示： 1234567In [10]: import osIn [11]: ?os.path.is*os.path.isabsos.path.isdiros.path.isfileos.path.islinkos.path.ismount 获取当前命名空间下的所有对象，除了使用通配符的方式以外，也可以使用前面介绍的tab补全方式。 tab补全的方式更加实用一些，就如同IPython提供的获取帮助信息的方式比标准的Python Shell获取帮 助信息更实用一样。在IPython中，可以通过标准的help函数获取对象的帮助信息，也可以使用“？”和 “？？”获取对象的帮助信息，如下所示： 123456789101112131415161718192021222324252627282930313233In [12]: import jsonIn [13]: import osIn [14]: os.path.isfile?Signature: os.path.isfile(path)Docstring: Test whether a path is a regular fileFile: ~/.pyenv/versions/3.8.1/lib/python3.8/genericpath.pyType: functionIn [15]: json.dump?Signature:json.dump(obj,fp,*,skipkeys=False,ensure_ascii=True,check_circular=True,allow_nan=True,cls=None,indent=None,separators=None,default=None,sort_keys=False,**kw,)Docstring:Serialize ``obj`` as a JSON formatted stream to ``fp`` (a``.write()``-supporting file-like object).If ``skipkeys`` is true then ``dict`` keys that are not basic types(``str``, ``int``, ``float``, ``bool``, ``None``) will be skippedinstead of raising a ``TypeError``. 当我们输入对象名称，再输入一个问号以后按回车键，就会显示相应的帮助信息。如果帮助信息比较 长，则会以分页的方式显示帮助信息。如果因为帮助信息太多而进入了分页页面，可以通过“q”键退出， 退出以后可以继续进行编程。 例如，json这个标准库下有一个dump函数和一个dumps函数，Python初学者总是容易混淆。这个时 候，如果能够充分利用IPython，就可以方便地获取到帮助信息，使用时不容易犯错。下面就是一个典 型的Python工程师使用json模块的方式，先构造了一个字典，希望将字典转换成json字符串。因为不知 道应该使用json.dump函数还是json.dumps函数，所以，在交互式编程中通过“json.dump?”语句获取dump函数的帮助信息。获取完json.dump函数的帮助信息以后，按“q”键退出，退出以后继续进行编 程。如下所示： 12345In [18]: import jsonIn [19]: d=dict(a=1,b=2,c=3)In [20]: json.dump?In [21]: json.dumps(d)Out[21]: '&#123;\"a\": 1, \"b\": 2, \"c\": 3&#125;' 在IPython中，除了使用一个问号获取帮助信息以外，也可以使用两个问号获取帮助信息。两个问号获 取到的帮助信息更加全面，甚至会包含函数的实现源码。 除了使用问号的方式获取对象的帮助信息以外，IPython还提供了另外一种方式获取对象的信息，可以 分别获取对象的定义、文档和文件等。如下所示： 12345678910111213141516171819202122In [22]: import jsonIn [23]: %pdef jsonObject is not callable.In [24]: %pdef json.dumpjson.dump(obj,fp,*,skipkeys=False,ensure_ascii=True,check_circular=True,allow_nan=True,cls=None,indent=None,separators=None,default=None,sort_keys=False,**kw,)In [25]: %pfile json.dumpIn [26]: %pdoc json.dumpIn [27]: %pinfo json （4）magic函数IPython提供了很多功能强大的函数，如前面已经提到的%pfile、%pdoc、%pinfo等。为了区分 IPython提供的函数和用户的输入，所有IPython提供的函数都以“%”开头。以“%”开头的这类功能强大的 函数，在IPython中称为magic函数。magic函数主要是为IPython提供增强的功能、与操作系统交互、 操纵用户的输入和输出以及对IPython进行配置。 IPython会将任何第一个字母为“%”的行，视为对magic函数的特殊调用。因此，所有的magic函数都是 以“%”开头。在IPython中，有两种不同的方法可以获取magic函数列表，分别是通过“%”获取所有的 magic函数和通过“%lsmagic”获取所有的magic函数。 下面是一个用lsmagic函数获取magic函数列表的例子： 1234567891011121314151617181920212223In [28]: %lsmagicOut[28]:Available line magics:%alias %alias_magic %autoawait %autocall %autoindent %automagic%bookmark %cat %cd %clear %colors %conda %config %cp %cpaste%debug %dhist %dirs %doctest_mode %ed %edit %env %gui %hist%history %killbgscripts %ldir %less %lf %lk %ll %load %load_ext%loadpy %logoff %logon %logstart %logstate %logstop %ls %lsmagic%lx %macro %magic %man %matplotlib %mkdir %more %mv %notebook%page %paste %pastebin %pdb %pdef %pdoc %pfile %pinfo %pinfo2 %pip%popd %pprint %precision %prun %psearch %psource %pushd %pwd %pycat%pylab %quickref %recall %rehashx %reload_ext %rep %rerun %reset%reset_selective %rm %rmdir %run %save %sc %set_env %store %sx%system %tb %time %timeit %unalias %unload_ext %who %who_ls %whos%xdel %xmodeAvailable cell magics:%%! %%HTML %%SVG %%bash %%capture %%debug %%file %%html %%javascript%%js %%latex %%markdown %%perl %%prun %%pypy %%python %%python2%%python3 %%ruby %%script %%sh %%svg %%sx %%system %%time %%timeit%%writefileAutomagic is ON, % prefix IS NOT needed for line magics. 可以看到，IPython提供了很多magic函数。并且，随着IPython的功能越来越多，magic函数还会不断 增加。那么，有没有一种好的方法能够快速了解magic函数的用法呢？前面介绍的通过问号获取对象帮 助信息的方法对magic函数也适用。因此，只要输入一个magic函数，后面再输入一个问号，回车以后 就能够看到这个magic函数的帮助信息。如下所示： 123456789101112131415In [29]: %save?Docstring:Save a set of lines or a macro to a given filename.Usage: %save [options] filename n1-n2 n3-n4 ... n5 .. n6 ...Options: -r: use 'raw' input. By default, the 'processed' history is used,so that magics are loaded in their transformed version to validPython. If this option is given, the raw input as typed as thecommand line is used instead. -f: force overwrite. If file exists, %save will prompt for overwriteunless -f is given. -a: append to the file instead of overwriting it.: IPython的官方文档将magic函数分为三类，分别是： 1231）操作代码的magic函数，如%run、%edit、%save、%macro、%recall；2）控制IPython的magic函数，如%colors、%xmode、%autoindent、%automagic；3）其他magic函数，如%reset、%timeit、%%writefile、%load、%paste。 为了演示magic函数的使用，我们来看一个实际的例子。假设你是一名DBA，并且非常喜欢Python这门 编程语言，会经常使用Python管理MySQL。因此，你经常需要使用Python连接MySQL执行SQL语句 （Python连接MySQL的知识将在11章介绍）。使用Python执行SQL语句，对于普通的查询语句，返回 的结果将是一个二维的元组。但是，如果执行的是一些管理类的SQL语句或者监控类的SQL语句， Python驱动将会以怎样的方式返回MySQL的查询结果呢？ 例如，需要执行下面的SQL语句，并获取返回结果： 1234567show slave status；show master status；show variables like '%innodb%buffer%'；show status like '%select%'；set global innodb_buffer_pool_dump_pct = 30；GRANT ALL PRIVILEGES ON . TO ['lmx'@'localhost'](mailto:'lmx'@'localhost')WITH GRANT OPTION。 为了得到Python执行上面SQL语句的结果，需要在Python中连接MySQL并进行认证。认证完成以后执 行SQL语句获取输出。由于你经常需要验证SQL语句，因此，使用Python连接MySQL并认证这些代码需 要反复输入。为了节省输入时间，我们可以将Python连接MySQL并认证的逻辑保存到外部文件中，在 需要的时候通过%load这个magic函数将外部代码导入到IPython中执行即可。例如，我们在一个名为 connect.py的外部文件中保存了连接MySQL的代码，在Ipython中使用%load导入外部Python文件： 1234567891011In [30]: %load connect.py In [31]: import MySQLdb as dbconn = db.connect(host=\"localhost\", db=\"test\", user='lmx',passwd='my_passwd', unix_socket='/tmp/mysql.sock')cur = conn.cursor()sql = \"select 1\"cur.execute(sql)rows = cur.fetchall()print rows((1L,),) 使用%load命令导入外部的Python文件并执行以后，可以继续使用已经建立的MySQL连接执行SQL语 句。这个例子主要用以演示magic函数的用法，IPython提供了大量的magic函数，每一个magic函数的 具体用法都可以通过问号表达式获取相应的帮助文档。 （5）保存历史保存编码历史这方面，IPython相比标准的Python Shell有了质的提升。用户可以非常灵活地操作 IPython的输入历史和输出历史。下面我们简单看几个例子： _i, _ii, _iii 分别保存了最近的三次输入； _, , _ 分别保存了最近的三次输出； 可以像Bash一样，通 过ctrl+p, ctrl+n查找输入； 可以像Bash一样，使用ctrl+r进行反向查找； IPython的输入历史在 当前会话退出以后会进行持久化，下一次进入IPython时，依然可以查找前一次会话的输入历史； %edit IPython可以通过%edit编辑历史输入并重新执行； %save IPython可以通过%save将 IPython中的代码保存到程序文件中； %rerun IPython可以指定代码行数重新运行； 12345In [32]: %rerun 21=== Executing: ===json.dumps(d)=== Output: ===Out[32]: '&#123;\"a\": 1, \"b\": 2, \"c\": 3&#125;' （6）与操作系统交互IPython比标准的Python Shell好用的另一个理由是，它能够更好地与操作系统进行交互。在使用 Python进行交互式编程时，不用退出Python Shell就可以执行Linux命令。magic函数里的%cd和%pwd 作用相当于Linux下的cd命令和pwd命令。此外，在IPython中，可以通过“!cmd”的形式执行任何Linux 命令。如下所示： 123456In [33]: %lsanaconda-ks.cfg initial-setup-ks.cfg Python-3.8.1/ Python-3.8.1.tgzIn [34]: %pwdOut[34]: '/root'In [35]: ! wc -l /tmp/storage.log0 /tmp/storage.log 也可以通过赋值的方式捕获命令的输出： 1234567891011121314In [36]: data=!dfIn [37]: dataOut[37]:['文件系统 1K-块 已用 可用 已用% 挂载点','/dev/mapper/centos-root 17811456 6060988 11750468 35% /','devtmpfs 480872 0 480872 0% /dev','tmpfs 497948 0 497948 0% /dev/shm','tmpfs 497948 8696 489252 2% /run','tmpfs 497948 0 497948 0% /sys/fs/cgroup','/dev/sda1 1038336 169504 868832 17% /boot','tmpfs 99592 64 99528 1% /run/user/1000','tmpfs 99592 0 99592 0% /run/user/0']In [38]: data[1].split()[4]Out[38]: '35%' 在Python生态中，除了IPython这个增强的Python Shell以外，还有bython和ptpython这两个不错的 Python Shell。后面这两个工具都有自己的特色，但是都没有IPython使用广泛。而且，由于IPython使 用最为广泛，很多开源项目（如流行的爬虫框架Scrapy）对IPython进行了集成，所以，建议读者学习 IPython。 3.3、 jupyter的使用（1）、jupyter介绍jupyter就是以前的IPython Notebook，是一种新兴的交互式数据分析与记录工具。它通过浏览器访问 本地或者远端的IPython进程，并利用浏览器的图形界面，增强IPython的可视化输出。jupyter定义了 一种全新的文件格式，文件的后缀名是ipynb。ipynb文件包含了代码，用以说明每一步的计算和输出。 也就是说，ipynb文件完整记录了计算过程中的所有相关信息，并且，能够支持图片、视频和公式等副 文本格式，是科学计算、数据分析和编程教学的优秀工具。 正是由于jupyter丰富的可视化输出，其广泛应用于以下场景： 1234编程教学；数据分析；科学计算；幻灯片演示。 （2）、 jupyter notebook的使用IPython Shell与jupyter分离以后，jupyter需要额外进行安装。直接使用pip安装即可： 12345678910111213[root@localhost ~]# pip install jupyterLooking in indexes: https://pypi.douban.com/simple/Collecting jupyterDownloadinghttps://pypi.doubanio.com/packages/83/df/0f5dd132200728a86190397e1ea87cd76244e42d39ec5e88efd25b2abd7e/jupyter-1.0.0-py2.py3-none-any.whl……省略部分信息Successfully installed MarkupSafe-1.1.1 Send2Trash-1.5.0 attrs-19.3.0bleach-3.1.0 defusedxml-0.6.0 entrypoints-0.3 ipykernel-5.1.3 ipywidgets7.5.1 jinja2-2.10.3 jsonschema-3.2.0 jupyter-1.0.0 jupyter-client-5.3.4jupyter-console-6.0.0 jupyter-core-4.6.1 mistune-0.8.4 nbconvert-5.6.1nbformat-5.0.3 notebook-6.0.2 pandocfilters-1.4.2 prometheus-client-0.7.1prompt-toolkit-2.0.10 pyrsistent-0.15.7 python-dateutil-2.8.1 pyzmq-18.1.1qtconsole-4.6.0 terminado-0.8.3 testpath-0.4.4 tornado-6.0.3 webencodings0.5.1 widgetsnbextension-3.5.1 由于我们是在Linux下安装jupyter，如果我们的Linux没有图形界面，可以通过设置–no-browser和设 置–ip=0.0.0.0进行外部访问，如果不指定–ip参数，默认IP是localhost，也就是只有本地才能访问。如 下所示： 12345678910111213141516[root@localhost ~]# jupyter notebook --no-browser --ip=0.0.0.0 --allow-root[I 13:06:33.656 NotebookApp] 启动notebooks 在本地路径: /root[I 13:06:33.657 NotebookApp] 本程序运行在: http://localhost:8888/?token=33fdb8256c7c1d13a23b8189ea0f4e7b7f434f14cc07ea5e[I 13:06:33.657 NotebookApp] or http://127.0.0.1:8888/?token=33fdb8256c7c1d13a23b8189ea0f4e7b7f434f14cc07ea5e[I 13:06:33.657 NotebookApp] 使用control-c停止此服务器并关闭所有内核(两次跳过确认).[C 13:06:33.727 NotebookApp] To access the notebook, open this file in a browser: file:///root/.local/share/jupyter/runtime/nbserver-3998-open.html Or copy and paste one of these URLs: http://localhost:8888/?token=33fdb8256c7c1d13a23b8189ea0f4e7b7f434f14cc07ea5e or http://127.0.0.1:8888/?token=33fdb8256c7c1d13a23b8189ea0f4e7b7f434f14cc07ea5e 从jupyter notebook的输出结果可以看到，jupyter notebook命令给出了一个URL，我们只需将该URL 拷贝至浏览器中，然后将0.0.0.0替换为Linux服务器的IP即可。 在Windows下可以使用远程连接工具xmanager来操作 通过浏览器访问jupyter给我们的URL，就可以登录到jupyter的主界面。这个界面会显示当前目录下的 所有文件 登录jupyter的主界面后，我们如果要创建一个文件，只需要单击“新建”，选择你希望启动的Notebook 类型即可。我们选择Python 3。选择Python 3以后，浏览器会打开一个新的页面。在这个新的页面中， 可以看到一个空的Notebook界面。 jupyter界面由以下部分组成： 1234标题栏菜单栏快捷键编辑区 在菜单栏中有一个“帮助”选项，读者可以通过该选项得到jupyter的使用说明。jupyter本身是图形界面的 应用，使用比较简单，因此，本教程不会花很多篇幅来介绍jupyter的使用。 在jupyter的编辑区中默认有一个输入框。输入框在jupyter中称为cell。我们可以通过菜单栏的“cell”选项 控制cell的格式、执行cell的代码。与此同时，我们也可以通过快捷键控制cell，如ctrl+enter快捷键用以 执行cell中的代码，shift+enter快捷键用以执行当前cell中的代码，并且在当前cell下方创建一个新的 cell。 jupyter之所以能够进行编程教学和幻灯片演示，是因为它可以支持富文本格式和markdown格式。我们 只需修改cell的类型为“Markdown”，就可以在cell中使用markdown语句进行输入了。我们也可以在 jupyter中画图。为了在jupyter中画图，我们需要先安装matplotlib。如下所示： 12345678910111213[root@localhost ~]# pip install matplotlibLooking in indexes: https://pypi.douban.com/simple/Collecting matplotlibDownloadinghttps://pypi.doubanio.com/packages/53/6c/7b400d45f0ecd6703b2779a7dfda6578579a353748e1b43d8353cb7f5b7f/matplotlib-3.1.2-cp38-cp38-manylinux1_x86_64.whl(13.1MB)|████████████████████████████████| 13.1MB 1.8MB/s……省略部分信息Installing collected packages: kiwisolver, numpy, pyparsing, cycler,matplotlibSuccessfully installed cycler-0.10.0 kiwisolver-1.1.0 matplotlib-3.1.2numpy-1.18.1 pyparsing-2.4.6 安装matplotlib以后就可以在jupyter中画图了，下面给出了一个jupyter使用的例子。 12345import matplotlib.pyplot as pltimport numpyx = numpy.arange(11)y = x**2plt.plot(x, y) 运行结果如下图： 四、Python工作环境管理Python 2和Python 3之间存在着较大的差异，并且，由于各种原因导致了Python 2和Python 3的长期 共存。在实际工作过程中，我们可能会同时用到Python 2和Python 3，因此，需要经常在Python 2和 Python 3之间进行来回切换。此外，如果你是喜欢尝鲜的人，那么，你很有可能在Python新版本出来 的时候立即下载Python的版本，试验Python的特性。 在Python世界里，除了需要对Python的版本进行管理以外，还需要对不同的软件包进行管理。大部分 情况下，对于开源的库我们使用***版本即可。但是，有时候可能需要对相同的Python版本，在不同的 项目中使用不同版本的软件包。 在这一节里，我们将介绍两个工具，即pyenv和virtualenv。前者用于管理不同的Python版本，后者用 于管理不同的工作环境。有了这两个工具，Python相关的版本问题将不再是问题。 4.1、使用pyenv管理不同的Python版本安装不同的Python版本并不是一件容易的事情，在不同的Python版本之间来回切换更加困难，而且， 多版本并存非常容易互相干扰。因此，我们需要一个名为pyenv的工具。pyenv是一个Python版本管理 工具，它能够进行全局的Python版本切换，也可以为单个项目提供对应的Python版本。使用pyenv以 后，可以在服务器上安装多个不同的Python版本，也可以安装不同的Python实现。不同Python版本之 间的切换也非常简单。接下来我们就一起看一下pyenv的安装和使用。 1、.pyenv的安装我们直接从GitHub下载项目到本地，然后，分别执行以下命令进行安装即可 12345678910111213141516[root@localhost .pip]# yum -y install git[root@localhost .pip]# git clone https://github.com/yyuu/pyenv.git ～/.pyenv正克隆到 '～/.pyenv'...remote: Enumerating objects: 8, done.remote: Counting objects: 100% (8/8), done.remote: Compressing objects: 100% (8/8), done.remote: Total 17600 (delta 2), reused 2 (delta 0), pack-reused 17592接收对象中: 100% (17600/17600), 3.44 MiB | 601.00 KiB/s, done.处理 delta 中: 100% (11954/11954), done.[root@localhost .pip]# echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt;～/.bash_profile[root@localhost .pip]# echo 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt;～/.bash_profile[root@localhost .pip]# echo 'eval \"$(pyenv init -)\"' &gt;&gt;～/.bash_profile 安装完成以后需要重新载入配置文件，或者退出以后重新登录，以使～/.bash_profile中的配置生效。 笔者一般选择使用source命令重新载入配置文件，如下所示： 1[root@localhost .pip]# source ~/.bash_profile 至此，pyenv就安装完成了，我们可以通过下面的命令验证pyenv是否正确安装并获取pyenv的帮助信 息： 123456789101112131415161718192021222324252627282930[root@localhost ~]# pyenv --helpUsage: pyenv &lt;command&gt; [&lt;args&gt;]Some useful pyenv commands are: commands List all available pyenv commands commands List all available pyenv commands exec Run an executable with the selected Python version global Set or show the global Python version help Display help for a command hooks List hook scripts for a given pyenv command init Configure the shell environment for pyenv install Install a Python version using python-build local Set or show the local application-specific Python version prefix Display prefix for a Python version rehash Rehash pyenv shims (run this after installing executables) root Display the root directory where versions and shims are kept shell Set or show the shell-specific Python version shims List existing pyenv shims uninstall Uninstall a specific Python version version Show the current Python version and its origin --version Display the version of pyenv version-file Detect the file that sets the current pyenv version version-name Show the current Python version version-origin Explain how the current Python version is set versions List all Python versions available to pyenv whence List all Python versions that contain the given executable which Display the full path to an executableSee `pyenv help &lt;command&gt;' for information on a specific command.For full documentation, see: https://github.com/pyenv/pyenv#readme 2、pyenv的使用我们通过pyenv的install命令，可以查看pyenv当前支持哪些Python版本，如下所示 1234567891011121314[root@localhost ~]# pyenv install --listAvailable versions:2.1.3……省略部分信息3.8.03.8-dev3.8.13.9-dev……省略部分信息anaconda3-2018.12anaconda3-2019.03anaconda3-2019.07anaconda3-2019.10……省略部分信息 由于pyenv可以安装的Python版本列表非常长，所以，这里进行了省略。读者可以在自己电脑上安装 pyenv，然后执行pyenv install –list命令进行查看。可以看到，pyenv不但可以安装不同的Python版 本，而且还可以安装不同的Python实现，也可以安装***版本的Python用以学习。 查看当前系统中包含的Python版本： 12[root@localhost ~]# pyenv versions* system (set by /root/.pyenv/version) 使用pyenv安装不同的Python版本： 12pyenv install -v 3.6.0pyenv install -v 2.7.13 再次查看当前系统中包含的Python版本： 1234[root@localhost ~]# pyenv versions* system (set by /root/.pyenv/version)2.7.133.8.1 由于我们安装了2个Python版本，加上我们系统自身的Python，当前系统中存在3个不同的Python版 本。其中，输出结果前面的“*”表示当前正在使用的版本。我们也可以通过pyenv global选择不同的 Python版本，如下所示： 12345678910111213141516[root@localhost ~]# pyenv global 3.8.1[root@localhost ~]# pyenv versionssystem2.7.13* 3.8.1 (set by /root/.pyenv/version)[root@localhost ~]# pythonPython 3.8.1 (default, Jan 14 2020, 12:20:36)[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linuxType \"help\", \"copyright\", \"credits\" or \"license\" for more information.&gt;&gt;&gt; exit()[root@localhost ~]# pyenv global 2.7.13[root@localhost ~]# pythonPython 2.7.13 (default, Jan 14 2020, 12:27:38)[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux2Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.&gt;&gt;&gt; 使用pyenv以后，可以快速切换Python的版本。切换Python版本以后，与版本相关的依赖也会一起切 换。因此，我们不用担心不同的版本在系统中是否会相互干扰。例如，切换Python版本以后，相应的 pip也会跟着切换，所以不用担心自己使用的pip版本和Python版本不匹配的问题，如下所示： 1234[root@localhost ~]# pyenv global 3.8.1[root@localhost ~]# pip --versionpip 19.2.3 from /root/.pyenv/versions/3.8.1/lib/python3.8/site-packages/pip(python 3.8) 如果想要删除Python版本，使用uninstall命令即可。如下所示： 1pyenv uninstall 2.7.10 4.2、 使用virtualenv管理不同的项目virtualenv本身是一个独立的项目，用以隔离不同项目的工作环境。例如，用户lmx希望在项目A中使用 Flask 0.8这个版本，与此同时，又想在项目B中使用Flask 0.9这个版本。如果我们全局安装Flask，必然 无法满足用户的需求。这个时候，我们就可以使用virtualenv。 读者需要注意pyenv和virtualenv的区别。pyenv用以管理不同的Python版本，例如，你的系统工作时 使用Python 2.7.13，学习时使用Python 3.6.0。virtualenv用以隔离项目的工作环境，例如，项目A和 项目B都是使用Python 2.7.13，但是，项目A需要使用Flask 0.8版本，项目B需要使用Flask 0.9版本。我 们只要组合pyenv和virtualenv这两个工具，就能够构造Python和第三方库的任意版本组合，拥有很好 的灵活性，也避免了项目之间的相互干扰。 virtualenv本身是一个独立的工具，用户可以不使用pyenv而单独使用virtualenv。但是，如果你使用了 pyenv，就需要安装pyenv-virtualenv插件，而不是通过virtualenv软件使用virtualenv的功能。 1、pyenv-virtualenv的安装安装和使用pyenv-virtualenv插件如下所示： 123456789[root@localhost ~]# git clone https://github.com/yyuu/pyenv-virtualenv.git$(pyenv root)/plugins/pyenv-virtualenv正克隆到 '/root/.pyenv/plugins/pyenv-virtualenv'...remote: Enumerating objects: 2064, done.remote: Total 2064 (delta 0), reused 0 (delta 0), pack-reused 2064接收对象中: 100% (2064/2064), 580.31 KiB | 264.00 KiB/s, done.处理 delta 中: 100% (1413/1413), done.[root@localhost ~]# echo 'eval \"$(pyenv virtualenv-init -)\"'&gt;&gt;~/.bash_profile 与安装pyenv类似，安装完成以后需要重新载入配置文件，或者退出用户再登录，以使得配置文件生 效： 12345678[root@localhost ~]# source ~/.bash_profile[root@localhost ~]# pyenv help virtualenvUsage: pyenv virtualenv [-f|--force] [VIRTUALENV_OPTIONS] [version]&lt;virtualenv-name&gt; pyenv virtualenv --version pyenv virtualenv --help -f/--force Install even if the version appears to be installedalready 2、pyenv-virtualenv的使用有了pyenv-virtualenv以后，我们可以为同一个Python解释器，创建多个不同的工作环境。例如，我们 新建两个工作环境： 12[root@localhost ~]# pyenv virtualenv 3.8.1 first_project[root@localhost ~]# pyenv virtualenv 3.8.1 second_project 可以使用virtualenvs子命令查看工作环境： 12345[root@localhost ~]# pyenv virtualenvs 3.8.1/envs/first_project (created from /root/.pyenv/versions/3.8.1) 3.8.1/envs/second_project (created from /root/.pyenv/versions/3.8.1) first_project (created from /root/.pyenv/versions/3.8.1) second_project (created from /root/.pyenv/versions/3.8.1) 创建完工作环境以后，可以通过activate和deactivate子命令进入或退出一个工作环境。进入工作环境 以后，左边的提示符会显示你当前所在的工作环境，以免因为环境太多导致操作错误。 123456789101112131415[root@localhost ~]# pyenv activate first_projectpyenv-virtualenv: prompt changing will be removed from future release.configure `export PYENV_VIRTUALENV_DISABLE_PROMPT=1' to simulate thebehavior.(first_project) [root@localhost ~]# pip install flask==1.1.1Looking in indexes: https://pypi.douban.com/simple/Collecting flask Downloadinghttps://pypi.doubanio.com/packages/9b/93/628509b8d5dc749656a9641f4caf13540e2cdec85276964ff8f43bbb1d3b/Flask-1.1.1-py2.py3-none-any.whl (94kB) |████████████████████████████████| 102kB 4.7MB/s……省略部分信息Successfully installed Jinja2-2.10.3 MarkupSafe-1.1.1 Werkzeug-0.16.0click-7.0 flask-1.1.1 itsdangerous-1.1.0(first_project) [root@localhost ~]# pyenv deactivate 接下来，我们看一下在不同的工作环境安装不同的Flask版本： 12345[root@localhost ~]# pyenv activate first_project(first_project) [root@localhost ~]# pip install flask==1.1.1(first_project) [root@localhost ~]# pyenv deactivate(second_project) [root@localhost ~]# pip install flask==0.10.1 如果想要删除虚拟环境，则使用： 1(first_project) [root@localhost ~]# pyenv virtualenv-delete first_project 使用pyenv和python-virtualenv插件，我们就能够自由地在不同的版本之间进行切换，相比管理Python 版本，不但节省了时间，也避免了工作过程中的相互干扰。","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Python数据类型之字典","slug":"Python数据类型之字典","date":"2020-04-26T16:00:00.000Z","updated":"2020-04-27T14:41:05.001Z","comments":true,"path":"Python数据类型之字典.html","link":"","permalink":"https://pdxblog.top/Python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B9%8B%E5%AD%97%E5%85%B8.html","excerpt":"","text":"Python数据类型之字典字典的语法键值对形式，键值之间用“:”分隔，键值对包含在一个”{}“里 12345678910111213#之前的数据类型都是与变量所关联name = 'bily' #字符串类型name = 89 #数字类型key = 88.8list = [3,4,5,6] #列表类型tuple = (6,4,68) #元组#字典的例子#字典存放的数据更大#平常查字典的时候目录和页数是对应的#在python中叫做键和值dict = &#123;'name':'老周','age':'29','job':'程序员'&#125;print(dict)&#123;'name': '老周', 'age': '29', 'job': '程序员'&#125; 键是唯一、无序的 123dict = &#123;'name':'老周','age':'29','job':'程序员','age':'33'&#125;print(dict)&#123;'name': '老周', 'age': '33', 'job': '程序员'&#125; #后面的替换了前面的 键值可以是数字、字符串、元组，一般用于字符串 123dict1 = &#123;1:101,2:102&#125;print(dict1)&#123;1: 101, 2: 102&#125; 简单的字典字典的访问：字典名称[键] 123456#定义字典dict2 = &#123;'河北':'邯郸','甘肃':'兰州','四川':'成都'&#125;#对字典进行访问（取值）===&gt; 值=字典名称[键]hd = dict2['河北']print(hd)邯郸 添加键值对 1234#添加键值对（字典的数据）dict2['山西'] = '太原'print(dict2)&#123;'河北': '邯郸', '甘肃': '兰州', '四川': '成都', '山西': '太原'&#125; 修改字典的值 1234#修改字典的值dict2['河北'] = '邢台'print(dict2)&#123;'河北': '邢台', '甘肃': '兰州', '四川': '成都', '山西': '太原'&#125; 删除键值对：del 1234#删除字典的键值对del dict2['四川']print(dict2)&#123;'河北': '邢台', '甘肃': '兰州', '山西': '太原'&#125; 除了这个横向排序，还可以纵向排序 1234567891011121314151617181920212223student = &#123; 'name':'张三', 'gender':'男', 'age':'20', 'phone':'13813812138'&#125;print(student)&#123;'name': '张三', 'gender': '男', 'age': '20', 'phone': '13813812138'&#125;shell = &#123; 'pwd':'显示当前目录', 'cd':'切换目录', 'mv':'移动文件或目录', 'mkdir':'创建目录', 'cp':'复制文件或目录'&#125;for i in shell: print(i+'：'+shell[i])pwd：显示当前目录cd：切换目录mv：移动文件或目录mkdir：创建目录cp：复制文件或目录 遍历字典遍历所有的键值对： key value items() 123456789101112131415#定义一个字典dict = &#123;'name':'老周','age':'29','job':'程序员'&#125;#遍历字典：键值对的集合、键的集合、值的集合for key,value in dict.items(): print(key+\":\"+value)name:老周age:29job:程序员#字典常用的函数print(dict.items())print(dict.keys())print(dict.values())dict_items([('name', '老周'), ('age', '29'), ('job', '程序员')])dict_keys(['name', 'age', 'job'])dict_values(['老周', '29', '程序员']) 按顺序遍历字典中的所有的键：sorteed() 123456789101112131415#将字典的键排序print(sorted(dict1.keys()))print(sorted(dict1.values()))for info in sorted(dict1.keys()): print(info,end=\" \") print(\"\\n\")for info in sorted(dict1.values()): print(info,end=\" \")['age', 'job', 'name']['29', '程序员', '老周']age job name 29 程序员 老周 遍历字典中所有的值：values() 其他方法（扩展） 计算字典元素个数，即键的总数：len(dict) 123#字典元素的个数print(len(dict))3 返回一个字典的浅复制：dict.copy() 123456#定义一个空字典dict1 = &#123;&#125;#复制字典dict1 = dict.copy()print(dict1)&#123;'name': '老周', 'age': '29', 'job': '程序员'&#125; 返回并删除字典中的最后一对键和值：popitem() 12345678910111213dict = &#123;'name':'老周','age':'29','job':'程序员'&#125;print(dict)print(dict.popitem())print(dict)&#123;'name': '老周', 'age': '29', 'job': '程序员'&#125;('job', '程序员')&#123;'name': '老周', 'age': '29'&#125;#删除指定的值print(dict.pop('age'))print(dict)&#123;'name': '老周', 'age': '29', 'job': '程序员'&#125;29&#123;'name': '老周', 'job': '程序员'&#125; 删除字典内所有元素：dict(clear) 123456dict = &#123;'name':'老周','age':'29','job':'程序员'&#125;print(dict)dict.clear()print(dict)&#123;'name': '老周', 'age': '29', 'job': '程序员'&#125;&#123;&#125; 字典的嵌套列表里嵌套字典 12345678#定义字典dict = &#123;'name':'老周','age':'29','job':'程序员'&#125;dict1 = &#123;'age':'29','job':'程序员'&#125;dict2 = &#123;'name':'老周','job':'程序员'&#125;#列表里嵌套字典list = [dict,dict1,dict2]print(list)[&#123;'name': '老周', 'age': '29', 'job': '程序员'&#125;, &#123;'age': '29', 'job': '程序员'&#125;, &#123;'name': '老周', 'job': '程序员'&#125;] 字典里嵌套列表 123dict3 = &#123;'pet':['cat','dog','duck']&#125;print(dict3&#123;'pet': ['cat', 'dog', 'duck']&#125; 字典里嵌套字典 123456789101112131415dict4 = &#123; 'age':&#123;'girl':'18','boy':'20'&#125;, 'job':&#123;'man':'IT','women':'db'&#125;&#125;print(dict4)for key,value in dict4.items(): print('key:'+key,end=\" \") for v in value.items(): print(v)&#123;'age': &#123;'girl': '18', 'boy': '20'&#125;, 'job': &#123;'man': 'IT', 'women': 'db'&#125;&#125;key:age ('girl', '18')('boy', '20')key:job ('man', 'IT')('women', 'db') 练习1、创建两个字典来表示老师，然后将这两个字典存储到一个名为person的列表中。遍历这个列表，将其中每个老师的信息都打印出来 123456789101112131415teacher1 = &#123; '语文':'张三', '数学':'李四', '英语':'王五',&#125;teacher2 = &#123; '物理':'赵六', '历史':'葛七', '政治':'周八'&#125;person = [teacher1,teacher2]for teacher in person: print(teacher)&#123;'语文': '张三', '数学': '李四', '英语': '王五'&#125;&#123;'物理': '赵六', '历史': '葛七', '政治': '周八'&#125; 2、创建多个字典，每个字典都使用一种宠物的名字命名；在每个字典中，包含宠物的类型和主人的名字。将这些字典存储在一个名为pets的列表中，再遍历该列表，将宠物的信息都打印出来 123456789101112131415161718192021dog = &#123; '小型犬':'贵宾犬', '中型犬':'柯基犬', '大型犬':'金毛犬'&#125;cat = &#123; '小型':'新加坡猫', '中型':'波斯猫', '大型':'沙特尔猫'&#125;pig = &#123; '华北型':'东北民猪', '华南型':'海南猪', '江海型':'太湖猪'&#125;pets = [dog,cat,pig]for pet in pets: print(pet)&#123;'小型犬': '贵宾犬', '中型犬': '柯基犬', '大型犬': '金毛犬'&#125;&#123;'小型': '新加坡猫', '中型': '波斯猫', '大型': '沙特尔猫'&#125;&#123;'华北型': '东北民猪', '华南型': '海南猪', '江海型': '太湖猪'&#125;","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Python函数","slug":"Python函数","date":"2020-03-24T16:00:00.000Z","updated":"2020-03-25T11:49:43.109Z","comments":true,"path":"Python函数.html","link":"","permalink":"https://pdxblog.top/Python%E5%87%BD%E6%95%B0.html","excerpt":"","text":"函数1、为什么要使用函数函数中的代码一次编写，所处运行 函数可以让代码复用，减少代码冗余 2、定义函数关键字：def 函数名称右侧有小括号，结尾处有冒号 函数内第一行通常书写注释，表明该函数的意义 注释后空一行，开始写代码块 函数结束后，空两行 函数调用后空一行，再执行别的代码 函数类型无参函数带参函数 注意事项：调用函数时，实参传递的个数要与形参保持一致","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Python小练习","slug":"Python小练习","date":"2020-03-24T16:00:00.000Z","updated":"2020-03-25T11:49:43.113Z","comments":true,"path":"Python小练习.html","link":"","permalink":"https://pdxblog.top/Python%E5%B0%8F%E7%BB%83%E4%B9%A0.html","excerpt":"","text":"1）猜拳游戏1234567891011121314151617181920212223242526272829303132333435363738'''猜拳游戏：根据用户输入的数字，分别给出提示：“猜大了”或“猜小了”或“猜对了”，只有3次机会，否则退出程序'''import randomnum = random.randint(0, 10)time = 0while time &lt;= 3: guess = int(input('请输入你猜的数字：')) if guess &lt; num: print('猜小了！！！') time += 1 elif guess &gt; num: print('猜大了！！！') time += 1 else: print('恭喜你，猜对了！！！') break print(f'移动三次机会，现在是第&#123;time&#125;次！！！') if time == 3: quiz = input('三次都没对，是否继续（y/n）') if quiz == 'y': time = 0 continue elif quiz == 'n': breakprint(num)# 运行结果请输入你猜的数字：7猜小了！！！移动三次机会，现在是第1次！！！请输入你猜的数字：10恭喜你，猜对了！！！10进程已结束，退出代码 0 2）跑马灯12345678910111213141516171819202122232425262728293031323334353637383940414243'''跑马灯特效'''import osimport timedef main(): content = '武汉加油，中国加油' while True: os.system('cls') print(content) time.sleep(0.2) content = content[1:] + content[0]if __name__ == '__main__': main()#运行结果 武汉加油，中国加油 汉加油，中国加油武 加油，中国加油武汉 油，中国加油武汉加 ，中国加油武汉加油 中国加油武汉加油， 国加油武汉加油，中 加油武汉加油，中国 油武汉加油，中国加 武汉加油，中国加油 汉加油，中国加油武 加油，中国加油武汉 油，中国加油武汉加 ，中国加油武汉加油 中国加油武汉加油， 国加油武汉加油，中 加油武汉加油，中国 油武汉加油，中国加 武汉加油，中国加油 汉加油，中国加油武进程已结束，退出代码 -1 3）幸运数1234567891011121314151617181920212223242526272829303132# 输入一个4位数，如果各个数字之和大于20，则次数为幸运数def lucky_numbers(num): # num = int(input('请输入一个4位数：')) # print('您输入的是：' + str(num)) # 分解四位数，获取各位数字 # 获取个位数字 ge_wei = num % 10 shi_wei = int(num % 100 / 10) bai_wei = int(num / 100 % 10) qian_wei = int(num / 1000) # 求四个数值的和，并进行判断，如果大于20，则输出提示：是幸运数 if (ge_wei + shi_wei + bai_wei + qian_wei) &gt; 20: print('是幸运数字') else: print('不是幸运数字,谢谢参与')def main(): num = int(input('请输入一个4位数：')) print('您输入的是：' + str(num)) lucky_numbers(num)main()#运行结果请输入一个4位数：6666您输入的是：6666是幸运数字进程已结束，退出代码 0 4）温度格式转换123456789101112131415161718192021222324252627282930313233# 使用while实现：输出摄氏温度与华氏温度的对照表，要求它从摄氏温度0度到250度，每隔20度为一项，# 对照表中的条目不超过10条。## 转换关系：华氏温度 = 摄氏温度 * 9 / 5.0 + 32## 循环操作：计算摄氏温度，并输出对照条目# 循环条件：# 条目&lt;=10 &amp;&amp; 摄氏温度 &lt;= 250celsius = 0Fahrenheit = 0i = 0while i &lt; 10 and celsius &lt;= 250: i = i + 1 celsius = celsius + 20 Fahrenheit = celsius * 9 / 5.0 + 32 print('摄氏温度：', celsius, '华氏温度：', Fahrenheit, end=\" \") print()# 运行结果摄氏温度： 20 华氏温度： 68.0 摄氏温度： 40 华氏温度： 104.0 摄氏温度： 60 华氏温度： 140.0 摄氏温度： 80 华氏温度： 176.0 摄氏温度： 100 华氏温度： 212.0 摄氏温度： 120 华氏温度： 248.0 摄氏温度： 140 华氏温度： 284.0 摄氏温度： 160 华氏温度： 320.0 摄氏温度： 180 华氏温度： 356.0 摄氏温度： 200 华氏温度： 392.0 进程已结束，退出代码 0","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Python编写简单的学生管理系统","slug":"Python编写简单的学生管理系统","date":"2020-03-24T16:00:00.000Z","updated":"2020-03-25T11:49:43.118Z","comments":true,"path":"Python编写简单的学生管理系统.html","link":"","permalink":"https://pdxblog.top/Python%E7%BC%96%E5%86%99%E7%AE%80%E5%8D%95%E7%9A%84%E5%AD%A6%E7%94%9F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F.html","excerpt":"","text":"Python编写简单的学生管理系统 一共两个文件，其中一个定义函数，另一个是主程序，调用函数，运行程序 CMS.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485'''编写“学生信息管理系统”，要求如下：必须使用自定义函数，完成对程序的模块化学生信息至少包含：姓名、年龄、学号，除此以外可以适当添加必须完成的功能：添加、删除、修改、查询、退出'''# 定义一个列表用来存储多个学生信息stuList = []# 定义系统菜单显示函数def displayMenu(): # 完成显示系统菜单的功能 print(\"*\" * 40) print(\"学 生 信 息 管 理 系 统 \") print(\"1、添加学生信息\") print(\"2、删除学生信息\") print(\"3、修改学生信息\") print(\"4、查询学生信息\") print(\"5、退出学生信息管理系统\") print(\"*\" * 40)def addNewStu(): # 完成添加学生信息的功能 name = input(\"请输入学生的姓名：\") stuId = input(\"请输入学生的学号：\") age = input(\"请输入学生的年龄：\") # 定义一个字典用来存储每个学生的信息 stuDict = &#123;&#125; stuDict['name'] = name stuDict['stuId'] = stuId stuDict['age'] = age global stuList # 将每个学生的信息添加到列表中 stuList.append(stuDict)def delStu(): global stuList # 完成删除学生信息的功能 delName = input(\"请输入你要删除的学生姓名：\") delFlag = 0 for tempStu in stuList: if delName == tempStu['name']: delName = stuList.index(tempStu) # 获取要删除的学生所在列表中的索引 del stuList[delName] # 按索引删除 delFlag = 1 # 删除成功 break if delFlag == 0: print(\"没有此人，请中心输入！！！\")def reviseStu(): global stuList # 完成修改学生信息的功能 reviseName = input(\"请输入你要修改信息的学生姓名：\") reviseFlag = 0 for tempStuDict in stuList: if reviseName == tempStuDict['name']: # 修改学生的信息 newStuId = input(\"请输入要修改后学生的学号：\") newAge = input(\"请输入要修改后学生的年龄：\") tempStuDict['stuId'] = newStuId tempStuDict['age'] = newAge reviseFlag = 1 break if reviseFlag == 0: print(\"没有此人，请重新输入\")def inquireStu(): global stuList # 完成查询学生信息的功能 inquireName = input(\"请输入你要查询的学生的姓名：\") inquireFlag = 0 for temp in stuList: if inquireName == temp['name']: print(\"%s\\t%s\\t%s\" % (temp['name'], temp['stuId'], temp['age'])) inquireFlag = 1 # 表示查询成功 break if inquireFlag == 0: print(\"查无此人...\") test_student.py 123456789101112131415161718192021222324252627282930313233343536'''测试学生信息管理系统的功能'''import student_sys.CMS as stu# 主函数：程序从这里开始运行def main(): # 菜单显示 # 1、提示用户选择功能 stu.displayMenu() while True: # 2、获取用户的输入 key = int(input(\"请输入你选择的功能序号：\")) if key == 1: stu.addNewStu() elif key == 2: stu.delStu() elif key == 3: stu.reviseStu() elif key == 4: stu.inquireStu() elif key == 5: print('退出程序！！！') return else: print(\"输入有误，请重新输入！！！只能输入1-5的数字！！！\") print(\"\")# 调用主函数，运行程序main() 运行结果 123456789101112131415161718192021****************************************学 生 信 息 管 理 系 统 1、添加学生信息2、删除学生信息3、修改学生信息4、查询学生信息5、退出学生信息管理系统****************************************请输入你选择的功能序号：1请输入学生的姓名：john请输入学生的学号：003请输入学生的年龄：18请输入你选择的功能序号：4请输入你要查询的学生的姓名：johnjohn 003 18请输入你选择的功能序号：5退出程序！！！进程已结束，退出代码 0","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Python之if语句","slug":"Python之if语句","date":"2020-03-12T16:00:00.000Z","updated":"2020-03-13T11:48:20.969Z","comments":true,"path":"Python之if语句.html","link":"","permalink":"https://pdxblog.top/Python%E4%B9%8Bif%E8%AF%AD%E5%8F%A5.html","excerpt":"","text":"Python之if语句应用场景 编程时经常需要检查一列条件并根据此条件决定采取什么措施 选择条件的实例：简单的if结构 12345678910111213#如果天气晴朗，我们去室外散步；否则继续宅在家里'''如果 天气晴朗: 我们去室外散步否则: 继续宅在家里'''state = '晴朗'if state == '晴朗': print('室外散步')else: print('宅在家里')室外散步 条件表达测试布尔表达式 比较运算符：（==、!=、&gt;、&lt;、&gt;=、&lt;=） 逻辑运算符：(and、or) 成员运算符：（in、not in） if语句结构 使用不同的条件做不同的事情 简单的if语句 if-else语句 if-elif-else结构 多重if结构 注意事项：else代码也不是必须的 if语句在列表中的应用 检查元素 确定列表是不空的 使用多个列表 条件表达测试布尔表达式 比较运算符 12345678910111213141516171819202122#比较运算符在条件表达式的应用：ATM/客服电话(请输入1，请输入2....)#==key = 1if key == 1: print('存款')else: print('取款')存款#！=if key != 1: print('不存款')else: print('存款')存款#&gt;=age = 18if age &gt;= 18: print(\"允许进入网吧\")else: print('未成年人禁止进入')允许进入网吧#其他运算符都是一个道理 逻逻辑运算符：and、or 12345678910111213141516171819202122232425#andage = 16money = 50if age &gt;= 18 and money &gt;= 100: print('欢迎光临')else: print('抱歉')抱歉 #and两边的条件必须都得成立#orage = 16money = 100if age &gt;= 18 or money &gt;= 100: print('欢迎光临')else: print('抱歉')欢迎光临 #or两边的条件只需要满足一个就行#age = 16money = 100id_hard = True #布尔值if (age &gt;= 18 or money &gt;= 100) and id_hard: #对于布尔值（True/or）可以省略 print('欢迎光临')else: print('抱歉')欢迎光临 总结and和or的区别： and两边的条件必须都得成立 or两边的条件只需要满足一个就行 成员运算符：in、not in 12345678910111213141516171819202122232425262728293031#定义列表#innames = ['John','Bili','Laoyew']name = 'Kety'if name in names: print('存在')else: print('不存在')不存在#在列表中加一个kety查看效果names = ['John','Bili','Laoyew','kety']name = 'Kety'if name in names: print('存在')else: print('不存在')不存在#因为添加的kety是小写，而定义的变量是大写的Kety，对于这种情况可以采用忽略大小写转换的方法names = ['John','Bili','Laoyew','kety']name = 'Kety'if name.lower() in names: print('存在')else: print('不存在')存在#not inif name.upper() not in names: print('no')else: print('yes')no 条件测试的表达是的结果就是布尔值，要么是True，要么是False，不能用一个等值条件来做 if语句结构 123456789101112131415161718192021222324#简单的ifage = 0if age == 0: print('婴儿')婴儿#if-elseage = 3if age &gt;= 2 and age &lt;= 4: print('蹒跚学步')else: print('婴儿')蹒跚学步#if-elif-elseage = 7if age == 0: print('婴儿')elif age &gt;= 2 and age &lt;= 4: print('蹒跚学步')elif age &gt; 4 and age &lt;= 6: print('上幼儿园')else: print('其他')其他#else代码不是必须的，如果去掉else，是没有输出结果的 如果在符合一个条件之后里面又有一个条件该怎么表示 123456789101112131415#男女学生参加100米赛跑，如果在10秒内跑完的，进入决赛；#进入决赛，分男子组合进行比赛#多重if结构second = 6gender = '男'if second &lt; 10: print('进入决赛') if gender == '男': print('进入男子组') elif gender == '女': print('进入女子组')else: print('重在参与，弘扬体育精神')进入决赛进入男子组 if语句在列表中的应用 12345678910111213141516171819202122for f in fruits: if f == 'pear': print('做个梨罐头') elif f == 'orange': print('做句子罐头') elif f == 'apple': print('做苹果罐头') else: print('做沙拉')做苹果罐头做沙拉做个梨罐头做句子罐头 #最后结果的顺序是列表内的顺序fruits = ['apple','banana','pear','orange']if len(fruits) == 0: print('没有水果')else: print('开始做水果罐头了') for fruit in fruits: print(fruit,end=\" \")开始做水果罐头了apple banana pear orange 练习1、求100以内数字的偶数之和与奇数之和 123456#偶数之和print(sum(range(2,101,2)))#奇数之和print(sum(range(1,101,2)))25502500 2、输出100以内7的倍数的数字 1234for i in range(1,101): if i % 7 == 0: print(i,end=\" \") 7 14 21 28 35 42 49 56 63 70 77 84 91 98 3、打印直角三角形，奇数用*号代替，偶数用#号代替 12345678910111213141516for i in range(1,10): for j in range(i): if j+1 in range(1,11,2): print('*',end=\" \") else: print('#',end=\" \") print()* * # * # * * # * # * # * # * * # * # * # * # * # * # * * # * # * # * # * # * # * # * # *","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Python数据类型之列表的基本操作","slug":"Python数据类型之列表的基本操作","date":"2020-03-12T16:00:00.000Z","updated":"2020-03-13T11:48:20.960Z","comments":true,"path":"Python数据类型之列表的基本操作.html","link":"","permalink":"https://pdxblog.top/Python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B9%8B%E5%88%97%E8%A1%A8%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C.html","excerpt":"","text":"什么是列表列表是Python中非常重要的数据类型，通常作为函数的返回值。由一组元素组成，列表可以实现添加、删除和查找操作，元素值可以被修改 由一系列按照特定的顺序排列起来的元素所组成列表 123456789101112131415161718#普通的变量定义形式tom = \"Tom\"jack = \"Jack\"john = \"John\"pet1 = \"cat\"pet2 = \"dog\"pet3 = \"bird\"#定义列表，语法格式name = ['Tom','Jack','John']pet = ['cat','dog','bird'] #相比一个一个来定义会比较简洁，方便#打印列表print(name)print(pet)['Tom', 'Jack', 'John']['cat', 'dog', 'bird'] 用索引访问列表元素 12345678910#通过索引读取列表中的元素，索引从0开始，-1代表最后一个print(name[0])print(pet[2])Tombird #这些数字就是列表中排列的顺序，是从0开始，按照一定的顺序，以此类推#-1代表元素的最后一个，-2代表倒数第二个，以此类推print(name[-1])print(pet[-2])Johndog 列表的基本操作修改： 通过索引获取元素，进行修改 添加： append(元素)：在列表末尾添加 insert(索引，元素)：在列表指定位置插入 删除： del：根据元素的索引删除元素，被删除的元素不可以在进行访问 pop()：删除列表末尾的元素 pop(索引)：弹出指定位置的元素，被删除的元素还能继续使用 remove(value)：根据元素的值进行删除 排序 sorted(列表)：临时排序 sort()：永久排序 sort(reverse=True)：倒序排序 len()：列表长度 修改列表的元素 1234#通过索引获取元素，进行修改name[1] = 'San'print(name)['Tom', 'San', 'John'] #之前定义的是Jack，现在修改成了San 向列表中添加元素 1234567891011121314#在列表末尾添加新元素name.append('Bob')print(name)['Tom', 'San', 'John', 'Bob']#在列表指定位置添加新元素print(pet)pet.insert(0,'penguin')print(pet)['cat', 'dog', 'bird']['penguin', 'cat', 'dog', 'bird']pet.insert(-2,'pig')print(pet)['penguin', 'cat', 'pig', 'dog', 'bird'] 删除列表中的元素 1234567891011121314151617181920212223242526272829303132333435363738394041#根据元素的索引删除元素print(pet)del pet[0]print(pet)['penguin', 'cat', 'pig', 'dog', 'bird']['cat', 'pig', 'dog', 'bird'] #使用del删除的元素不可以进行访问#删除列表末尾的元素print(pet)new_pet = pet.pop() #不加参数在表最后一个print(new_pet)print(pet)['cat', 'pig', 'dog', 'bird']bird['cat', 'pig', 'dog'] #弹出指定位置的元素print(pet)pet.pop(2)print(pet)['cat', 'pig', 'dog']['cat', 'pig']#访问刚才删除的值print(new_pet)bird#根据元素的值进行删除print(pet)pet.remove('cat')print(pet)['cat', 'pig']['pig']#删除一个不存在值pet.remove('d')print(pet)---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-16-086f691d7e32&gt; in &lt;module&gt; 1 #删除一个不存在值----&gt; 2 pet.remove('d') 3 print(pet)ValueError: list.remove(x): x not in list#删除的值必须是真实存在的，不然就会报错 列表排序 按照26个字母的顺序进行排序 12345678910111213141516171819202122232425#定义列表：汽车的品牌print('原始顺序：')brand = ['audi','bmw','toyota','byd','luhu']print(brand)#临时排序print('临时排序：')print(sorted(brand))print(brand)#永久排序print('永久排序：')brand.sort()print(brand)#倒序排序print('倒序排序：')brand.sort(reverse=True)print(brand)原始顺序：['audi', 'bmw', 'toyota', 'byd', 'luhu']临时排序：['audi', 'bmw', 'byd', 'luhu', 'toyota']['audi', 'bmw', 'toyota', 'byd', 'luhu'] #排完序之后就又变会原来的顺序了永久排序：['audi', 'bmw', 'byd', 'luhu', 'toyota']倒序排序：['toyota', 'luhu', 'byd', 'bmw', 'audi'] 获取列表的长度 123#获取列表的长度print(len(brand))5 常见错误：索引越界 比如说列表长度为5，也就是说索引最多到4 1234567891011#正确索引print(brand[4])audi#错误索引，索引越界print(brand[5])---------------------------------------------------------------------------IndexError Traceback (most recent call last)&lt;ipython-input-10-173eb23c555f&gt; in &lt;module&gt;----&gt; 1 print(brand[5])IndexError: list index out of range #索引错误，列表索引超出范围 练习列表练习（一） 定义一个列表，存储5个科目名称 新增科目（末尾新增&amp;指定位置新增） 修改科目 删除科目，并且在打印科目列表的时候，能够显示删除了哪个科目 删除指定名称的科目 删除第2个科目 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#1、定义一个列表，存储5个科目名称print('定义列表：')subjects = ['语文','数学','英语','物理','历史']print(subjects)#2、新增科目（末尾新增&amp;指定位置新增）#末尾新增print('末尾新增：')print(subjects)subjects.append('化学')print(subjects)#指定位置新增print('指定位置新增：')print(subjects)subjects.insert(0,'生物')print(subjects)#3、修改科目print('修改科目：')print(subjects)subjects[1] = '政治'print(subjects)#4、删除科目，并且在打印科目列表的时候，能够显示产出了哪个科目print('删除科目，并且显示删除了哪个科目：')print(subjects)new_subjects = subjects.pop()print(new_subjects)print(subjects)#5、删除指定名称的科目print('删除指定名称的科目：')print(subjects)subjects.remove('物理')print(subjects)#6、删除第二个科目print('删除第二个科目：')print(subjects)subjects.pop(1)print(subjects)---------------------------------------------------------------------------定义列表：['语文', '数学', '英语', '物理', '历史']末尾新增：['语文', '数学', '英语', '物理', '历史']['语文', '数学', '英语', '物理', '历史', '化学']指定位置新增：['语文', '数学', '英语', '物理', '历史', '化学']['生物', '语文', '数学', '英语', '物理', '历史', '化学']修改科目：['生物', '语文', '数学', '英语', '物理', '历史', '化学']['生物', '政治', '数学', '英语', '物理', '历史', '化学']删除科目，并且显示删除了哪个科目：['生物', '政治', '数学', '英语', '物理', '历史', '化学']化学['生物', '政治', '数学', '英语', '物理', '历史']删除指定名称的科目：['生物', '政治', '数学', '英语', '物理', '历史']['生物', '政治', '数学', '英语', '历史']删除第二个科目：['生物', '政治', '数学', '英语', '历史']['生物', '数学', '英语', '历史'] 列表练习（二） 将5个城市的名称存储到列表中，并且保证名称不是按照字母顺序排列的 打印出原始的城市列表信息 使用sorted()方法按字母顺序打印城市列表，但是不要修改列表元素的顺序 打印该列表，确认城市名称排列顺序没有被修改 使用sort()方法排列城市名称，确保永久性修改排列顺序 12345678910111213141516171819202122232425262728#1、将5个城市的名称存储带列表中，并且保证名称不是按照字母顺序排列的print('定义城市列表：')city = ['zhengzhou','shanghai','beijing','guangzhou','luoyang']#2、打印出来原始的城市列表信息print('原始列表信息：')print(city)#3、使用sorted()方法按字母顺序打印城市列表，但不要修改列表元素的顺序print('sorted()方法排序--临时排序：')print(city)print(sorted(city))#4、打印该列表，确认城市名称排序没有被修改print('验证城市名称排序没有被修改：')print(city)#5、使用sort()方法排序城市名称，确保永久性排列顺序print('使用sort()排序--永久排序：')city.sort()print(city)---------------------------------------------------------------------------定义城市列表：原始列表信息：['zhengzhou', 'shanghai', 'beijing', 'guangzhou', 'luoyang']sorted()方法排序--临时排序：['zhengzhou', 'shanghai', 'beijing', 'guangzhou', 'luoyang']['beijing', 'guangzhou', 'luoyang', 'shanghai', 'zhengzhou']验证城市名称排序没有被修改：['zhengzhou', 'shanghai', 'beijing', 'guangzhou', 'luoyang']使用sort()排序--永久排序：['beijing', 'guangzhou', 'luoyang', 'shanghai', 'zhengzhou']","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Python数据类型之列表的进阶操作","slug":"Python数据类型之列表的进阶操作","date":"2020-03-12T16:00:00.000Z","updated":"2020-03-13T11:48:20.965Z","comments":true,"path":"Python数据类型之列表的进阶操作.html","link":"","permalink":"https://pdxblog.top/Python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B9%8B%E5%88%97%E8%A1%A8%E7%9A%84%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C.html","excerpt":"","text":"Python数据类型之列表的进阶操作遍历 重复性的内容需要按照步骤，分步式的读取出来 对文件内容已经有相似结构的循环读取 for循环 魔法推导式 常见的错误：缩进错误、遗漏冒号 创建数值列表 range(参数1，参数2，参数3)：包头不包尾 ​ 参数1：起始值（包含自己） ​ 参数2：终止值（不包含自己） ​ 参数3：步长（间隔数） 创建数字列表 数字列表的简单统计计算{最大值：max()、最小值：min()、总和：sum() } 切片 把一段数据进行分割 遍历切片 复制列表(把某一个列表中的数据给别的一个，同样的制造一份出来) 元组 元组与列表的区别（列表的值可以修改，元组的值不可修改） 定义元组 修改元组变量 遍历列表123456789#定义一个列表names = [\"张三\",\"李四\",\"王五\",\"赵六\",\"田七\"]zhang_san = names[0]li_si = names[1]wang_wu = names[2]print(zhang_san+\" \"+li_si+\" \"+wang_wu)张三 李四 王五#想要读取某一个值，这样就很麻烦#使用for循环来读取，重复的有规律的读取内容 for循环 123456789for name in names: print(name,end=\" \")张三 李四 王五 赵六 田七#如果是英文的名字，还可以进行一些操作，比如说大小写转换names2 = [\"anlen\",\"bob\"]for name in names2: print(name.title())AnlenBob 魔法推导式 12[name for name in names2]-----------------------------------------------------------------------------------------['anlen', 'bob'] 常见的错误: 缩进错误 1234567names2 = [\"anlen\",\"bob\"]for name in names2:print(name.title()) #print没有缩进 File \"&lt;ipython-input-12-89d8b78a3789&gt;\", line 3 print(name.title()) ^IndentationError: expected an indented bloc #提示缩进错误 遗漏冒号 1234567names2 = [\"anlen\",\"bob\"]for name in names2 #遗漏冒号 print(name.title()) File \"&lt;ipython-input-13-ee6eafbd451b&gt;\", line 2 for name in names2 ^SyntaxError: invalid syntax #提示语法错误 创建数值列表123456#循环输出1到10nums = [1,2,3,4,5,6,7,8,9,10]for num in nums: print(num,end=\" \")1 2 3 4 5 6 7 8 9 10#这样会很麻烦，使用range()就会很方便 range(参数1，参数2，参数3)：包头不包尾 1234567891011121314for num in range(1,11): print(num,end=\" \")1 2 3 4 5 6 7 8 9 10#输出1-10之间的偶数for num in range(2,11,2): #cong2开始每次加2 print(num,end=\" \")#输出1-10之间的奇数for num in range(1,11,2): #从1开始每次加2 print(num,end=\" \")1 3 5 7 9#输出奇数的平方for num in range(1,11,2): print(num**2,end=\" \")1 9 25 49 81 创建数字列表 123numbers = list(range(1,11))print(numbers)[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 数字列表的简单计算 1234567#数字列表的最大值、最小值、总和print(max(numbers))print(min(numbers))print(sum(numbers))10155 切片123#定义一个列表pets = [\"cat\",\"dog\",\"duck\",\"pig\"]print(pets) 把数据进行分割，截取特定的值 1234567891011121314151617181920212223#取开头pets[0] 'cat'#中间截取pets[1:3]['dog', 'duck']#全部截取pets[0:4]['cat', 'dog', 'duck', 'pig']#取最后pets[-1]'pig'#从开头到结尾pets[0:]['cat', 'dog', 'duck', 'pig']#从中间某一个到结尾pets[1:]['dog', 'duck', 'pig'] 遍历切片 123for pet in pets[-3:]: print(pet,end=\" \")dog duck pig 复制列表 1234567games = ['王者','吃鸡','英雄联盟']#friend_games = ['王者','吃鸡'] //这样代码创重复，是不可取的friend_games = games[:2]print('我喜欢的游戏有:'+str(games))print('我朋友喜欢的游戏有:'+str(friend_games)我喜欢的游戏有:['王者', '吃鸡', '英雄联盟']我朋友喜欢的游戏有:['王者', '吃鸡'] 元组123456789101112131415161718192021222324252627#定义列表nums = [1,2,3]nums[0] = 9print(nums)#定义元组numbers = (4,5,6)numbers[0] = 10print(numbers)[9, 2, 3]---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-1-9b5931007d67&gt; in &lt;module&gt; 5 #定义元组 6 numbers = (4,5,6)----&gt; 7 numbers[0] = 10 8 print(numbers[0])TypeError: 'tuple' object does not support item assignment #tuple是只读，而不支持写 #证明元组的值是不能被修改的#但它的访问形式和列表一样print(nums[0])print(numbers[0])94for num in numbers: print(num,end=\" \")4 5 6 给元组变量赋值，打包给它换 12345print(numbers)numbers = (0,1,2)print(numbers)(4, 5, 6)(0, 1, 2) 但是有些地方的值是不允许改变的，也不能改变的，比如（性别），这就可以使用元组 123sex = (\"男\",\"女\")print(sex)('男', '女') #这个值就是固定的，不能被改，也没法该 在日常生活中，要确定内容是否要进行二次修改来决定使用元组，还是列表 练习批量生成50个C类IP–192.168.1.x 12345pre = '192.168.1.'for ip in list(range(1,51)): print(pre+str(ip),end= \" \")192.168.1.1 192.168.1.2 192.168.1.3 192.168.1.4 192.168.1.5 192.168.1.6 192.168.1.7 192.168.1.8 192.168.1.9 192.168.1.10 192.168.1.11 192.168.1.12 192.168.1.13 192.168.1.14 192.168.1.15 192.168.1.16 192.168.1.17 192.168.1.18 192.168.1.19 192.168.1.20 192.168.1.21 192.168.1.22 192.168.1.23 192.168.1.24 192.168.1.25 192.168.1.26 192.168.1.27 192.168.1.28 192.168.1.29 192.168.1.30 192.168.1.31 192.168.1.32 192.168.1.33 192.168.1.34 192.168.1.35 192.168.1.36 192.168.1.37 192.168.1.38 192.168.1.39 192.168.1.40 192.168.1.41 192.168.1.42 192.168.1.43 192.168.1.44 192.168.1.45 192.168.1.46 192.168.1.47 192.168.1.48 192.168.1.49 192.168.1.50 for的双重循环使用 使用口诀： 外层循环控制行数 内层循环控制列数 外层循环执行1次，内层循环执行1轮 打印数字直角三角形 12345678910111213for i in range(1,10): for j in range(i): print(j+1,end=\"\") print()112123123412345123456123456712345678123456789 打印九九乘法表 12345678910111213for i in range(1,10): for j in range(i): print(str(j+1)+\"x\"+str(i)+\"=\"+str(i*(j+1)),end=\" \") print()1x1=1 1x2=2 2x2=4 1x3=3 2x3=6 3x3=9 1x4=4 2x4=8 3x4=12 4x4=16 1x5=5 2x5=10 3x5=15 4x5=20 5x5=25 1x6=6 2x6=12 3x6=18 4x6=24 5x6=30 6x6=36 1x7=7 2x7=14 3x7=21 4x7=28 5x7=35 6x7=42 7x7=49 1x8=8 2x8=16 3x8=24 4x8=32 5x8=40 6x8=48 7x8=56 8x8=64 1x9=9 2x9=18 3x9=27 4x9=36 5x9=45 6x9=54 7x9=63 8x9=72 9x9=81","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Python的变量和简单的数据类型","slug":"Python的变量和简单的数据类型","date":"2020-03-10T16:00:00.000Z","updated":"2020-03-11T09:59:20.859Z","comments":true,"path":"Python的变量和简单的数据类型.html","link":"","permalink":"https://pdxblog.top/Python%E7%9A%84%E5%8F%98%E9%87%8F%E5%92%8C%E7%AE%80%E5%8D%95%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html","excerpt":"","text":"IPython介绍ipython是一个python的交互式shell，比默认的python shell好用得多，支持变量自动补全，自动缩进，支持bash shell命令，内置了许多很有用的功能和函数。学习ipython将会让我们以一种更高的效率来使用python。同时它也是利用Python进行科学计算和交互可视化的一个最佳的平台 IPython提供了两个主要的组件： 一个强大的python交互式shell 供Jupyter notebooks使用的一个Jupyter内核（IPython notebook） IPython的主要功能如下： 运行ipython控制台 使用ipython作为系统shell 使用历史输入(history) Tab补全 使用%run命令运行脚本 使用%timeit命令快速测量时间 使用%pdb命令快速debug 使用pylab进行交互计算 使用IPython Notebook 安装IPython ipython支持Python2.7版本或者3.3以上的版本，我用的是windows下的python 3.8.2版本。 安装ipython很简单，可以直接使用pip管理工具即可: 1C:\\Users\\Admin&gt;pip3 install ipython 下载太慢可以使用国内镜像： 1C:\\Users\\Admin&gt;pip3 install -i https://pypi.douban.com/simple ipython 查看当前库 1234567891011121314151617181920C:\\Users\\Admin&gt;pip3 listPackage Version---------------- -------backcall 0.1.0colorama 0.4.3decorator 4.4.2ipython 7.13.0ipython-genutils 0.2.0jedi 0.16.0parso 0.6.2pickleshare 0.7.5pip 19.2.3prompt-toolkit 3.0.4Pygments 2.6.1setuptools 41.2.0six 1.14.0traitlets 4.3.3wcwidth 0.1.8WARNING: You are using pip version 19.2.3, however version 20.0.2 is available.You should consider upgrading via the 'python -m pip install --upgrade pip' command. 根据提示更新下载源 1C:\\Users\\Admin&gt;python3 -m pip install --upgrade pip 更显失败的利用豆瓣镜像源更新 1C:\\Users\\Admin&gt;python -m pip install --upgrade pip -i https://pypi.douban.com/simple 交互式使用IPython IPython支持所有python的标准输入输出，也就是我们在IDLE中或者Python shell中能用的，在IPython中都能够使用，唯一的不同之处使ipython会使用In [x]和Out [x]表示输入输出，并表示出相应的序号 12345678910#在cmd窗口输出ipython即可C:\\Users\\Admin&gt;ipythonPython 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AMD64)]Type 'copyright', 'credits' or 'license' for more informationIPython 7.13.0 -- An enhanced Interactive Python. Type '?' for help.In [1]: print('hello python!')hello python!#退出In [2]: exit Jupyter Notebook介绍Jupyter Notebook是基于网页的用于交互计算的应用程序。其可被应用于全过程计算：开发、文档编写、运行代码和展示结果。——Jupyter Notebook官方介绍 简而言之，Jupyter Notebook是以网页的形式打开，可以在网页页面中直接编写代码和运行代码，代码的运行结果也会直接在代码块下显示。如在编程过程中需要编写说明文档，可在同一个页面中直接编写，便于作及时的说明和解释 组成部分 1、网页应用 网页应用即基于网页形式的、结合了编写说明文档、数学公式、交互计算和其他富媒体形式的工具。简言之，网页应用是可以实现各种功能的工具 2、文档 即Jupyter Notebook中所有交互计算、编写说明文档、数学公式、图片以及其他富媒体形式的输入和输出，都是以文档的形式体现的。 这些文档是保存为后缀名为.ipynb的JSON格式文件，不仅便于版本控制，也方便与他人共享。 此外，文档还可以导出为：HTML、LaTeX、PDF等格 Jupyter Notebook的主要特点 编程时具有语法高亮、缩进、tab补全的功能。 可直接通过浏览器运行代码，同时在代码块下方展示运行结果。 以富媒体格式展示计算结果。富媒体格式包括：HTML，LaTeX，PNG，SVG等。 对代码编写说明文档或语句时，支持Markdown语法。 支持使用LaTeX编写数学性说明 安装Jupyter Notebook 安装前提 安装Jupyter Notebook的前提是需要安装了Python（3.3版本及以上，或2.7版本 使用pip命令安装 1C:\\Users\\Admin&gt;pip3 install -i https://pypi.douban.com/simple jupyter 运行Jupyter Notebook 1C:\\Users\\Admin&gt;jupyter notebook 执行命令之后，在终端中将会显示一系列notebook的服务器信息，同时浏览器将会自动启动Jupyter Notebook 启动过程中终端显示内容如下： 123456789101112[I 19:14:50.207 NotebookApp] Serving notebooks from local directory: F:\\python1\\1[I 19:14:50.208 NotebookApp] The Jupyter Notebook is running at:[I 19:14:50.208 NotebookApp] http://localhost:8888/?token=c3ac105eef676358b314723d6d56a53ca773fac409c6fac8[I 19:14:50.208 NotebookApp] or http://127.0.0.1:8888/?token=c3ac105eef676358b314723d6d56a53ca773fac409c6fac8[I 19:14:50.208 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[C 19:14:50.227 NotebookApp] To access the notebook, open this file in a browser: file:///C:/Users/Admin/AppData/Roaming/jupyter/runtime/nbserver-6444-open.html Or copy and paste one of these URLs: http://localhost:8888/?token=c3ac105eef676358b314723d6d56a53ca773fac409c6fac8 or http://127.0.0.1:8888/?token=c3ac105eef676358b314723d6d56a53ca773fac409c6fac8 注意：之后在Jupyter Notebook的所有操作，都请保持终端不要关闭，因为一旦关闭终端，就会断开与本地服务器的链接，你将无法在Jupyter Notebook中进行其他操作啦 如果你同时启动了多个Jupyter Notebook，由于默认端口“8888”被占用，因此地址栏中的数字将从“8888”起，每多启动一个Jupyter Notebook数字就加1，如“8889”、“8890”…… 启动服务器不打开浏览器 如果你只是想启动Jupyter Notebook的服务器但不打算立刻进入到主页面，那么就无需立刻启动浏览器。在终端中输入 1F:\\python1\\1&gt;jupyter notebook --no-browser 此时，将会在终端显示启动的服务器信息，并在服务器启动之后，显示出打开浏览器页面的链接。当你需要启动浏览器页面时，只需要复制链接，并粘贴在浏览器的地址栏中，轻按回车变转到了你的Jupyter Notebook页面 12345678910111213F:\\python1\\1&gt;jupyter notebook --no-browser[I 19:17:51.674 NotebookApp] Serving notebooks from local directory: F:\\python1\\1[I 19:17:51.675 NotebookApp] The Jupyter Notebook is running at:[I 19:17:51.675 NotebookApp] http://localhost:8888/?token=703a6064cab58c5fe00f63133ee3dd55573fb09cf6fae2d4[I 19:17:51.675 NotebookApp] or http://127.0.0.1:8888/?token=703a6064cab58c5fe00f63133ee3dd55573fb09cf6fae2d4[I 19:17:51.675 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[C 19:17:51.678 NotebookApp] To access the notebook, open this file in a browser: file:///C:/Users/Admin/AppData/Roaming/jupyter/runtime/nbserver-7204-open.html Or copy and paste one of these URLs: http://localhost:8888/?token=703a6064cab58c5fe00f63133ee3dd55573fb09cf6fae2d4 or http://127.0.0.1:8888/?token=703a6064cab58c5fe00f63133ee3dd55573fb09cf6fae2d4 浏览器输入：http://localhost:8888就可以访问 根据终端的提示输入token 123Or copy and paste one of these URLs: http://localhost:8888/?token=703a6064cab58c5fe00f63133ee3dd55573fb09cf6fae2d4 or http://127.0.0.1:8888/?token=703a6064cab58c5fe00f63133ee3dd55573fb09cf6fae2d4 这个目录取决于你在哪个目录下启动的 安装完成后就开始使用什么是变量： 简单理解来说就是，起一个名字用等于号给它赋予一个值，这就是变量（固定不变的量叫做常量） 1234567891011121314151617#定义变量sum1 = 0 #定义变量2sum2 = 1 #定义变量2print(sum1) #输出定义的变量10print(sum2) #输出定义的变量21#也可以一次定义多个变量a,b,c = 1,2,3print(a)1print(b)2print(c)3print(a,b,c)1 2 3 变量的用法有什么好处： 在某种程度上可以实现代码的复用 123456789101112131415161718192021222324252627#需要很多地方都用到这个数据，而且这些数据还会存放在计算机内，占用内存print('北京欢迎你')print('北京欢迎你')print('北京欢迎你')print('北京欢迎你')print('北京欢迎你')print('北京欢迎你')北京欢迎你北京欢迎你北京欢迎你北京欢迎你北京欢迎你北京欢迎你#定义一个变量message=\"北京欢迎你\"，message就比北京欢迎你占的字符小，占用内存也小message = \"北京欢迎你\"print(message)print(message)print(message)print(message)print(message)print(message)北京欢迎你北京欢迎你北京欢迎你北京欢迎你北京欢迎你北京欢迎你 变量的命令规则： 只能包换字母、数字、下划线，但是不能以数字打头 不能包含空格，但是可以用下划线分隔其中的单词 不能使用Python关键字和函数的名称用作变量，比如：import、class、and、return、def、from …..等等 变量名应该简短并且见名之意 慎用小写字母 i 和大写字母 O（容易和数字混淆的字母） 只能包换字母、数字、下划线，但是不能以数字打头 1234567891011121314#错误例子$a=89$a File \"&lt;ipython-input-6-cf2d0c5b3bb7&gt;\", line 1 $a=89 ^SyntaxError: invalid #非法的输入#数字开头也不行1a=891a File \"&lt;ipython-input-7-e78e6d530639&gt;\", line 1 1a=89 ^SyntaxError: invalid syntax 不能包含空格，但是可以用下划线分隔其中的单词 1234567891011#包换空格示例a b=89a b File \"&lt;ipython-input-9-1917d5b6e6ca&gt;\", line 1 a b=89 ^SyntaxError: invalid syntax#以下划线分隔a_b=89a_b89 不能使用Python关键字和函数的名称用作变量 比如：import、class、and、return、def、from …..等等 1234567891011import = 'ere' File \"&lt;ipython-input-11-58ece2b8d285&gt;\", line 1 import = 'ere' ^SyntaxError: invalid syntaxand = \"ddd\" File \"&lt;ipython-input-13-9498d9360549&gt;\", line 1 and = \"ddd\" ^SyntaxError: invalid syntax#只要变量名称是绿色或者是红色的就不能使用 变量名应该简短并且见名之意 12345num1=1num2=2teacher_name = \"苏珊\" #老师的名字student_name = \"吉米\" #学生的名字lengh_of_student = 2 #学生名字的长度 慎用小写字母 i 和大写字母 O 1234l=1 #小写L=数字1O=0 #大写O=数字0i=1 #小写I=数字1#这些容以与数字混淆的慎用 使用常见的错误： 变量未定义就使用 12345678print=(num)---------------------------------------------------------------------------NameError Traceback (most recent call last)&lt;ipython-input-15-f827eafea48a&gt; in &lt;module&gt;----&gt; 1 print=(num)NameError: name 'num' is not defined#变量要定义后再进行使用 Python常用的数据类型1、字符串 Python中的字符串类型是一组包含数字、字母和符号的集合，作为一个整体使用 字符串的表达形式 在Python中有三种表示字符串的方式，单引号、双引号、三引号 123456789101112131415161718str1 = 'hello'str2 = \"world\"str3 = '''春眠不觉晓处处闻啼鸟夜来风雨声花落知多少'''print(str1)print(str2)print(str3)helloworld春眠不觉晓处处闻啼鸟夜来风雨声花落知多少 需要注意的是，单引号和双引号的作用是一样的，可以根据习惯使用，但是定义多行文字时，必须要使用三引号 使用字符串的注意事项 字符串的定义方式单引号、双引号、三引号大部分情况下作用是相同的，但在特殊情况下使用也有所区别，下面是需要注意的地方 单引号、双引号、三引号它们是成对出现的，如以单引号开头就要以单引号结尾，不能混合使用表示字符串 如果字符串中单独出现单引号或双引号，可以使用另一种引号定义 当字符串中出现单引号、双引号等特殊字符时，还可以使用转义字符定义。Python中的转移字符是“\\”，只要在特殊字符前面加上“\\”，就可以原样输出，而不用去管定义字符串使用的是单引号还是双引号 字符串的常用方法 修改字符串大小写（title、upper、lower） 拼接字符串（+、函数str() ） 添加空白（空格、换行符：\\n、水平制表位：\\t、end=’’：不进行换行直接连起来） 删除空白(开头：lstrip()、末尾：retrip()、两端：strip() ) 修改字符串大小写 1234567name = \"ad loveLace\"print(name.title()) #首字母大写print(name.upper()) #全部大写print(name.lower()) #全部小写Ad LovelaceAD LOVELACEad lovelace 拼接字符串 1234#将name与要定义的age拼接起来age = 18print('我叫'+name+',今年'+str(age)+'岁')我叫ad loveLace,今年18岁 PS：print输出的默认是字符串，而定义的age的值是数字，所以需要函数str() 将数字转化为字符串类型，不然会报错（TypeError: can only concatenate str (not &quot;int&quot;) to str） 添加空白 123456789101112131415161718192021222324#让18岁之间有空格age = 18print('我叫'+name+',今年 '+str(age)+' 岁')我叫ad loveLace,今年 18 岁#换行符：\\nage = 18print('我叫'+name+',\\n今年 '+str(age)+' 岁')我叫ad loveLace,今年 18 岁#水平制表为\\tage = 18print('我叫\\t'+name+',\\n今年 '+str(age)+' 岁')我叫 ad loveLace,今年 18 岁#end=''//先来看看不是使用end=''的效果print('我叫'+name+',')print('今年 '+str(age)+' 岁')我叫ad loveLace,今年 18 岁 #输出为了两行//使用end=''将两行连起来print('我叫'+name+',',end='')print('今年 '+str(age)+' 岁')我叫ad loveLace,今年 18 岁 删除空白 比如说在登录帐户的时候，一不小心手一抖多了个空格，但是是不允许有空格的，这时候就用到了删除空白 12345678910#定义变量，值的坐中又均有空格content = \" test demon \"print(content) #没有改变的情况print(content.lstrip()) #去掉左边的空格print(content.rstrip()) #去掉右边的空格print(content.strip()) #去掉两边的空格 test demontest demon test demontest demon 常见的错误 单引号里面含撇号 12345say = 'Let'go!' File \"&lt;ipython-input-28-5ccc2ed9867a&gt;\", line 1 say = 'Let'go!' ^SyntaxError: invalid syntax 如果引号里面需要用到撇号或着双引号，可以使用不用的引号引用 123456789101112#双引号里套撇号say_content = \"Lest'go!\"print(say_content)Lest'go!#三引号里套撇号s= '''Let'go!''' #这个就有点大材小用了，尽量别这样print(s)Let'go!#单引号里套双引号d = '\"This is my book!\"'print(d)\"This is my book!\" 2、数字 数字类型包括整型、浮点型、布尔型等，声明时由Python内置的基本数据类型来管理变量，在程序的后台实现数值与类型的关联，以及转换等操作。根据变量的值自动判断变量的类型，我们无需关心变量空间是什么类型，只要知道创建的变量中存放了一个数，程序只是对这个数值进行操作 123456789101112num1 = 8num2 = 0.5print(num1+num2)print(num1-num2)print(num1*num2)print(num1/num2)print(num1%num28.57.54.016.00.0 注释（#号和三引号） 12345678910111213141516'''以下代码是关于Python数据类型的演示案例'''#Python中的数字类型num1 = 8num2 = 0.5print(num1+num2)print(num1-num2)print(num1*num2)print(num1/num2)print(num1%num2)8.57.54.016.00.0 小练习1、将用户的姓名存到一个变量中，并向该用户显示一条信息，显示内容为：“你好，艾瑞克，今天的Python课你学到东西了吗？” 123name = \"艾瑞克\"print('你好，'+name+',今天的Python课你学到东西了吗')你好，艾瑞克,今天的Python课你学到东西了吗 2、将一个人的名字存到变量中，再以小写、大写和首字母大写的方式显示这个人的名字 1234567noe_name = \"mike\"print(noe_name.lower())print(noe_name.upper())print(noe_name.title())mikeMIKEMike 3、按一下格式打印诗词： 《自由》 为人进出的门紧锁着; 想死的门敞开着。 有个病毒在外面高喊着: “出来玩吧，给你自由！” 但我深深地知道…… 出去了，不一定还能回来。 人的生命只有一次， 算球了， 再关十几天就自由了！ 1234567891011121314151617181920212223poem = '''《自由》 为人进出的门紧锁着;想死的门敞开着。有个病毒在外面高喊着:“出来玩吧，给你自由！”但我深深地知道……出去了，不一定还能回来。人的生命只有一次，算球了，再关十几天就自由了！'''print(poem)《自由》 为人进出的门紧锁着;想死的门敞开着。有个病毒在外面高喊着:“出来玩吧，给你自由！”但我深深地知道……出去了，不一定还能回来。人的生命只有一次，算球了，再关十几天就自由了","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Jenkins+GitLba针对k8s集群持续集成","slug":"Jenkins+GitLba针对k8s集群持续集成","date":"2020-03-08T16:00:00.000Z","updated":"2020-03-09T07:17:32.856Z","comments":true,"path":"Jenkins+GitLba针对k8s集群持续集成.html","link":"","permalink":"https://pdxblog.top/Jenkins+GitLba%E9%92%88%E5%AF%B9k8s%E9%9B%86%E7%BE%A4%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90.html","excerpt":"","text":"Jenkins+GitLba针对k8s集群持续集成持续集成概念 持续集成Continuous Integration ​ 持续集成是指开发者在代码的开发过程中，可以频繁的将代码部署集成到主干，并进程自动化测试 持续交付Continuous Delivery ​ 持续交付指的是在持续集成的环境基础之上，将代码部署到预生产环境 持续部署Continuous Deployment ​ 在持续交付的基础上，把部署到生产环境的过程自动化，持续部署和持续交付的区别就是最终部署到生产环境是自动化的 环境准备 k8s集群环境，三台服务器，IP分别为：192.168.1.70（master）、50（node01）、40（node02） Jenkins+GitLab部署在一台Docker服务器上（192.168.1.30），主要用于向仓库上传私有镜像 环境一共4台服务器，全部指向一个私有仓库，共享Docker镜像 实验所用到的软件都可以在网盘下载： 下载地址 提取码：trv8 验证k8s集群没有问题 12345[root@master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSIONmaster Ready master 64d v1.15.0node01 Ready &lt;none&gt; 64d v1.15.0node02 Ready &lt;none&gt; 64d v1.15.0 首先我们需要做一个registry私有仓库，可以选择任意一台服务器都可以，这里我们选择kubernetes-master作为registry私有仓库 Harbor也是可以的 123456789[root@master ~]# docker run -d --restart=always -p 5000:5000 registry:2[root@master ~]# vim /usr/lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.70:5000[root@master ~]# systemctl daemon-reload [root@master ~]# systemctl restart docker#将nginx镜像上传到私有仓库[root@master ~]# docker pull nginx[root@master ~]# docker tag nginx:latest 192.168.1.70:5000/nginx:v1[root@master ~]# docker push 192.168.1.70:5000/nginx:v1 node01、node02加入私有仓库，并拉去镜像 123456789#注意实在那台机器上操作[root@master ~]# scp /usr/lib/systemd/system/docker.service node01:/usr/lib/systemd/system/docker.service [root@master ~]# scp /usr/lib/systemd/system/docker.service node02:/usr/lib/systemd/system[root@node01 ~]# systemctl daemon-reload [root@node01 ~]# systemctl restart docker[root@node02 ~]# systemctl daemon-reload [root@node02 ~]# systemctl restart docker[root@node01 ~]# docker pull 192.168.1.70:5000/nginx:v1[root@node02 ~]# docker pull 192.168.1.70:5000/nginx:v1 建立yaml配置文件让k8s自己控制容器集群，用来模拟我们部署的服务 123456789101112131415161718192021222324252627[root@master ~]# vim nginx.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginxspec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: 192.168.1.70:5000/nginx:v1 imagePullPolicy: Always ports: - containerPort: 80[root@master ~]# kubectl apply -f nginx.yaml deployment.extensions/nginx created[root@master ~]# kubectl get deployments.NAME READY UP-TO-DATE AVAILABLE AGEnginx 2/2 2 2 61s[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEnginx-57b9fd468d-dn2v7 1/1 Running 0 29snginx-57b9fd468d-q5qm6 1/1 Running 0 29s 容器的ip只能在容器本机上访问，集群内的其他主机和集群外的主机都没办法访问，这个时候就需要将容器的端口映射到服务器上的端口了，所以需要做一个service的模板。service 模板可以将容器的端口映射到服务器的端口上，并且可以固定映射在服务器上的端口 12345678910111213141516171819[root@master ~]# vim nginx-svc.yamlapiVersion: v1kind: Servicemetadata: name: nginxspec: type: NodePort ports: - port: 80 targetPort: 80 nodePort: 31234 selector: name: nginx[root@master ~]# kubectl apply -f nginx-svc.yaml service/nginx created[root@master ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 64dnginx NodePort 10.105.100.181 &lt;none&gt; 80:31234/TCP 6s 浏览器访问测试：192.168.1.70:31234 kubernetes完毕，开始配hijenkins+gitlab联动 基本环境准备 12345678910111213[root@localhost ~]# hostnamectl set-hostname autoweb[root@localhost ~]# systemctl disable firewalld[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# setenforce 0setenforce: SELinux is disabled[root@localhost ~]# vim /etc/selinux/configSELINUX=disabled[root@localhost ~]# bash#加入私有仓库[root@autoweb ~]# vim /usr/lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.70:5000[root@autoweb ~]# systemctl daemon-reload[root@autoweb ~]# systemctl restart docker 首先安装jenkins 1234567891011121314151617#配置java环境[root@autoweb ~]# tar zxf jdk-8u231-linux-x64.tar.gz [root@autoweb ~]# mv jdk1.8.0_231 /usr/java #注意这里的位置，不要多一个“/”#配置环境变量[root@autoweb ~]# vim /etc/profile#在最后一行添加以下内容export JAVA_HOME=/usr/javaexport JRE_HOME=/usr/java/jreexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATHexport CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar#使环境变量生效[root@autoweb ~]# source /etc/profile#验证环境变量[root@autoweb ~]# java -versionjava version \"1.8.0_231\"Java(TM) SE Runtime Environment (build 1.8.0_231-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode) 部署tomcat，将jenkins的包放进tomcat里 123456789101112131415161718192021[root@autoweb ~]# tar zxf apache-tomcat-7.0.54.tar.gz [root@autoweb ~]# mv apache-tomcat-7.0.54 /usr/tomcat7[root@autoweb ~]# cd /usr/tomcat7/webapps/[root@autoweb webapps]# rm -rf *[root@autoweb webapps]# cp /root/jenkins.war .#修改tomcat的字符集[root@autoweb webapps]# vim /usr/tomcat7/conf/server.xml#在72行左右追加 &lt;Connector port=\"8080\" protocol=\"HTTP/1.1\" connectionTimeout=\"20000\" redirectPort=\"8443\" URIEncoding=\"UTF-8\" /&gt;#修改启动脚本，添加jenkins的家目录，这个很重要[root@autoweb webapps]# cd /usr/tomcat7/bin/[root@autoweb bin]# vim catalina.sh#!/bin/shexport CATALINA_OPTS=\"-DJENKINS_HOME=/data/jenkins\"export JENKINS_JAVA_OPTIONS=\"-Djava.awt.headless=true -Dhudson.ClassicPluginStrategy.noBytecodeTransformer=true\"#启动tomcat[root@autoweb bin]# ./catalina.sh start[root@autoweb bin]# netstat -anput | grep 8080tcp6 0 0 :::8080 :::* LISTEN 6266/java 浏览器访问：192.168.1.30:8080/jenkins开始配置安装jenkins 根据提示查看密码并输入 12[root@autoweb bin]# cat /data/jenkins/secrets/initialAdminPassword7ea904846c9c4cfbb19d3e31d22f889f 左边是自动安装， 右边是自定义安装，如果不是这个画面则说明网络很卡或者没有网，网速可以的就选择左边的 由于网络问题，下载插件会非常慢，这里我就不下载，回到上一步，断网之后再点继续，跳过插件安装 断网之后，它会等待网络连接，过程有点慢，但是比自动下载插件快多了 在创建用户名密码的时候就可以联网了 然后一路默认就行 因为没有安装，所以就导入插件 插件存放目录：/data/jenkins/plugins 1234567#将原来的插件目录删除[root@autoweb jenkins]# rm -rf plugins/#导入下载好的插件，并解压[root@autoweb jenkins]# tar zxf plugins.tar.gz#重新运行jenkins，让它自动识别新导入的插件[root@autoweb bin]# ./catalina.sh stop[root@autoweb bin]# ./catalina.sh start 使用浏览器重新访问 因为很多插件需要翻墙才可以继续下载，Jenkins还提供了代理的设置 设置插件的国内下载地址 在Jenkins插件管理-高级设置界面，定位到页面最底部中的【升级站点】模块，将对应URL输入框中的url的https修改为http，即http://updates.jenkins.io/update-center.json，然后点击【提交】保存修改项 往下拉，找到插件管理 点击高级 还需要安装三个插件 因为在我导入的插件里就有这三个，所以不需要安装，如果选择推荐安装的可以搜索并安装这三个插件 点击可选插件，搜索GitLab就能找到，然后直接安装即可（搜索的时候注意大小写区分） jenkis安装完成以后，再去安装gitlab 12345[root@autoweb ~]# yum -y install curl policycoreutils openssh-server openssh-clients postfix git[root@autoweb ~]# systemctl enable sshd[root@autoweb ~]# systemctl start sshd[root@autoweb ~]# systemctl enable postfix[root@autoweb ~]# systemctl start postfix 安装gitlab-ce 1[root@autoweb ~]# curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash 因为网络问题，就不使用这种方式，国内的用户，使用清华大学的镜像进行安装 12345678910111213#编写yum源[root@autoweb ~]# vim /etc/yum.repos.d/gitlab-ce.repo[gitlab-ce]name=gitlab-cebaseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7repo_gpgcheck=0gpgcheck=0enabled=1gpgkey=https://packages.gitlab.com/gpg.key#将gitlab源加入yum[root@autoweb ~]# yum makecache #下载安装gitlab，这个软件包有点大，需要点时间[root@autoweb ~]# yum -y install gitlab-ce 修改端口防止端口冲突，默认是80端口，unicorn默认是8080 也是tomcat的端口 12345[root@autoweb ~]# vim /etc/gitlab/gitlab.rb#在29行左右的地方进行修改和追加external_url 'http://192.168.1.30:90'unicorn['listen'] = '127.0.0.1'unicorn['port'] = 3000 启动gitlab，这个过程可能会有点慢 1[root@autoweb ~]# gitlab-ctl reconfigure 浏览器访问：192.168.1.30:90 设置密码登录gitlab 默认用户名是root jenkins：工具集成平台 gitlab：软件托管平台 部署这两个服务的联动，需要经过ssh验证 首先我们需要在gitlab上绑定jenkins服务器的ssh公钥，因为是在同一台服务器上，所以就自己给自己绑定，这里我们使用的是root用户的公私钥，切记生产环境是不允许随便用root的 1234[root@autoweb ~]# ssh-keygen -t rsa #一路回车即可#复制公钥[root@autoweb ~]# cat /root/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0PGtK/uFWejdLYfEqEGiimWrpndlkMZMNyvMisdCvWYU2E2UKxpr/h/QCZQpgpe7uAPZpivnsu3XZ4pE4qe7WG4iKEqF6oVmJL9JjmNh86vOUDGttOU3aZJnLp95yoYczBUSObNsSAzkHIMR2u0Zk5nFW2Roe2FHNUyj0e2qM/Zm5+M5EVgpHd9UG5A4Z/Loid/got6Xaufoen1hSFY+S18QPuheN1auaTfnyA0wwf+rlWjqwCLJeUBl0PgOgNNyZ27++U6QBUPNwgC3SVdhbJMCFE9/H2+0aMnjVqnakfJmt95UI/QOJAmnrusps/XPbqdnxZ5u89i97QxuogAgL root@autoweb 在gitlab上导入公钥 新建一个代码仓库 输入一个仓库的名字 权限选择公共的（public）然后直接点击创建 点击新建一个new.file 写入代码，起一个名字然后保存 将项目克隆到本地测试是否可用 123456[root@autoweb ~]# git clone git@192.168.1.30:root/test.git[root@autoweb ~]# cd test/[root@autoweb test]# lsindex.html[root@autoweb test]# cat index.html print: \"hello world!!!\" 测试没有问题 打开jenkins，新建任务 地址粘贴进去后没有报错则没错 下面的这个插件很重要，就是他实现自动化更新的webhook插件，安装过了就会有这条，然后点击这条下面出来的这些东西保持默认就行。同时注意复制 这个里面写的是jenkins构建时候会执行的shell脚本，这个是最重要的，就是他实现了下端kubernetes自动更新容器的操作 代码内容 123456789101112#!/bin/bashbackupcode=\"/data/backcode/$JOB_NAME/$BUILD_NUMBER\"mkdir -p $backupcodechmod 644 \"$JENKINS_HOME\"/workspace/\"$JOB_NAME\"/*rsync -acP \"$JENKINS_HOME\"/workspace/\"$JOB_NAME\"/* $backupcodeecho From 192.168.1.70:5000/nginx:v1 &gt; \"$JENKINS_HOME\"/workspace/Dockerfileecho COPY ./\"$JOB_NAME\"/* /usr/share/nginx/html/ &gt;&gt; \"$JENKINS_HOME\"/workspace/Dockerfiledocker rmi 192.168.1.70:5000/nginx:v1docker build -t 192.168.1.70:5000/nginx:v1 /\"$JENKINS_HOME\"/workspace/.docker push 192.168.1.70:5000/nginx:v1ssh root@192.168.1.70 kubectl delete deployment nginxssh root@192.168.1.70 kubectl apply -f /root/nginx.yaml 复制地址去gitlab上绑定webhook 保存，登录gitlab，点击下图这个设置 将复制的地址粘贴 往下拉，去掉ssh验证，添加webhook 出现报错，提示本地连接不了，因为gitlab默认设置不允许想自己发送web hook 解决办法 保存之后重试，成功的如下图所示 测试jenkins与gitlab连通) 403报错的，解决办法 回到Jenkins开启匿名访问权限 点击全局安全配置 保存之后再点击系统设置去掉勾选 保存之后，回到gitlab上再次测试 出现蓝条说明jenkins已经连通gitlab jenkins和gitlab 都已经互相的ssh通过了，然后我们最后需要做的一个ssh是关于jenkins 这里是从autoweb向master节点做免密登录 1[root@autoweb ~]# ssh-copy-id root@192.168.1.70 环境全部部署完毕！！！开始测试 立即构建 构建完成之后查看网页的变化 回到Gitlab更新代码测试 通过web hook这个插件会自动识别代码的更新，然后自动构建保证实时同步，持续集成再次查看网页的变化 构建的历史版本的存放目录：/data/backcode/test 123456789[root@autoweb test]# pwd/data/backcode/test[root@autoweb test]# ls1 2 3 4 5#还有一个关键的目录，这个目录下是将新代码构建成镜像的关键，他和上个目录实时同步[root@autoweb workspace]# pwd/data/jenkins/workspace[root@autoweb workspace]# lsDockerfile test 构建的历史版本的存放目录：/data/backcode/test 123456789[root@autoweb test]# pwd/data/backcode/test[root@autoweb test]# ls1 2 3 4 5#还有一个关键的目录，这个目录下是将新代码构建成镜像的关键，他和上个目录实时同步[root@autoweb workspace]# pwd/data/jenkins/workspace[root@autoweb workspace]# lsDockerfile test 测试完成 关于 kubernetes 还有好几种集群管理方法，我们这次用的 deployment模板 就是其中之一， 其他的还有pod 模板 和 rc 模板， 这些都是功能很强大的集群调度模板。 还有更多功能待开发","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"Python的认识与安装","slug":"Python的认识与安装","date":"2020-03-08T16:00:00.000Z","updated":"2020-03-10T12:41:07.527Z","comments":true,"path":"Python的认识与安装.html","link":"","permalink":"https://pdxblog.top/Python%E7%9A%84%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%AE%89%E8%A3%85.html","excerpt":"","text":"初识Python人生苦短，我用Python Python的由来： 1989年圣诞节期间，在阿姆斯特丹，Guido为了打发圣诞节的无趣，决心开发一个新的脚本解释程序，作为ABC语言的一种继承。Python是一种跨平台的计算机程序设计语言。是一种面向对象的动态类型语言，最初被设计用于编写自动化脚本，随着版本的不断更新和语言新功能的添加，越多被用于独立的、大型项目的开发 Python的应用领域 web全栈 算法工程师 人工只能 游戏开发 机器学习 信息安全 网络爬虫 自动化测试 数据分析 自动化运维 Python语言特点 简单易学 语法优美 丰富强大的库 开发效率高 应用领域广泛 安装python1、windows系统 下载地址： https://www.python.org/ftp/python/3.8.2/python-3.8.2-amd64.exe 如果感觉下载太慢，这里附上网盘连接提取码：tp9n 也可以去python官网下载想要的版本 注意事项： 默认安装没有勾选“自动添加python的环境变量” 不要把python抽象安装到含有中文的目录中 双击安装，勾选两个选项，选择第二个自定义安装 默认就行，直接Next 自定义目录禁止中文目录安装，选择目录之后，直接Install 检查python是否安装成功: 12345678C:\\Users\\Admin&gt;pythonPython 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AMD64)] on win32Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.&gt;&gt;&gt; print ('hello python.')hello python.&gt;&gt;&gt; exit()C:\\Users\\Admin&gt; 退出python的两种方法 ctrl+z 回车 exit() Python目录介绍 Python二进制文件的启动目录：F:\\Python Python库的安装命令端：F:\\Python\\Scripts 2、linux系统 Linux环境自带了Python 2.x版本，但是如果要更新到3.x的版本，可以在Python的官方网站下载Python的源代码并通过源代码构建安装的方式进行安装，具体的步骤如下所示（以CentOS为例） 1）安装依赖库（因为没有这些依赖库可能在源代码构件安装时因为缺失底层依赖库而失败） 1[root@python ~]# yum -y install wget gcc zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel libffi-devel 这一步可能会出现以下错误： 1234567已加载插件：fastestmirror, langpacks/var/run/yum.pid 已被锁定，PID 为 15320 的另一个程序正在运行。Another app is currently holding the yum lock; waiting for it to exit... 另一个应用程序是：PackageKit 内存：120 M RSS （546 MB VSZ） 已启动： Fri Dec 27 15:48:54 2019 - 09:04之前 状态 ：睡眠中，进程ID：15320 解决办法： 1[root@python ~]# rm -rf /var/run/yum.pid 2）下载python源代码并解压 12[root@python ~]# wget https://www.python.org/ftp/python/3.8.1/Python-3.8.1.tgz[root@python ~]# tar zxf Python-3.8.1.tgz 3）设置全局变量 12[root@python ~]# export LANG=zh_CN.UTF-8[root@python ~]# export LANGUAGE=zh_CN.UTF-8 否则可能会出现以下错误 1234generate-posix-vars failedmake[1]: *** [pybuilldddir.txt] 错误 1make[1]: 离开目录\"/root/Python-3.8.1\"make: *** [profile-opt] 错误2 4）编译安装 123[root@python ~]# cd Python-3.8.1[root@python ~]# ./configure --prefix=/usr/local/python381 --enable-optimizations[root@python ~]# make &amp;&amp; make install 5）修改用户主目录下名为.bash_profile的文件，配置PATH环境变量并使其生效 123[root@python ~]# vim .bash_profile#添加export PATH=$PATH:/usr/local/python381/bin 6）激活环境变量 1[root@python ~]# source .bash_profile 7）运行Python程序 1234#查看Python版本信息[root@python ~]# python3 --version#进入交互环境[root@python ~]# python3 检查Python的版本 123456&gt;&gt;&gt; import sys&gt;&gt;&gt; print(sys.version_info)sys.version_info(major-3, minor=8 micro=1 releaselevel='final', serial=0)&gt;&gt;&gt; print(sys.version)3.8.1 (default, Mar 9 2020, 12:21:51)[GCC 4.8.5 20150623 (Red Hat 4.8.5-.9)]","categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"}]},{"title":"Helm应用以及服务升级与回滚","slug":"Helm应用以及服务升级与回滚","date":"2020-03-03T16:00:00.000Z","updated":"2020-03-04T12:15:12.356Z","comments":true,"path":"Helm应用以及服务升级与回滚.html","link":"","permalink":"https://pdxblog.top/Helm%E5%BA%94%E7%94%A8%E4%BB%A5%E5%8F%8A%E6%9C%8D%E5%8A%A1%E5%8D%87%E7%BA%A7%E4%B8%8E%E5%9B%9E%E6%BB%9A.html","excerpt":"","text":"Helm应用以及服务升级与回滚helm：包管理工具 官方提提供的仓库：https://hub.helm.sh/ Charts：是一个Helm的程序包，它包含了运行一个kubernetes应用程序所需要的镜像、依赖关系和资源定义等 Release：应用程序运行charts后，得到的一个实例 部署一个实例： helm install + charts -n release名称 根据以一个包运行一个实例 1234[root@master ~]# helm install stable/redis -n redis --dry-run--dry-run：用来测试有没有问题，如果没有问题就可以运行[root@master ~]# helm install stable/redis -n redis#这里是运行不成功的，因为他需要镜像，PV等准备工作 运行之后会有三部分描述 1、关于这个Release的描述 2、关于这个Release资源的描述 3、怎么使用这个Release 根据提示可以获得redis的密码，等等一些信息 12[root@master ~]# kubectl get secret --namespace default redis-redis -o jsonpath=\"&#123;.data.redis-password&#125;\" | base64 --decoderTmeGF2rcY 删除实例： 1[root@master ~]# helm delete redis --purge 查询chart包 1[root@master ~]# helm search mysql 运行一个实例： 1[root@master ~]# helm install stable/mysql -n mysql 我们运行过的实例都会生成一个charts包存放在这个缓存目录下 1234[root@master archive]# pwd/root/.helm/cache/archive[root@master archive]# lsmysql-0.3.5.tgz redis-1.1.15.tgz Charts包解压目录 1234567891011121314[root@master archive]# tar zxf mysql-0.3.5.tgz[root@master archive]# tree -C mysqlmysql├── Chart.yaml├── README.md├── templates│ ├── configmap.yaml│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── NOTES.txt│ ├── pvc.yaml│ ├── secrets.yaml│ └── svc.yaml└── values.yaml Chart.yaml：这个chart包的概要信息 ​ name和version这两个是必填项，其他可选 README.md：是这个chart包的一个使用帮助文档 templates：chart包内各种资源对象模板 values.yaml：是这个chart包的默认的值，可以被template内的yaml文件使用 我们在部署之前还可以提前查看这个包会有什么东西 1[root@master ~]# helm inspect values stable/prometheus 除了部署实例后会生成chart包，还可以下载chart包 1[root@master ~]# helm fetch stable/prometheus Helm部署安装一个Mysql服务 部署NFS服务： 123[root@master ~]# mkdir /data[root@master ~]# vim /etc/exports/data *(rw,sync,no_root_squash) 创建PV： 1234567891011121314151617181920[root@master ~]# vim nfs-pv1.ymlapiVersion: v1kind: PersistentVolumemetadata: name: mysqlpvspec: capacity: storage: 8Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle nfs: path: /data/mysqlpv server: 192.168.1.70[root@master ~]# kubectl apply -f nfs-pv1.yml persistentvolume/mysqlpv created[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmysqlpv 8Gi RWO Recycle Available 16s[root@master ~]# mkdir /data/mysqlpv 部署mysql实例（镜像提前准备好）： 123456789101112131415[root@master ~]# helm install stable/mysql -n bdqn-mysql --set mysqlRootPassword=123.com #创建实例，并设置密码[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEbdqn-mysql-mysql-7b89c7b99-kg4wf 0/1 Init:0/1 0 17s #正在初始化[root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEbdqn-mysql-mysql Bound mysqlpv 8Gi RWO 48s#还需要一个小镜像docker pull busybox:1.25.0[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEbdqn-mysql-mysql-7b89c7b99-kg4wf 1/1 Running 0 2m10s[root@master ~]# kubectl get deployments.NAME READY UP-TO-DATE AVAILABLE AGEbdqn-mysql-mysql 1/1 1 1 2m13s 查看密码是否设置成功： 12[root@master ~]# kubectl get secret --namespace default bdqn-mysql-mysql -o jsonpath=\"&#123;.data.mysql-root-password&#125;\" | base64 --decode; echo123.com 验证数据有没有问题： 1234567891011[root@master ~]# kubectl exec -it bdqn-mysql-mysql-7b89c7b99-kg4wf -- mysql -u root -p123.commysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.01 sec) 在部署mysql的时候，如何开启storageclass，以及如何使用？ 首先需要部署storageclass（存储类），让他可以自动创建PV 1、需要基于NFS环境 2、RBAC权限 3、nfs-deployment. 4、storageclass helm在创建实例的时候是基于templates模板里的内容创建的，而模板内需要的信息则是在values.yaml文件里，我们则需要修改values.yaml文件里的内容就行 12345[root@master mysql]# vim values.yaml 找到storageClass，去掉注释即可 storageClass: \"test-nfs\" #storageclass的名称 accessMode: ReadWriteOnce size: 8Gi 然后直接部署实例，在部署实例的时候 -f 指定这个values.yaml这个文件即可，它会根据实例的PVC自动创建PV 如果想将sservice资源对象的类型更改为NodePort，又应该怎么做 同样只需要修改values.yaml文件就行 123#将类型给位NodePort即可，还可以自己指定端口 type: NodePort port: 3306 在创建实例的时候只需要指定以下这个文件就行 12345[root@master mysql]# helm install stable/mysql -n bdqn-mysql --set mysqlRootPassword=123.com -f values.yaml[root@master mysql]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEbdqn-mysql-mysql NodePort 10.104.45.139 &lt;none&gt; 3306:30165/TCP 13mkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 60d 服务的升级： 1234[root@master mysql]# helm upgrade --set imageTag=5.7.15 bdqn-mysql stable/mysql -f values.yaml[root@master mysql]# kubectl get deployments. -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORzhb-mysql 1/1 1 1 9m22s zhb-mysql mysql:5.7.15 app=zhb-mysql 回滚 1234567891011#查看有哪些版本[root@master mysql]# helm history zhbREVISION UPDATED STATUS CHART DESCRIPTION 1 Mon Mar 2 16:33:26 2020 SUPERSEDED mysql-0.3.5 Install complete2 Mon Mar 2 16:37:04 2020 DEPLOYED mysql-0.3.5 Upgrade complete回滚到1版本[root@master mysql]# helm rollback zhb 1Rollback was a success.[root@master mysql]# kubectl get deployments. -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORzhb-mysql 1/1 1 1 13m zhb-mysql mysql:5.7.14 app=zhb-mysql","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"Helm自定义模板以及私有库部署","slug":"Helm自定义模板以及私有库部署","date":"2020-03-03T16:00:00.000Z","updated":"2020-03-04T12:15:12.351Z","comments":true,"path":"Helm自定义模板以及私有库部署.html","link":"","permalink":"https://pdxblog.top/Helm%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E6%9D%BF%E4%BB%A5%E5%8F%8A%E7%A7%81%E6%9C%89%E5%BA%93%E9%83%A8%E7%BD%B2.html","excerpt":"","text":"Helm自定义模板以及私有库部署 开发自己的chart 12345678910111213141516171819202122232425[root@master ~]# helm create mychartCreating mychart[root@master ~]# tree -C mychart/mychart/├── charts├── Chart.yaml├── templates│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── ingress.yaml│ ├── NOTES.txt│ ├── service.yaml│ └── tests│ └── test-connection.yaml└── values.yaml[root@master mychart]# pwd/root/mychart[root@master mychart]# vim values.yamlreplicaCount: 1image: repository: nginx tag: stable pullPolicy: IfNotPresent ---------- #这些都都是默认信息 因为这是我们自己开发的，所以有可能会出现错误，一般我们在部署之前都会进行调试 调试 1[root@master ~]# helm install --dry-run --debug mychart 修改values.yaml文件，模拟一些错误来查看效果 123[root@master ~]# helm install --dry-run --debug mychart): error converting YAML to JSON: yaml: line 12: could not find expected ':'#会告诉你第12行少了个：但是这个12行不是特别准确，只是一个大概的方向 安装chart 四种方法 通过仓库安装 将chart下载下来，通过tar包安装 通过chart本地目录安装（将tar包解压得到的目录） 通过URL安装 1、通过仓库安装（以redis为例） 1[root@master ~]# helm install stable/redis -n redis 2、将chart下载下来，通过tar包安装 12[root@master ~]# helm fetch stable/redis[root@master ~]# helm install redis-1.1.15.tgz 3、通过chart本地目录安装 123456789101112131415[root@master ~]# tar zxf redis-1.1.15.tgz[root@master ~]# tree -C redisredis├── Chart.yaml├── README.md├── templates│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── networkpolicy.yaml│ ├── NOTES.txt│ ├── pvc.yaml│ ├── secrets.yaml│ └── svc.yaml└── values.yaml[root@master ~]# helm install redis 4、通过URL安装 1[root@master ~]# helm install http://xxx/charts/xxx.tgz -n name 使用本地目录安装刚刚自定义的mychart： 要求： 副本Pod数量为3个 service类型为NodePort 映射的端口为31033 123456789[root@master ~]# cd mychart/[root@master mychart]# vim values.yamlreplicaCount: 3service: type: NodePort port: 80 nodePort: 31033[root@master mychart]# helm install -n test ../mychart/ 12345678910111213[root@master mychart]# kubectl get podNAME READY STATUS RESTARTS AGEtest-mychart-657bfc65b8-5j8qn 1/1 Running 0 30stest-mychart-657bfc65b8-bgt2s 1/1 Running 0 30stest-mychart-657bfc65b8-v2sph 1/1 Running 0 30s[root@master mychart]# kubectl get deployments.NAME READY UP-TO-DATE AVAILABLE AGEtest-mychart 3/3 3 3 46s[root@master mychart]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 61dtest-mychart NodePort 10.104.53.170 &lt;none&gt; 80:30393/TCP 50s#可以看到这里的端口与我们指定的并不一致 因为这些yaml文件都是引用template模板，template模板目录下的service.yaml里面没有nodePort字段，只需要在service.yaml文件里添加就行 12345678910111213141516171819#实例的更新[root@master templates]# pwd/root/mychart/templates[root@master templates]# vim service.yamspec: type: &#123;&#123; .Values.service.type &#125;&#125; ports: - port: &#123;&#123; .Values.service.port &#125;&#125; targetPort: http protocol: TCP name: http nodePort: &#123;&#123; .Values.service.nodePort&#125;&#125;#再更新以下，进行验证[root@master ~]# helm upgrade test mychart/ -f mychart/values.yaml[root@master ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 61dtest-mychart NodePort 10.104.53.170 &lt;none&gt; 80:31033/TCP 10m#端口已经改变了 一般在企业都是使用自己的私有镜像，那么就需要搭建私有仓库 12345678910111213[root@master ~]# vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.70:5000[root@master ~]# systemctl daemon-reload [root@master ~]# systemctl restart docke#node01、node02也加入私有仓库[root@node01 ~]# vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.70:5000[root@node01 ~]# systemctl daemon-reload [root@node01 ~]# systemctl restart docke[root@node02 ~]# vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --insecure-registry 192.168.1.70:5000[root@node02 ~]# systemctl daemon-reload [root@node02 ~]# systemctl restart docke 导入私有镜像，并上传到私有仓库 12345678[root@master ~]# docker load &lt; httpd-v1.tar &amp;&amp; docker load &lt; httpd-v2.tar &amp;&amp; docker load &lt; httpd-v3.tar[root@master ~]# docker tag httpd:v1 192.168.1.70:5000/httpd:v1[root@master ~]# docker push 192.168.1.70:5000/httpd:v1[root@master ~]# docker push 192.168.1.70:5000/httpd:v1 [root@master ~]# docker tag httpd:v2 192.168.1.70:5000/httpd:v2[root@master ~]# docker tag httpd:v3 192.168.1.70:5000/httpd:v3[root@master ~]# docker push 192.168.1.70:5000/httpd:v2[root@master ~]# docker push 192.168.1.70:5000/httpd:v3 node01、node02拉去镜像 123456[root@node01 ~]# docker pull 192.168.1.70:5000/httpd:v1[root@node01 ~]# docker pull 192.168.1.70:5000/httpd:v2[root@node01 ~]# docker pull 192.168.1.70:5000/httpd:v3[root@node02 ~]# docker pull 192.168.1.70:5000/httpd:v1[root@node02 ~]# docker pull 192.168.1.70:5000/httpd:v2[root@node02 ~]# docker pull 192.168.1.70:5000/httpd:v3 练习： 使用mychart部署一个实例，名为bdqn，使用私有镜像v1版本 完成之后，将实例做一个升级，将镜像改为v2版本 123456789101112131415161718192021222324252627282930313233343536#修改yaml文件，运行实例[root@master ~]# vim mychart/values.yamlimage: repository: 192.168.1.70:5000/httpd tag: v1 pullPolicy: IfNotPresent[root@master ~]# helm install -n bdqn mychart/[root@master ~]# kubectl get deployments. -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORbdqn-mychart 1/1 1 1 8s mychart 192.168.1.70:5000/httpd:v1 app.kubernetes.io/instance=bdqn,app.kubernetes.io/name=mychart[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEbdqn-mychart-574ffc5496-bd8vf 1/1 Running 0 22s#实例升级，通过yaml文件的方式[root@master ~]# vim mychart/values.yamlimage: repository: 192.168.1.70:5000/httpd tag: v2[root@master ~]# helm upgrade bdqn mychart/ -f mychart/values.yaml[root@master ~]# kubectl get deployments. -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORbdqn-mychart 1/1 1 1 2m54s mychart 192.168.1.70:5000/httpd:v2 app.kubernetes.io/instance=bdqn,app.kubernetes.io/name=mychart[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEbdqn-mychart-85dcbbcb8f-8h47q 1/1 Running 0 5m28s#还可以通过命令的方式[root@master ~]# helm upgrade bdqn mychart/ --set imageTAG=v2//这种方法更新完成后，查看deployment的时候，镜像显示是没有更新的#还可以通过edit的方式进行更改[root@master ~]# kubectl edit deployments. bdqn-mychart spec: containers: - image: 192.168.1.70:5000/httpd:v3[root@master ~]# kubectl get deployments. -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORbdqn-mychart 1/1 1 1 28m mychart 192.168.1.70:5000/httpd:v3 app.kubernetes.io/instance=bdqn,app.kubernetes.io/name=mychar 创建自己的Repo仓库 我们自己创建的chart包，如果公司内其他的同事也需要用，我们可以cp一份给他，但是效率太低，所以就可以创建一个repo仓库，解决这个需求 1、在node01上运行一个httpd的容器（作为私有仓库） 12345[root@node01 ~]# mkdir /var/www[root@node01 ~]# docker run -d -p 8080:80 -v /var/www/:/usr/local/apache2/htdocs httpd:latest[root@node01 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES03e5e3ef5c95 httpd:latest \"httpd-foreground\" 13 seconds ago Up 12 seconds 0.0.0.0:8080-&gt;80/tcp kind_roentgen 2、master节点上，使用helm package将mychart目录打包 1234[root@master ~]# helm package mychart/Successfully packaged chart and saved it to: /root/mychart-0.1.0.tgz[root@master ~]# lsmychart-0.1.0.tgz 3、执行helm repo index生成仓库的index文件 123456789101112131415161718192021[root@master ~]# mkdir myrepo[root@master ~]# mv mychart-0.1.0.tgz myrepo/[root@master ~]# ls myrepo/mychart-0.1.0.tgz[root@master ~]# helm repo index myrepo/ --url http://192.168.1.50:8080/charts #第一步运行的容器的IP+端口[root@master ~]# ls myrepo/index.yaml mychart-0.1.0.tgz[root@master ~]# cat myrepo/index.yaml apiVersion: v1entries: mychart: - apiVersion: v1 appVersion: \"1.0\" created: \"2020-03-04T11:08:33.079034645+08:00\" description: A Helm chart for Kubernetes digest: f2a297c4b377ae7f208848bef8823eeb74ebb7270d8bf07f58270678d0784056 name: mychart urls: - http://192.168.1.50:8080/charts/mychart-0.1.0.tgz version: 0.1.0generated: \"2020-03-04T11:08:33.07808906+08:00\" 4、将生成的tar包和index.yaml上传到node01的/var/www/charts目录下 123456789#因为node01上没有sharts目录，所以需要创建[root@node01 ~]# mkdir /var/www/chart[root@master ~]# cd myrepo/[root@master myrepo]# scp index.yaml mychart-0.1.0.tgz node01:/var/www/chartsindex.yaml 100% 400 0.4KB/s 00:00 mychart-0.1.0.tgz 100% 2861 2.8KB/s 00:00#在node01上进行验证[root@node01 ~]# ls /var/www/chartsindex.yaml mychart-0.1.0.tgz 5、添加新的repo仓库 1234567[root@master myrepo]# helm repo add myrepo http://192.168.1.50:8080/charts\"myrepo\" has been added to your repositories[root@master myrepo]# helm repo listNAME URL stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/chartslocal http://127.0.0.1:8879/charts #这个是不能跨主机的只能在本地使用 myrepo http://192.168.1.50:8080/charts #这个可以跨主机 至此，已经可以正常供内网环境使用这个charts包的私有仓库了 6、验证，我们就可以直接使用新的repo仓库部署实例了 12345678[root@master myrepo]# helm search mychartNAME CHART VERSION APP VERSION DESCRIPTION local/mychart 0.1.0 1.0 A Helm chart for Kubernetesmyrepo/mychart 0.1.0 1.0 A Helm chart for Kubernetes[root@master myrepo]# helm install myrepo/mychart -n test[root@master ~]# helm listNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACEtest 1 Wed Mar 4 11:23:25 2020 DEPLOYED mychart-0.1.0 1.0 7、如果以后仓库中新添加了chart包，需要用helm repo update命令更新本地的index文件 练习： 新创建一个bdqn的chart包，然后将chart包上传到上述repo源中 123456789101112131415161718192021222324252627#创建新的chart[root@master ~]# helm create bdqnCreating bdqn#将这个chart目录打包[root@master ~]# helm package bdqn/Successfully packaged chart and saved it to: /root/bdqn-0.1.0.tgz#移动到myrepo下[root@master ~]# mv bdqn-0.1.0.tgz myrepo/[root@master ~]# ls myrepo/bdqn-0.1.0.tgz index.yaml mychart-0.1.0.tgz#更新index文件[root@master ~]# helm repo index myrepo/ --url http://192.168.1.50:8080/charts[root@master myrepo]# scp bdqn-0.1.0.tgz index.yaml node01:/var/www/chartsbdqn-0.1.0.tgz 100% 2826 2.8KB/s 00:00 index.yaml 100% 720 0.7KB/s 00:00#更新repo仓库[root@master myrepo]# helm repo updateHang tight while we grab the latest from your chart repositories......Skip local chart repository...Successfully got an update from the \"myrepo\" chart repository...Successfully got an update from the \"stable\" chart repositoryUpdate Complete.#搜索验证[root@master myrepo]# helm search bdqnNAME CHART VERSION APP VERSION DESCRIPTION local/bdqn 0.1.0 1.0 A Helm chart for Kubernetesmyrepo/bdqn 0.1.0 1.0 A Helm chart for Kubernetes 使用url部署这个bdqn实例 12345[root@master ~]# helm install http://192.168.1.50:8080/charts/bdqn-0.1.0.tgz -n t1[root@master ~]# helm listNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE t1 1 Wed Mar 4 11:52:50 2020 DEPLOYED bdqn-0.1.0 1.0 default test 1 Wed Mar 4 11:23:25 2020 DEPLOYED mychart-0.1.0 1.0","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"HPA","slug":"HPA","date":"2020-02-28T16:00:00.000Z","updated":"2020-02-29T12:11:53.950Z","comments":true,"path":"HPA.html","link":"","permalink":"https://pdxblog.top/HPA.html","excerpt":"","text":"HPA HPA的全称为Horizontal Pod Autoscaling，它可以根据当前pod资源的使用率（如CPU、磁盘、内存等），进行副本数的动态的扩容与缩容，以便减轻各个pod的压力。当pod负载达到一定的阈值后，会根据扩缩容的策略生成更多新的pod来分担压力，当pod的使用比较空闲时，在稳定空闲一段时间后，还会自动减少pod的副本数量 前提条件：系统应该能够获取当前Pod的资源使用情况（意思是可以执行 kubectl top pod命令，并且能够得到反馈信息） heapster：这个组件之前是集成在k8s集群的，不过在1.12版本之后就被移除了。如果还想使用此功能，应该部署metricServer这个k8s集群资源使用情况的聚合器 要是想实现自动扩容缩容的功能，还需要部署heapster服务，而这个服务集成在Prometheus的MetricServer服务中，也就是说需要部署Prometheus服务，但是我们也可以直接部署heapster服务 实现Pod的扩容与缩容示例 因为heapster集成在MetricServer服务中，所以首先部署这个服务 1、首先安装MerticServer服务，从Github上克隆项目 1[root@master ~]# git clone https://github.com/kubernetes-incubator/metrics-server.git 2、修改yaml文件 1234567[root@master ~]# vim metrics-server/deploy/kubernetes/metrics-server-deployment.yaml image: k8s.gcr.io/metrics-server-amd64:v0.3.1 #更换镜像版本 //在44行添加 command: - /metrics-server - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP 3、下载metrics-server镜像k8s.gcr.io/metrics-server-amd64:v0.3.1（因为国内无法访问k8s.gcr.io，所以采用以下办法） pull-google-container 工具脚本 12345678910[root@master ~]# vim pull-google.sh image=$1 echo $1 img=`echo $image | sed 's/k8s\\.gcr\\.io/anjia0532\\/google-containers/g;s/gcr\\.io/anjia0532/g;s/\\//\\./g;s/ /\\n/g;s/_/-/g;s/anjia0532\\./anjia0532\\//g' | uniq | awk '&#123;print \"\"$1\"\"&#125;'` echo \"docker pull $img\" docker pull $img echo \"docker tag $img $image\" docker tag $img $image [root@master ~]# chmod +x pull-google.sh &amp;&amp; cp pull-google.sh /usr/local/bin/pull-google-container [root@master ~]# pull-google-container k8s.gcr.io/metrics-server-amd64:v0.3.1 4、将镜像打包发给k8s各个节点 12345[root@master ~]# docker save &gt; metrics-server-amd64.tar k8s.gcr.io/metrics-server-amd64:v0.3.1 [root@master ~]# scp metrics-server-amd64.tar node01:/root [root@master ~]# scp metrics-server-amd64.tar node02:/root [root@node01 ~]# docker load &lt; metrics-server-amd64.tar [root@node02 ~]# docker load &lt; metrics-server-amd64.tar 5、运行yaml文件 123456789101112[root@master ~]# kubectl apply -f metrics-server/deploy/kubernetes/ clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created serviceaccount/metrics-server created deployment.apps/metrics-server created service/metrics-server created clusterrole.rbac.authorization.k8s.io/system:metrics-server created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created [root@master ~]# kubectl get pod -n kube-system metrics-server-849dcc6bb4-hr5xp 1/1 Running 0 13s 6、验证 12345678910[root@master ~]# kubectl top node error: metrics not available yet #这里等一会就行 [root@master ~]# kubectl top pod -n kube-system metrics-server-849dcc6bb4-hr5xp NAME CPU(cores) MEMORY(bytes) metrics-server-849dcc6bb4-hr5xp 1m 13Mi [root@master ~]# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% master 56m 2% 1145Mi 66% node01 12m 0% 478Mi 27% node02 11m 0% 452Mi 26% 这里，我们使用一个测试镜像，这个镜像基于php-apache制作的docker镜像，包含了一些可以运行cpu密集计算任务的代码（模拟压力测试） 12345678910[root@master ~]# kubectl run php-apache --image=mirrorgooglecontainers/hpa-example:latest --requests=cpu=200m --expose --port=80[root@master ~]# kubectl get deployments.NAME READY UP-TO-DATE AVAILABLE AGEphp-apache 1/1 1 1 33s[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEphp-apache-794cdd478f-l9kxn 1/1 Running 0 6m27s[root@master ~]# kubectl top pod php-apache-794cdd478f-l9kxn NAME CPU(cores) MEMORY(bytes) php-apache-794cdd478f-l9kxn 0m 9Mi 创建HPA控制器 123456[root@master ~]# kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10horizontalpodautoscaler.autoscaling/php-apache autoscaled[root@master ~]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEphp-apache Deployment/php-apache 0%/50% 1 10 1 2m1s限制cpu使用率不能超过50%，最少有一个Pod，最多有10个 实时监控Pod的状态 123[root@master ~]# kubectl get pod -wNAME READY STATUS RESTARTS AGEphp-apache-794cdd478f-l9kxn 1/1 Running 0 40m 创建一个应用，用来不停的访问我们刚刚创建的php-apache的svc资源 1[root@master ~]# kubectl run -i --tty load-generator --image=busybox /bin/sh 进入Pod内，执行此命令用来模拟访问php-apache的svc资源 12#对Pod进行死循环请求/ # while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done 运行一段时间后查看pod的数量变化 123456789101112NAME READY STATUS RESTARTS AGEload-generator-7d549cd44-xm98c 1/1 Running 1 25mphp-apache-867f97c8cb-4r6sk 1/1 Running 0 19mphp-apache-867f97c8cb-4rcpk 1/1 Running 0 13mphp-apache-867f97c8cb-5pbxf 1/1 Running 0 16mphp-apache-867f97c8cb-8htth 1/1 Running 0 13mphp-apache-867f97c8cb-d94h9 0/1 ContainerCreating 0 13mphp-apache-867f97c8cb-drh52 1/1 Running 0 18mphp-apache-867f97c8cb-f67bs 0/1 ContainerCreating 0 17mphp-apache-867f97c8cb-nxc2r 1/1 Running 0 19mphp-apache-867f97c8cb-vw74k 1/1 Running 0 39mphp-apache-867f97c8cb-wb6l5 0/1 ContainerCreating 0 15m 当停止死循环请求后，也并不会立即减少pod数量，会等一段时间后减少pod数量，防止流量再次激增。 至此，pod副本数量的自动扩缩容就实现了 12345678910[root@master ~]# kubectl get hpa -wNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEphp-apache Deployment/php-apache 106%/50% 1 10 8 50mphp-apache Deployment/php-apache 102%/50% 1 10 8 50mphp-apache Deployment/php-apache 93%/50% 1 10 8 51mphp-apache Deployment/php-apache 87%/50% 1 10 8 51mphp-apache Deployment/php-apache 82%/50% 1 10 8 51mphp-apache Deployment/php-apache 77%/50% 1 10 8 51mphp-apache Deployment/php-apache 68%/50% 1 10 8 52mphp-apache Deployment/php-apache 61%/50% 1 10 8 52m 资源限制 以下只是yaml文件中的一段，并不是整个yaml文件 基于Podkubernetes对资源的限制实际上是通过cgroup来控制的，cgroup是容器的一组用来控制内核如何运行进程的相关属性集合，针对内存、cpu和各种设备都有对应的cgroup 默认情况下，Pod运行没有cpu和内存的限额，这意味着系统中的任何Pod将能够想执行该Pod所在的节点一样，消耗足够多的cpu和内存，一般会针对某些应用的pod资源进行资源限制，这个资源限制通过resources的requeste和limits来实现 12345678910111213141516[root@master ~]# vim cgroup-pod.yamlspec: containers: - name: xxx imagePullPolicy: Always image: xxx ports: - protocol: TCP containerPort: 80 resources: limits: cpu: \"4\" memory: 2Gi requests: cpu: 260m memory: 260Mi requests：要分配的资源，limits为最高请求的资源值。可以简单的理解为初始值和最大值 基于名称空间 1）计算资源配额 123456789101112[root@master ~]# vim compute-resources.yamlapiVersion: v1kind: ResourceQuotametadata: name: compute-resourcesspec: hard: pods: \"20\" requests.cppu: \"20\" requests.memory: 100Gi limits.cpu: \"40\" limits.memory: 200Gi 2）配置对象数量配额限制 123456789101112[root@master ~]# vim object-counts.yamlapiVersion: v1kind: ResourceQuotametadata: name: object-countsspec: hard: configmaps: \"10\" persistentvolumeclaims: \"4\" replicationcontrollers: \"20\" secrets: \"10\" service.loadbalancers: \"2\" 3）配置CPU和内存的LimitRange 1234567891011121314[root@master ~]# vim limitRange.yamlapiVersion: v1kind: LimiRangemetadata: name: mem-limit-rangespec: limits: - default: memory: 50Gi cpu: 5 defaultRequest: memory: 1Gi cpu: 1 type: Container default即limit的值 defaultRequest即request的值","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"Heldm工具部署","slug":"Helm工具部署","date":"2020-02-28T16:00:00.000Z","updated":"2020-02-29T12:11:53.955Z","comments":true,"path":"Helm工具部署.html","link":"","permalink":"https://pdxblog.top/Helm%E5%B7%A5%E5%85%B7%E9%83%A8%E7%BD%B2.html","excerpt":"","text":"Helm 在Kubernetes中部署容器云的应用也是一项有挑战性的工作，Helm就是为了简化在Kubernetes中安装部署容器云应用的一个客户端工具。通过helm能够帮助开发者定义、安装和升级Kubernetes中的容器云应用，同时，也可以通过helm进行容器云应用的分享。在Kubeapps Hub中提供了包括Redis、MySQL和Jenkins等常见的应用，通过helm可以使用一条命令就能够将其部署安装在自己的Kubernetes集群中 helm的整体架构如下图所示，Helm架构由Helm客户端、Tiller服务器端和Chart仓库所组成；Tiller部署在Kubernetes中，Helm客户端从Chart仓库中获取Chart安装包，并将其安装部署到Kubernetes集群中 Helm是管理Kubernetes包的工具，Helm能提供下面的能力： 创建新的charts 将charts打包成tgz文件 与chart仓库交互 安装和卸载Kubernetes的应用 管理使用Helm安装的charts的生命周期 在Helm中，有三个需要了解的重要概念： chart：是创建Kubernetes应用实例的信息集合； config：创建发布对象的chart的配置信息 release：chart的运行实例，包含特定的config helm组件 在Helm中有两个主要的组件，既Helm客户端和Tiller服务器： Helm客户端：这是一个供终端用户使用的命令行工具，客户端负责如下的工作： 本地chart开发 管理仓库 与Tiller服务器交互 ​ 发送需要被安装的charts，请求关于发布版本的信息，求更新或者卸载已安装的发布版本 Tiller服务器：Tiller服务部署在Kubernetes集群中，Helm客户端通过与Tiller服务器进行交互，并最终与Kubernetes API服务器进行交互。 Tiller服务器负责如下的工作： 监听来自于Helm客户端的请求 组合chart和配置来构建一个发布 在Kubernetes中安装，并跟踪后续的发布 通过与Kubernetes交互，更新或者char 客户端负责管理chart，服务器发展管理发布 Helm技术实现 Helm客户端是使用Go语言编写的，它通过gRPC协议与Tiller服务器交互。 Tiller服务器也是使用Go语言编写的，它使用Kubernetes客户端类库（当前是哦那个REST+JSON）与Kubernetes进行通讯。 Tiller服务器通过Kubernetes的ConfigMap存储信息，因此本身没有用于存储数据库 helm安装部署12345678910#从GitHub上下载helm[root@master ~]# wget https://get.helm.sh/helm-v2.14.3-linux-amd64.tar.gz#解压，获取helm的命令[root@master ~]# tar -zxvf helm-v2.14.3-linux-amd64.tar.gz[root@master ~]# mv linux-amd64/helm /usr/local/bin/[root@master ~]# chmod +x /usr/local/bin/helm #确认命令可用[root@master ~]# helm help#设置tab键自动补全[root@master ~]# source &lt;(helm completion bash) 安装Tiller服务 1234567891011121314151617181920212223#创建授权用户，并授予权限[root@master ~]# vim tiller-rbac.yamlapiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: tiller namespace: kube-system[root@master ~]# kubectl apply -f tiller-rbac.yaml serviceaccount/tiller createdclusterrolebinding.rbac.authorization.k8s.io/tiller created 初始化，生成一个包的仓库 1[root@master ~]# helm init --service-account=tiller 查看pod 12345678910[root@master ~]# kubectl get pod -n kube-systemtiller-deploy-8557598fbc-5cfv6 0/1 ImagePullBackOff 0 4m6s//这个镜像是下载不下来的，因为默认是从谷歌下载的#修改yaml文件，将镜像改为阿里云的[root@master ~]# kubectl edit pod -n kube-system tiller-deploy-8557598fbc-5cfv6//修改spec字段的image image: registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.3保存退出，它会自动下载镜像[root@master ~]# kubectl get pod -n kube-system | grep tillertiller-deploy-8557598fbc-5cfv6 1/1 Running 0 13m 查看helm仓库信息 1234[root@master ~]# helm repo listNAME URL stable https://kubernetes-charts.storage.googleapis.comlocal http://127.0.0.1:8879/charts 添加阿里云的仓库源 123456[root@master ~]# helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts\"stable\" has been added to your repositories[root@master ~]# helm repo listNAME URL stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/chartslocal http://127.0.0.1:8879/charts 查看版本信息，必须保证可以看到client和server，才可以正常使用helm 123[root@master ~]# helm versionClient: &amp;version.Version&#123;SemVer:\"v2.14.3\", GitCommit:\"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085\", GitTreeState:\"clean\"&#125;Server: &amp;version.Version&#123;SemVer:\"v2.14.3\", GitCommit:\"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085\", GitTreeState:\"clean\"&#125;","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"k8s监控","slug":"k8监控","date":"2020-02-27T16:00:00.000Z","updated":"2020-02-28T07:02:46.103Z","comments":true,"path":"k8监控.html","link":"","permalink":"https://pdxblog.top/k8%E7%9B%91%E6%8E%A7.html","excerpt":"","text":"一、k8s的UI访问界面-dashboard General-purpose web UI for Kubernetes clusters 用于Kubernetes集群的通用web UI 在dashbord中，虽然可以做到创建、删除、修改资源等操作，但通常情况下，我们会把它当作监控k8s集群的软件 dashboard能够直观的看到rc、deployment、pod、services等k8s组件的运行情况和日志信息。 1、从Github搜索dasgboard，下载yaml文件 1234567[root@master ~]# mkdir dashboard[root@master ~]# cd dashboard/[root@master dashboard]# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc5/aio/deploy/recommended.yaml[root@master dashboard]# lsrecommended.yaml[root@node01 ~]# docker pull kubernetesui/dashboard:v2.0.0-rc5[root@node02 ~]# docker pull kubernetesui/dashboard:v2.0.0-rc5 2、运行yaml文件： 1234567891011121314151617181920修改service类型类NodePort#在40行的spec字段修改[root@master dashboard]# vim recommended.yamlspec: type: NodePort[root@master dashboard]# kubectl apply -f recommended.yaml namespace/kubernetes-dashboard createdserviceaccount/kubernetes-dashboard createdservice/kubernetes-dashboard createdsecret/kubernetes-dashboard-certs createdsecret/kubernetes-dashboard-csrf createdsecret/kubernetes-dashboard-key-holder createdconfigmap/kubernetes-dashboard-settings createdrole.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createddeployment.apps/kubernetes-dashboard createdservice/dashboard-metrics-scraper createddeployment.apps/dashboard-metrics-scraper created 123456789101112[root@master dashboard]# kubectl get pod -n kubernetes-dashboard NAME READY STATUS RESTARTS AGEdashboard-metrics-scraper-7f5767668b-f7nh6 1/1 Running 0 9m32skubernetes-dashboard-57b4bcc994-2rj9k 1/1 Running 0 9m32s[root@master dashboard]# kubectl get svc -n kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdashboard-metrics-scraper ClusterIP 10.111.237.119 &lt;none&gt; 8000/TCP 16mkubernetes-dashboard NodePort 10.107.77.172 &lt;none&gt; 443:30361/TCP 16m[root@master dashboard]# kubectl get deployments. -n kubernetes-dashboard NAME READY UP-TO-DATE AVAILABLE AGEdashboard-metrics-scraper 1/1 1 1 11m #将收集到的数据制作成图表的形式kubernetes-dashboard 1/1 1 1 11m 3、通过浏览器访问：https://192.168.1.70:30361 两种登录方式： kubeconfig：配置文件 Token：令牌 基于Token的方法登录dashboard 1、创建一个dashboaed的管理用户 12[root@master dashboard]# kubectl create serviceaccount dashboard-admin -n kube-system serviceaccount/dashboard-admin created 2、将这个用户绑定为集群管理用户 1[root@master dashboard]# kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin 3、获取Token 1234567891011#得到Token的名称[root@master dashboard]# kubectl get secrets -n kube-system | grep dashboard-admindashboard-admin-token-mwht2 kubernetes.io/service-account-token 3 5m30s#查看上述得到的secret资源的详细信息，会得到Token[root@master dashboard]# kubectl get secrets -n kube-system dashboard-admin-token-mwht2 NAME TYPE DATA AGEdashboard-admin-token-mwht2 kubernetes.io/service-account-token 3 7m19s//这个类型不是Opaque，说明不是隐藏的，我们可以看到他的详细信息#获取详细信息，得到Token[root@master dashboard]# kubectl describe secrets -n kube-system dashboard-admin-token-mwht2token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tbXdodDIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNjliODE4YjYtOTA3Zi00NTBmLWI3NjgtMTc2ODIyM2Y1OTIyIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.WCcVrx6oXs0k7-0hQOOik30ZPJl0sNeQE987PHv_Jm9ZpLQ4P9VIQdN49uRvNsd7DF4Ozgu5enWFvNsiaCDmYHauK2LAoHbDBURE9wGx8VlMaaquZ1B_ur4lOluP6Ha3wdZB64fEdtrg-6-DjIS7SC2Kqr2Bcl8NeRdtABh3cufgJ2EQoU40-FUy-0ahegYixIrrQ-DXgZeGrXP79RzHmBXaSwbRwTqWXwNf0e25on_gCiiMC-MVmbZ0MXhNNv-jc8uD2obaEUTdOCLg__f482Zy7xLEMjBv9eVn0P5u7c8r45VfDs08zK4Leh5GI4KIgcuxt37TCtfmEz5XEoTLnA 4、在浏览器上使用Token登录 PS：如果是使用的是旧版本的dashboard，使用谷歌浏览器登录，可能不成功，需要换成其他的浏览器，比如火狐 //如果没有显示，就说明serviceaccount，没有绑定账号，就说明没有权限，就什么都看不到 这里我们可以创建资源，有三种方式 从表单创建默认的是Deployment资源对象 还有一些扩容缩容、更新，删除的操作 除了基于Token的方法登录dashboard，还有基于kuberconfig配置文件的登录方式 1、获取Tonke 1234567#得到Token的名称[root@master dashboard]# kubectl get secrets -n kube-system | grep dashboard-admindashboard-admin-token-mwht2 kubernetes.io/service-account-token 3 5m30s#查看上述得到的secret资源的详细信息，会得到Token[root@master dashboard]# kubectl get secrets -n kube-system dashboard-admin-token-mwht2 NAME TYPE DATA AGEdashboard-admin-token-mwht2 kubernetes.io/service-account-token 3 7m19s 2、生成kubeconfig配置文件 12345678910#设置一个环境变量代表获取Token[root@master dashboard]# DASH_TOKEN=$(kubectl get secrets -n kube-system dashboard-admin-token-mwht2 -o jsonpath=&#123;.data.token&#125; | base64 -d)#将k8s集群的配置信息写入kubeconfig配置文件中[root@master dashboard]# kubectl config set-cluster kubernetes --server=192.168.1.70:6443 --kubeconfig=/root/.dashboard-admin.conf#将Token写入配置文件里[root@master dashboard]# kubectl config set-credentials dashboard-admin --token=$DASH_TOKEN --kubeconfig=/root/.dashboard-admin.conf[root@master dashboard]# kubectl config set-context dashboard-admin@kubernetes --cluster=kubernetes --user=dashboard-admin --kubeconfig=/root/.dashboard-admin.conf[root@master dashboard]# kubectl config use-context dashboard-admin@kubernetes --kubeconfig=/root/.dashboard-admin.conf 3、将生成的/root/.dashboard-admin.config的配置文件，导出并保存 1[root@master dashboard]# sz /root/.dashboard-admin.conf 4、从浏览器选择kubeconfig的登录方式，然后导入配置文件即可 二、weave-scope监控k8s集群ScopeWeave Scope是Weaveworks开发的监控工具。Weave Scope在Kubernetes集群中生成进程，容器和主机的映射，以帮助实时了解Docker容器。还可基于图形UI管理容器并在容器上运行诊断命令 1、在Github上直接搜索scope，找到yaml文件并下载下来 2、往下拉找到kubernetes，点击 3、将这个yaml文件下载下来 1[root@master ~]# wget https://cloud.weave.works/k8s/scope.yaml 4、修改yaml，修改service的端口类型 123456789101112131415161718192021222324[root@master ~]# vim scope.yaml#在212行的spec字段中添加 type: NodePort#保存并退出，运行yaml文件root@master ~]# kubectl apply -f scope.yaml namespace/weave createdserviceaccount/weave-scope createdclusterrole.rbac.authorization.k8s.io/weave-scope createdclusterrolebinding.rbac.authorization.k8s.io/weave-scope createddeployment.apps/weave-scope-app createdservice/weave-scope-app createddeployment.apps/weave-scope-cluster-agent createddaemonset.apps/weave-scope-agent created[root@master ~]# kubectl get deployments. -n weave NAME READY UP-TO-DATE AVAILABLE AGEweave-scope-app 1/1 1 1 26m #展示信息weave-scope-cluster-agent 1/1 1 1 26m #收集信息[root@master ~]# kubectl get pod -n weave NAME READY STATUS RESTARTS AGEweave-scope-agent-jv4g8 1/1 Running 0 21sweave-scope-agent-kw7x9 1/1 Running 0 21sweave-scope-agent-vnqks 1/1 Running 0 21sweave-scope-app-78cff98cbc-nx6p5 1/1 Running 0 21sweave-scope-cluster-agent-7cc889fbbf-tnrhv 1/1 Running 0 21s 5、查看端口，使用浏览器访问：192.168.1.70:30366 123[root@master ~]# kubectl get svc -n weave NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEweave-scope-app NodePort 10.110.46.45 &lt;none&gt; 80:30366/TCP 41s 在scope的web界面中，可以查看很多的东西，pod、node节点等详细信息，包括打开容器的终端，查看其日志信息等等…… 三、PrometheusPrometheus可以原生地监测Kubernetes，Prometheus Operator简化了Kubernetes上的Prometheus设置，并允许使用Prometheus适配器提供自定义指标API。 Prometheus提供强大的查询语言和内置仪表板，用于查询和可视化数据 PS：在这里部署的prometheus，并不是Prometheus官网提供的，而是使用的coreOS提供的Prometheus项目 Prometheus各个组件的作用： MetricsServer：是k8s集群资源使用情况的聚合器，收集数据给k8s集群内使用，如kubectl，hpa，scheduler等 Prometheus Operator：是一个系统检测和警报工具箱，用来存储监控数据 Prometheus node-exporter：收集k8s集群内资源的数据，指定告警规则 Prometheus：收集apiServer，scheduler，contorller-manager，kubelet组件的数据，通过http协议传输 Grnfana：可视化数据统计和监控平台 1、克隆Prometheus的项目地址到本地 12345[root@master ~]# mkdir prometheus[root@master ~]# cd prometheus/[root@master prometheus]# git clone https://github.com/coreos/kube-prometheus.git[root@master prometheus]# lskube-prometheus 2、修改grafana-service.yaml文件，使用NodePort的暴露方式，暴露的端口为31001 12345678910[root@master prometheus]# cd kube-prometheus/manifests/[root@master manifests]# vim grafana-service.yaml#在spec字段下添加spec: type: NodePort ports: - name: http port: 3000 targetPort: http nodePort: 31001 3、修改prometheus-service.yaml文件，使用NodePort的暴露方式，暴露的端口为31002 123456789[root@master manifests]# vim prometheus-service.yaml#在spec字段下添加spec: type: NodePort ports: - name: web port: 9090 targetPort: web nodePort: 31002 4、修改alertmanager-service.yaml（配置告警模板）文件，使用NodePort的暴露方式，暴露的端口为31003 123456789[root@master manifests]# vim alertmanager-service.yaml#在spec字段下添加spec: type: NodePort ports: - name: web port: 9093 targetPort: web nodePort: 31003 5、将这个目录中的yaml，全部运行，是运行以上yaml文件的基础环境配置 12345[root@master manifests]# cd setup/ #如果想要运行上面的yaml，首先要运行基础环境的设置[root@master setup]# cd ..[root@master manifests]# pwd/root/prometheus/kube-prometheus/manifests[root@master manifests]# kubectl apply -f setup/ 6、运行主yaml文件 1234[root@master manifests]# cd ..[root@master kube-prometheus]# pwd/root/prometheus/kube-prometheus[root@master kube-prometheus]# kubectl apply -f manifests/ 7、浏览器访问：192.168.1.70:31001//根据提示修改密码，然后保存登录 //将这三个导入一下 浏览器访问grafan官网：https://grafana.com/导入监控模板 搜索prometheus，选择相应的模板 复制ID号 回到grafan，导入模板 部署成功以后，就可运行一条命令，查看资源使用情况（MetricsServer必须部署成功） 12345[root@master ~]# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% master 182m 9% 1380Mi 80% node01 383m 19% 1402Mi 81% node02 396m 19% 1406Mi 81% 总结dashboard： 可以查看集群中应用的运行状态，也能够修改、创建k8s集群中的个各种资源 用于Kubernetes集群的通用web UI，在dasgboard中，虽然可以做到创建、删除、修改资源等操作，但通常情况下，我们会把它仿作监控k8s集群的软件。dashboard能够直观的看到rc、deployment、pod、service等k8s组件与逆行的情况和日志信息 weave-scope： 可以查看集群中应用的运行状态，也能够修改、创建k8s集群中的个各种资源 Weave Scope是Weaveworks开发的监控工具。Weave Scope在Kubernetes集群中生成进程，容器和主机的映射，以帮助实时了解Docker容器。还可基于图形UI管理容器并在容器上运行诊断命令 Prometheus： Prometheus是一个开源系统监控和报警工具。 Prometheus服务可以直接通过目标拉取数据，或者间接地通过中间网关拉取数据。它在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中，PromQL和其他API可视化展示收集的数据在K8s中，关于集群的资源有metrics度量值的概念，有各种不同的exporter可以通过api接口对外提供各种度量值的及时数据，prometheus在与k8s融合工作的过程中就是通过与这些提供metric值的exporter进行交互，获取数据，整合数据，展示数据，触发告警的过程 Prometheus可以原生地监测Kubernetes，Prometheus Operator简化了Kubernetes上的Prometheus设置，并允许使用Prometheus适配器提供自定义指标API。 Prometheus提供强大的查询语言和内置仪表板，用于查询和可视化数据","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"Ingress实现虚拟主机和Https代理访问","slug":"Ingress实现虚拟主机","date":"2020-02-25T16:00:00.000Z","updated":"2020-02-28T07:01:19.747Z","comments":true,"path":"Ingress实现虚拟主机.html","link":"","permalink":"https://pdxblog.top/Ingress%E5%AE%9E%E7%8E%B0%E8%99%9A%E6%8B%9F%E4%B8%BB%E6%9C%BA.html","excerpt":"","text":"Ingress实现虚拟主机和Https代理访问虚拟主机，也叫“网站空间”，就是把一台运行在互联网上的物理服务器划分成多个“虚拟”服务器。虚拟主机技术极大的促进了网络技术的应用和普及。同时虚拟主机的租用服务也成了网络时代的一种新型经济形式 1、首先确定要运行Ingress-nginx-controller服务 123[root@master ~]# kubectl get pod -n ingress-nginx NAME READY STATUS RESTARTS AGEnginx-ingress-controller-5954d475b6-ktpf9 1/1 Running 1 43h 2、将Ingress-nginx-controller暴露为一个service资源对象 123[root@master ~]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.100.97.246 &lt;none&gt; 80:32007/TCP,443:30741/TCP 43h 3、创建一个Deployment资源和一个Service资源，并相互关联 1234567891011121314151617181920212223242526272829[root@master ~]# vim deploy1.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: deploy1spec: replicas: 2 template: metadata: labels: app: nginx1 spec: containers: - name: nginx1 image: nginx---apiVersion: v1kind: Servicemetadata: name: svc-1spec: selector: app: nginx1 ports: - port: 80 targetPort: 80[root@master ~]# kubectl apply -f deploy1.yaml deployment.extensions/deploy1 createdservice/svc-1 created 12345678[root@master ~]# kubectl get pod NAME READY STATUS RESTARTS AGEdeploy1-7df6778547-v6ww9 1/1 Running 0 2m33sdeploy1-7df6778547-vkvwf 1/1 Running 0 2m33s[root@master ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 52dsvc-1 ClusterIP 10.109.213.247 &lt;none&gt; 80/TCP 3m17s 4、创建另外“一对”服务（delpoy2.yaml和svc-2） 1234567891011121314151617181920212223242526272829[root@master ~]# vim deploy2.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: deploy2spec: replicas: 2 template: metadata: labels: app: nginx2 spec: containers: - name: nginx2 image: nginx #这里没有更换镜像，使用相同的nginx镜像---apiVersion: v1kind: Servicemetadata: name: svc-2spec: selector: app: nginx2 ports: - port: 80 targetPort: 80[root@master ~]# kubectl apply -f deploy2.yaml deployment.extensions/deploy2 createdservice/svc-2 created 12345678[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEdeploy2-7b6786d8bf-6xnjs 1/1 Running 0 19sdeploy2-7b6786d8bf-dvjqt 1/1 Running 0 19s[root@master ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 52dsvc-2 ClusterIP 10.106.67.155 &lt;none&gt; 80/TCP 24s 4、创建Ingress规则 12345678910111213141516171819202122232425262728293031[root@master ~]# vim ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-1spec: rules: - host: www1.bdqn.com http: paths: - path: / backend: serviceName: svc-1 servicePort: 80---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-2spec: rules: - host: www2.bdqn.com http: paths: - path: / backend: serviceName: svc-2 servicePort: 80[root@master ~]# kubectl apply -f ingress.yaml ingress.extensions/ingress-1 createdingress.extensions/ingress-2 created 123456789101112131415[root@master ~]# kubectl describe ingresses. ingress-1Rules: Host Path Backends ---- ---- -------- www1.bdqn.com / svc-1:80 (10.244.1.4:80,10.244.2.4:80)[root@master ~]# kubectl describe ingresses. ingress-1Rules: Host Path Backends ---- ---- -------- www2.bdqn.com / svc-2:80 (10.244.1.5:80,10.244.2.5:80)[root@master ~]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.100.97.246 &lt;none&gt; 80:32007/TCP,443:30741/TCP 43h 5、由于实验环境限制（这个域名是假的），所以自己用来模拟一个域名 123在windows上添加域名解析：C:\\Windows\\System32\\drivers\\etc192.168.1.70 www1.bdqn.com192.168.1.70 www2.bdqn.com Ingress资源实现https代理访问在上面的操作中，实现了使用ingress-nginx为后端所有pod提供一个统一的入口，那么，有一个非常严肃的问题需要考虑，就是如何为我们的pod配置CA证书来实现HTTPS访问？在pod中直接配置CA么？那需要进行多少重复性的操作？而且，pod是随时可能被kubelet杀死再创建的。当然这些问题有很多解决方法，比如直接将CA配置到镜像中，但是这样又需要很多个CA证书。 这里有更简便的一种方法，就拿上面的情况来说，后端有多个pod，pod与service进行关联，service又被ingress规则发现并动态写入到ingress-nginx-controller容器中，然后又为ingress-nginx-controller创建了一个Service映射到群集节点上的端口，来供client来访问。 在上面的一系列流程中，关键的点就在于Ingress规则，我们只需要在Ingress的yaml文件中，为域名配置CA证书即可，只要可以通过HTTPS访问到域名，至于这个域名是怎么关联到后端提供服务的pod，这就是属于k8s群集内部的通信了，即便是使用http来通信，也无伤大雅 1、生成一个证书： 1234567891011[root@master ~]# mkdir https[root@master ~]# cd https[root@master https]# openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=testsvc /0=testsvc\"Generating a 2048 bit RSA private key......................................................................................+++............+++writing new private key to 'tls.key'-----Subject Attribute 0 has no known NID, skipped[root@master https]# lstls.crt tls.key 2、创建secret资源，保存证书： 12[root@master https]# kubectl create secret tls tls-secret --key=tls.key --cert tls.crtsecret/tls-secret created 3、创建一个Deployment资源对象，用来模拟web服务 1234567891011121314151617181920212223242526272829[root@master https]# vim deploy3.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: deploy3spec: replicas: 2 template: metadata: labels: app: nginx3 spec: containers: - name: nginx3 image: nginx---apiVersion: v1kind: Servicemetadata: name: svc-3spec: selector: app: nginx3 ports: - port: 80 targetPort: 80[root@master https]# kubectl apply -f deploy3.yamldeployment.extensions/deploy3 createdservice/svc-3 created 12345678910[root@master https]# kubectl get podNAME READY STATUS RESTARTS AGEdeploy3-5c545fcc5f-4n9bw 1/1 Running 0 17sdeploy3-5c545fcc5f-7b4g2 1/1 Running 0 17s[root@master https]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 52dsvc-3 ClusterIP 10.97.212.56 &lt;none&gt; 80/TCP 22m[root@master https]# curl -I 10.97.212.56HTTP/1.1 200 OK 4、创建对应的Ingress规则 12345678910111213141516171819202122[root@master https]# vim ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-3spec: tls: #引用CA证书 - hosts: - www3.bdqn.com secretName: tls-secret rules: - host: www3.bdqn.com http: paths: - path: / backend: serviceName: svc-3 servicePort: 80[root@master https]# kubectl apply -f ingress.yaml ingress.extensions/ingress-3 created//同样，添加域名解析192.168.1.70 www3.bdqn.com 5、查找对应service-NodePort的443端口映射的端口，直接用浏览器访问即可 123[root@master https]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.100.97.246 &lt;none&gt; 80:32007/TCP,443:30741/TCP 44h 通过浏览器访问：https://www3.bdqn.com:30741","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"Ingress原理及配置","slug":"Ingress","date":"2020-02-23T16:00:00.000Z","updated":"2020-02-24T06:19:01.787Z","comments":true,"path":"Ingress.html","link":"","permalink":"https://pdxblog.top/Ingress.html","excerpt":"","text":"Ingress在Kubernetes中，服务和Pod的IP地址仅可以在集群网络内部使用，对于集群外的应用是不可见的。为了使外部的应用能够访问集群内的服务，在Kubernetes中目前提供了以下几种方案： NodePort LoadBalancer Ingress NodePort：简单的来说就是通过Service资源对象，为后端的Pod提供一个统一的访问入口，然后将Service的统一访问接口映射到集群节点上，最终实现client通过映射到集群节点上的端口访问到后端Pod提供的服务 但是，这种方法有个弊端，就是当新生成一个pod服务就需要创建对应的service将其映射到节点端口，当运行的pod过多时，我们节点暴露给client端的端口也会随之增加，这样我们整个k8s群集的危险系数就会增加，因为我们在搭建群集之处，官方明确指出，必须关闭firewalld防火墙及清空iptables规则，现在我们又暴露了那么多端口给client，安全系数可想而知 Ingress就解决了这个弊端： 简单的理解：原先暴露的service，现在给定一个统一的访问入口 Ingress资源对象的组成： Ingress-nginx-controller： 将新加入的Ingress转化为反向代理服务器的配置文件，并使之生效（动态的感知k8s集群内Ingress资源的变化，通过lua脚本实现） Ingress： 将反向代理服务器抽象成一个Ingress对象，每添加一个新的服务，只需要写一个新的Ingress的yaml文件即可，或修改已经存在的Ingress规则的yaml 在k8s集群前边部署一个反向代理服务器，这个服务器代理着k8s集群内部的service资源 Ingress-nginx可以解决什么问题： 动态的配置服务 ​ 如果按照传统方式, 当新增加一个服务时, 我们可能需要在流量入口加一个反向代理指向我们新的k8s服务. 而如果用了Ingress-nginx, 只需要配置好这个服务, 当服务启动时, 会自动注册到Ingress的中, 不需要而外的操作 减少不必要的端口暴露 ​ 配置过k8s的都清楚, 第一步是要关闭防火墙的, 主要原因是k8s的很多服务会以NodePort方式映射出去, 这样就相当于给宿主机打了很多孔, 既不安全也不优雅. 而Ingress可以避免这个问题, 除了Ingress自身服务可能需要映射出去, 其他服务都不要用NodePort方式 Ingress-nginx工作原理： 1）Ingress controller通过和kubernetes api交互，动态的去感知集群中Ingress规则变化，2）然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置，3）再写到nginx-ingress-controller的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中，4）然后reload一下使配置生效。以此达到域名分别配置和动态更新的问题 基于Nginx的Ingress controller根据不同的开发公司，又分为两种： k8s社区版的：Ingress-nginx nginx公司自己开发的：nginx-ingress Ingress-nginx配置实例：1）创建一个web服务，用deployment资源，用httpd奖项，然后创建一个service资源与之关联 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@master ~]# vim deploy_1.yamlapiVersion: v1kind: Namespacemetadata: name: bdqn-ns labels: name: bdqn-ns---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: httpd-deploy namespace: bdqn-nsspec: replicas: 2 template: metadata: labels: app: bdqn-ns spec: containers: - name: httpd image: httpd---apiVersion: v1kind: Servicemetadata: name: httpd-svc namespace: bdqn-nsspec: type: NodePort selector: app: bdqn-ns ports: - name: httpd-port port: 80 targetPort: 80 nodePort: 31033[root@master ~]# kubectl apply -f deploy_1.yaml namespace/bdqn-ns createddeployment.extensions/httpd-deploy createdservice/httpd-svc created 1234567891011121314151617181920[root@master ~]# kubectl get svc -n bdqn-ns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEhttpd-svc NodePort 10.97.86.190 &lt;none&gt; 80:31033/TCP 3m31s[root@master ~]# kubectl get pod -n bdqn-ns NAME READY STATUS RESTARTS AGEhttpd-deploy-966699d76-25wkn 1/1 Running 0 3m33shttpd-deploy-966699d76-6cdwf 1/1 Running 0 3m34s[root@master ~]# kubectl get deployments. -n bdqn-ns NAME READY UP-TO-DATE AVAILABLE AGEhttpd-deploy 2/2 2 2 3m37s[root@master ~]# kubectl describe svc -n bdqn-nsSelector: app=bdqn-nsType: NodePortIP: 10.97.86.190Port: httpd-port 80/TCPTargetPort: 80/TCPNodePort: httpd-port 31033/TCPEndpoints: 10.244.1.2:80,10.244.2.2:80Session Affinity: NoneExternal Traffic Policy: Cluster 2）创建一个web服务，用deployment资源，用tomcat镜像，然后创建一个service资源与之关联 1234567891011121314151617181920212223242526272829303132333435[root@master ~]# vim deploy_2.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: tomcat-deploy namespace: bdqn-nsspec: replicas: 2 template: metadata: labels: app: bdqn-tomcat spec: containers: - name: tomcat image: tomcat:8.5.45---apiVersion: v1kind: Servicemetadata: name: tomcat-svc namespace: bdqn-nsspec: type: NodePort selector: app: bdqn-tomcat ports: - name: tomcat-port port: 8080 targetPort: 8080 nodePort: 32033[root@master ~]# kubectl apply -f deploy_2.yaml deployment.extensions/tomcat-deploy createdservice/tomcat-svc created 1234567891011121314[root@master ~]# kubectl get deployments. -n bdqn-ns NAME READY UP-TO-DATE AVAILABLE AGEhttpd-deploy 2/2 2 2 9m58stomcat-deploy 2/2 2 2 58s[root@master ~]# kubectl get pod -n bdqn-ns NAME READY STATUS RESTARTS AGEhttpd-deploy-966699d76-25wkn 1/1 Running 0 10mhttpd-deploy-966699d76-6cdwf 1/1 Running 0 10mtomcat-deploy-759dc8c885-9wgqw 1/1 Running 0 70stomcat-deploy-759dc8c885-9xmhj 1/1 Running 0 70s[root@master ~]# kubectl get svc -n bdqn-ns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEhttpd-svc NodePort 10.97.86.190 &lt;none&gt; 80:31033/TCP 10mtomcat-svc NodePort 10.98.122.36 &lt;none&gt; 8080:32033/TCP 75s 3）创建Ingress -nginx-controller 12345678910[root@master ~]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.29.0/deploy/static/mandatory.yaml//将yaml文件下载下来在yaml文件中添加：hostNetwork: true spec: //在212行的spec字段下添加 hostNetwork: true //添加这行就行 # wait up to five minutes for the drain of connections[root@master ~]# kubectl apply -f mandatory.yaml[root@master ~]# kubectl get pod -n ingress-nginx NAME READY STATUS RESTARTS AGEnginx-ingress-controller-5954d475b6-xtzbc 1/1 Running 0 16m hostNetwork: true 在deployment资源中，如果添加了此字段，意味着Pod中运行的应用可以直接使用node节点的端口，这样node节点主机所在网络的其他主机，就可以通过访问该端口访问此应用。（类似于docker映射到宿主机的端口） 4）创建Ingress资源：（定义Ingress规则） 12345678910111213141516171819202122232425262728293031[root@master ~]# vim ingress.yamlapiVersion: extensions/v1beta1kind: Ingressmetadata: name: bdqn-ingress namespace: bdqn-ns annotations: nginx.ingress.kubernetes.io/rewrite-target: / #这个千万不要写错，不然后面无法访问spec: rules: - host: ingress.bdqn.com http: paths: - path: / backend: serviceName: httpd-svc servicePort: 80 - path: /tomcat backend: serviceName: tomcat-svc servicePort: 8080[root@master ~]# kubectl apply -f ingress.yaml ingress.extensions/bdqn-ingress created[root@master ~]# kubectl describe ingresses. -n bdqn-ns bdqn-ingressRules: Host Path Backends ---- ---- -------- ingress.bdqn.com / httpd-svc:80 (10.244.1.5:80,10.244.2.4:80) /tomcat httpd-tomcat:8080 (10.244.1.4:8080,10.244.2.5:8080)//如果没有这个信息说明Ingress创建的有问题 1234567891011121314151617[root@master ~]# kubectl exec -it -n ingress-nginx nginx-ingress-controller-5954d475b6-wkqr2 sh/etc/nginx $ cat nginx.conf//没创建Ingress之前这些值都是空的，这就是动态的感知，然后写入配置文件 location / &#123; set $namespace \"bdqn-ns\"; set $ingress_name \"bdqn-ingress\"; set $service_name \"httpd-svc\"; set $service_port \"80\"; set $location_path \"/\"; location ~* \"^/tomcat\" &#123; set $namespace \"bdqn-ns\"; set $ingress_name \"bdqn-ingress\"; set $service_name \"tomcat-svc\"; set $service_port \"8080\"; set $location_path \"/tomcat\"; 因为域名是自定义的，所以要配置域名解析，修改windows的host文件，将IP与域名绑定 1234567//查看Ingress-controller运行在哪个节点，IP 是 ingress-controller Pod运行所在的节点[root@master ~]# kubectl get pod -n ingress-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-ingress-controller-5954d475b6-wkqr2 1/1 Running 1 44h 192.168.1.50 node01 &lt;none&gt; &lt;none&gt;//找到host文件，进行修改C:\\Windows\\System32\\drivers\\etc192.168.1.50 ingress.bdqn.com 现在已经达到了我们想要的功能，现在可以通过ingress.bdqn.com访问httpd服务，通过ingress.bdqn.com/tomcat访问tomcat服务 在上面的访问测试中，虽然访问到了对应的服务，但是有一个弊端，就是在做DNS解析的时候，只能指定Ingress-nginx容器所在的节点IP。而指定k8s集群内部的其他节点IP（包括master）都是不可以访问到的，如果这个节点一旦宕机，Ingress-nginx容器被转移到其他节点上运行（不考虑节点标签的问题，其实保持Ingress-nginx的yaml文件中默认的标签的话，那么每个节点都是有那个标签的）。随之还要我们手动去更改DNS解析的IP（要更改为Ingress-nginx容器所在节点的IP，通过命令“kubectl get pod -n ingress-nginx -o wide”可以查看到其所在节点），很是麻烦 所以就要为ingress资源对象创建一个Service（NodePort），这样在配置DNS解析的时候，就可以通过Ingress.bdqn.com 所有node节点，包括master节点的IP来配置，很方便 5）创建service资源： 12345[root@master ~]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.29.0/deploy/static/provider/baremetal/service-nodeport.yaml[root@master ~]# kubectl apply -f service-nodeport.yaml[root@master ~]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.100.167.12 &lt;none&gt; 80:32756/TCP,443:30501/TCP 2m7s Service-Nodeport 因为Ingress-nginx-controller运行在了集群内的其中一个节点，为了保证即使这个节点宕机，我们对应的域名仍然能够正常的访问服务，所以我们将Ingress-nginx-controller也暴露为一个service资源 至此，这个域名就可以和集群中任意节点的 32756/30501端口进行绑定了","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"ConfigMap、Secret","slug":"ConfigMap、Secret","date":"2020-02-16T16:00:00.000Z","updated":"2020-02-17T07:16:38.374Z","comments":true,"path":"ConfigMap、Secret.html","link":"","permalink":"https://pdxblog.top/ConfigMap%E3%80%81Secret.html","excerpt":"","text":"ConfigMap、Secret为什么有这两个东西： 我们在kubernetes上部署应用的时候，经常会需要传一些配置给我们的应用，比如数据库地址啊，用户名密码啊之类的。我们要做到这个，有好多种方案，比如： 我们可以直接在打包镜像的时候写在应用配置文件里面，但是这种方式的坏处显而易见而且非常明显。 我们可以在配置文件里面通过env环境变量传入，但是这样的话我们要修改env就必须去修改yaml文件，而且需要重启所有的container才行。 我们可以在应用启动的时候去数据库或者某个特定的地方拿，没问题！但是第一，实现起来麻烦；第二，如果配置的地方变了怎么办？ 当然还有别的方案，但是各种方案都有各自的问题。 而且，还有一个问题就是，如果说我的一个配置，是要多个应用一起使用的，以上除了第三种方案，都没办法进行配置的共享，就是说我如果要改配置的话，那得一个一个手动改。假如我们有100个应用，就得改100份配置，以此类推…… kubernetes对这个问题提供了一个很好的解决方案，就是用ConfigMap和Secret 应用场景： 镜像往往是一个应用的基础，还有很多需要自定义的参数或配置，例如资源的消耗、日志的位置级别等等，这些配置可能会有很多，因此不能放入镜像中，Kubernetes中提供了Configmap来实现向容器中提供配置文件或环境变量来实现不同配置，从而实现了镜像配置与镜像本身解耦，使容器应用做到不依赖于环境配置 Secret资源对象： 可以保存轻量的敏感信息，比如数据库的用户名和密码或者认证秘钥等。它保存的数据是以秘文的方式存放的 configMap资源对象： 和Secret一样，拥有大多数共同的特性，但是区别是，configMap保存的是一些不太重要的信息，它保存的数据是以明文的方式存放的。 当我们创建上述两种资源对象时，其实就是将这两种资源对象存储的信息写入了k8s群集中的etcd数据中心 Secret与ConfigMap的异同： 相同之处： 都是用来保存轻量级信息的，可以供其他资源对象（Deployment、RC、RS和Pod）进行挂载使用 这两种资源对象的创建方法（4种）及引用方法（2种）都是一样的，都是以键值对的方式进行存储的 不同之处： Secret是用来保存敏感信息的，而configMap是用来保存一些不太重要的数据的，具体表现在当我们执行“kubectl describe ….”命令时，Secret这种类型的资源对象时查看不到其具体的信息的，而configMap是可以查看到其保存的具体内容的 Secret:Secret：用于保存一些敏感信息，比如数据库的用户名密码或者密钥。这些数据是比较少量的，将这些信息放在 secret中比放在 pod 的定义或者 docker 镜像中来说更加安全和灵活 用户可以创建自己的secret，系统也会有自己的secret 内置 secret 1[root@master ~]# kubectl get secrets -n kube-system Secret有三种类型： Opaque：base64编码格式的Secret，用来存储密码、密钥等；但数据也通过base64 –decode解码得到原始数据，所有加密性很弱 kubernetes.io/dockerconfigjson：用来存储私有docker registry的认证信息 kubernetes.io/service-account-token： 用于被serviceaccount引用，serviceaccout创建时Kubernetes会默认创建对应的secret。Pod如果使用了serviceaccount，对应的secret会自动挂载到Pod目录/run/secrets/ kubernetes.io/serviceaccount中 举例：保存数据库的用户名和密码 用户名：root 密码：123.com 有四种方法： 1、通过- -from-literal（文字的）： 也就是说需要保存什么，直接写出来就行 注意：每一个–from-literal只能保存一条信息 1[root@master ~]# kubectl create secret generic mysecret1 --from-literal=username=root --from-literal=password=123.com generic：通用的、一般的加密方式 1234[root@master ~]# kubectl get secrets NAME TYPE DATA AGEdefault-token-x9ptl kubernetes.io/service-account-token 3 42dmysecret1 Opaque（不透明的） 2 2m13s Opaque（不透明的）：也就是说你看不到 12345678[root@master ~]# kubectl describe secretsType: OpaqueData====password: 7 bytesusername: 4 bytes//这里看不到真正的值是什么 2、- -from-file（文件）： 同样，每一个只能保存一条信息 12345678[root@master ~]# echo root &gt; username[root@master ~]# echo 123.com &gt; password[root@master ~]# kubectl create secret generic mysecret2 --from-file=username --from-file=password [root@master ~]# kubectl get secrets NAME TYPE DATA AGEdefault-token-x9ptl kubernetes.io/service-account-token 3 42dmysecret1 Opaque 2 28mmysecret2 Opaque 2 28s 既然是通过文件创建的，那么把文件删除，这个secret是否还在 1234567[root@master ~]# rm -rf username password [root@master ~]# kubectl get secrets NAME TYPE DATA AGEdefault-token-x9ptl kubernetes.io/service-account-token 3 42dmysecret1 Opaque 2 29mmysecret2 Opaque 2 92s//它确实还会存在 3、通过- -from-env-file： 这种方法可以把用户名和密码写在一个文件里面，这样就比前两种方便 123456789101112131415[root@master ~]# vim env.txtusername=rootpassword=123.com[root@master ~]# kubectl create secret generic mysecret3 --from-env-file=env.txt[root@master ~]# kubectl get secrets NAME TYPE DATA AGEdefault-token-x9ptl kubernetes.io/service-account-token 3 42dmysecret1 Opaque 2 34mmysecret2 Opaque 2 6m27smysecret3 Opaque 2 16s[root@master ~]# kubectl describe secretsData====password: 7 bytesusername: 4 bytes 4、通过yaml配置文件 先看一下yaml怎么写 1234567891011121314[root@master ~]# kubectl get secrets mysecret1 -o yamlapiVersion: v1data: password: MTIzLmNvbQ== username: cm9vdA==kind: Secretmetadata: creationTimestamp: \"2020-02-14T02:01:34Z\" name: mysecret1 namespace: default resourceVersion: \"13766\" selfLink: /api/v1/namespaces/default/secrets/mysecret1 uid: ffce8c7a-2dfc-4958-9e3b-0fcb50d00ccatype: Opaque 可以看到数据是被加密后写入yaml文件里的，所以我们写的时候不能直接写数据，而是要加密一下 把保存的数据加密： 通过base64方式： 1234[root@master ~]# echo root | base64cm9vdAo=[root@master ~]# echo 123.com | base64MTIzLmNvbQo= 创建secret资源对象： 12345678910111213141516[root@master ~]# vim secret4.yamlapiVersion: v1kind: Secretmetadata: name: mysecret4data: username: cm9vdAo= password: MTIzLmNvbQo=[root@master ~]# kubectl apply -f secret4.yaml[root@master ~]# kubectl get secrets NAME TYPE DATA AGEdefault-token-x9ptl kubernetes.io/service-account-token 3 42dmysecret1 Opaque 2 42mmysecret2 Opaque 2 14mmysecret3 Opaque 2 8mmysecret4 Opaque 2 18s 这种方法虽然说我们看不到真正的数据是什么，但是这种方式也是不安全的，每一次编码后的数据是一样的，是由规律的，同样它是可以被解码的 解码： 12[root@master ~]# echo -n cm9vdAo | base64 --decoderoot 如何来使用Secret资源： Secret 可以作为数据卷被挂载，或作为环境变量 暴露出来以供 pod 中的容器使用。它们也可以被系统的其他部分使用，而不直接暴露在 pod 内 两种方法： 1、以Volume挂载的方式 1234567891011121314151617181920212223242526//创建Pod来引用secret[root@master ~]# vim pod.yamlapiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 300000 volumeMounts: - name: secret-test mountPath: \"/etc/secret-test\" readOnly: true //是否只读，也就是说对于/etc/secret-test只有只读的权限，不能修改 volumes: - name: secret-test secret: secretName: mysecret1[root@master ~]# kubectl apply -f pod.yaml[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEmypod 1/1 Running 0 2m4s 进入容器查看是否有我们保存的数据 123456789[root@master ~]# kubectl exec -it mypod /bin/sh/ # cd /etc/secret-test/etc/secret-test # cat usernameroot/etc/secret-test # cat password123.com/etc/secret-test # echo admin &gt; username/bin/sh: can't create username: Read-only file system//这个文件也是不能修改的，因为是只读文件 还可以自定义存放数据的文件名： 1234567891011121314151617181920212223//在volumes字段下追加items字段： volumes: - name: secret-test secret: secretName: mysecret1 items: - key: username path: my-group/my-username - key: password path: my-group/my-password[root@master ~]# kubectl apply -f pod.yaml pod/mypod created[root@master ~]# kubectl exec -it mypod /bin/sh/ # cd /etc/secret-test//etc/secret-test # lsmy-group/etc/secret-test # cd my-group//etc/secret-test/..2020_02_17_01_40_09.035305611/my-group # lsmy-username my-password/etc/secret-test/..2020_02_17_01_40_09.035305611/my-group # cat my-username root/etc/secret-test/..2020_02_17_01_40_09.035305611/my-group # cat my-password 123.com 2、以环境变量的方式 123456789101112131415161718192021222324252627282930[root@master ~]# cp pod.yaml pod-env.yaml [root@master ~]# vim pod-env.yamlapiVersion: v1kind: Podmetadata: name: mypod2spec: containers: - name: mypod2 image: busybox args: - /bin/sh - -c - sleep 300000 env: - name: SECRET_USERNAME valueFrom: secretKeyRef: //翻译过来就是机密键引用，提取mysecret2里面的数据到SECRET_USERNAME name: mysecret2 key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: //与上面同理 name: mysecret2 key: password[root@master ~]# kubectl apply -f pod-env.yaml[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEmypod 1/1 Running 0 18mmypod2 1/1 Running 0 79s 同样，进入pod查看 12345[root@master ~]# kubectl exec -it mypod2 /bin/sh/ # echo $SECRET_USERNAMEroot/ # echo $SECRET_PASSWORD123.com 如果现在将secret资源内保存的数据进行更新，使用此数据的应用内，数据是否也会更新 更新mysecret1的数据：把password—–&gt;123.com—-&gt;admin 12345678910111213[root@master ~]# echo admin | base64YWRtaW4K[root@master ~]# kubectl get secrets zhbsecret2 -o yaml//可以通过edit命令直接修改：[root@master ~]# kubectl edit secrets zhbsecret2data: username: cm9vdAo= password: YWRtaW4K //更改[root@master ~]# kubectl exec -it mypod /bin/sh/ # cd /etc/secret-test/my-group//etc/secret-test/..2020_02_17_01_57_25.223684048/my-group # cat my-password admin//可以看到已经跟着改变了 注意： ​ 这里引用数据是以volumes挂载使用数据的方式，才会实时更新 那么，以环境变量的方式引用的数据，是否会实时更新？ 12345678[root@master ~]# kubectl edit secrets mysecret4data: username: cm9vdAo= password: YWRtaW4K[root@master ~]# kubectl exec -it mypod2 /bin/sh/ # echo $SECRET_PASSWORD123.com//没有变化 总结： ​ 如果引用secret数据的应用，要求会随着secret资源对象内保存的数据的更新而实时更新，那么应该使用volumes挂载的方式引用资源。因为用环境变量的方式引用不会实时更新数据 ConfigMap: 和Secret资源类似，不同之处在于，secret资源保存的是敏感信息，而configmap保存的方式是以明文的方式存放的数据 什么是ConfigMap： ConfigMap对像是一系列配置的集合，k8s会将这一集合注入到对应的Pod对像中，并为容器成功启动使用。注入的方式一般有两种，一种是挂载存储卷，一种是传递变量。ConfigMap被引用之前必须存在，属于名称空间级别，不能跨名称空间使用，内容明文显示。ConfigMap内容修改后，对应的pod必须重启或者重新加载配置 创建ConfigMap的4种方式： username：adam age：18 和secretc创建的方式一模一样 1、通过- -from-literal（文字的）： 1234567891011121314[root@master ~]# kubectl create configmap myconfigmap1 --from-literal=username=adam --from-literal=age=18configmap/myconfigmap1 created[root@master ~]# kubectl get configmaps NAME DATA AGEmyconfigmap1 2 32s[root@master ~]# kubectl describe configmapsData====age:----18username:----adam 2、- -from-file（文件）： 12345678[root@master ~]# touch adam &gt; username[root@master ~]# touch 18 &gt; age[root@master ~]# kubectl create configmap myconfigmap2 --from-file=username --from-file=age configmap/myconfigmap2 created[root@master ~]# kubectl get configmaps NAME DATA AGEmyconfigmap1 2 4m23smyconfigmap2 2 63s 3、通过- -from-env-file： 12345678910[root@master ~]# vim env.txtusername&#x3D;adamage&#x3D;18[root@master ~]# kubectl create configmap myconfigmap3 --from-env-file&#x3D;env.txt configmap&#x2F;myconfigmap3 created[root@master ~]# kubectl get configmaps NAME DATA AGEmyconfigmap1 2 16mmyconfigmap2 2 5m18smyconfigmap3 2 8m56s 4、通过yaml配置文件 12345678910111213141516[root@master ~]# vim configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: myconfigmap4data: username: adam age: \"18\"[root@master ~]# kubectl apply -f configmap.yaml configmap/myconfigmap4 created[root@master ~]# kubectl get configmaps NAME DATA AGEmyconfigmap1 2 16mmyconfigmap2 2 5m18smyconfigmap3 2 8m56smyconfigmap4 2 4m8s 使用configmap：和secret一样，有两种方法 第一种方法是： 以volumes挂载的方式引用资源 1234567891011121314151617181920212223[root@master ~]# vim v-pod.yamlapiVersion: v1kind: Podmetadata: name: pod1spec: containers: - name: pod1 image: busybox args: - bin/sh - -c - sleep 300000 volumeMounts: - name: cmp-test mountPath: \"/etc/cmp-test\" readOnly: true volumes: - name: cmp-test configMap: name: myconfigmap1[root@master ~]# kubectl apply -f v-pod.yaml pod/pod1 created 第二种方式： 以环境变量的方式引用资源 123456789101112131415161718192021222324252627282930[root@master ~]# vim e-pod.yaml apiVersion: v1kind: Podmetadata: name: pod2spec: containers: - name: pod2 image: busybox args: - bin/sh - -c - sleep 300000 env: - name: CONFIGMAP_NAME valueFrom: configMapKeyRef: name: myconfigmap2 key: username - name: CONFIGMAP_AGE valueFrom: configMapKeyRef: name: myconfigmap2 key: age[root@master ~]# kubectl apply -f e-pod.yaml pod/pod2 created[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEpod1 1/1 Running 0 5m34spod2 1/1 Running 0 48s 如果现在将confgimap资源内保存的数据进行更新，使用此数据的应用内，数据是否也会更新 123456789[root@master ~]# kubectl edit configmaps myconfigmap1data: age: \"18\" username: root[root@master ~]# kubectl exec -it pod1 /bin/sh/ # cd /etc/cmp-test//etc/cmp-test # cat username root//这里会跟着更新，如果操作过快，它会反应不过来，你就话看不到改变，稍微等一下就行了 那么，以环境变量的方式引用的数据，是否会实时更新？ 1234567891011121314151617181920212223242526[root@master ~]# kubectl exec -it pod2 /bin/sh/ # echo $CONFIGMAP_AGE18/ # echo $CONFIGMAP_NAMEadam/ # exit[root@master ~]# kubectl edit configmaps myconfigmap2data: age: | 18 username: | root[root@master ~]# kubectl describe configmaps myconfigmap2Data====age:----18username:----root[root@master ~]# kubectl exec -it pod2 /bin/sh/ # echo $CONFIGMAP_NAMEadam//和secret一样，是不会更新的 小结Secret：用于存放一些敏感信息，比如数据库的用户名密码、密钥等，以密文的方式保存 创建Secret资源对象的四种方式： –from-literal（文字的）：需要保存什么内容直接写出来，一次只能保存一条 –from-file（文件）：把需要保存的内容写到文件里面，通过–from-file指定这个文件。一次只能保存一条 –from-env-file（环境变量）：把想要保存的内容都写入一个文件里面，通过–from-env-file指定 通过yaml配置文件：在data字段写入要保存的内容，注意是以密文的格式写入（使用base64的方式加密就行） 引用Secret资源的两种方法： Volumes挂载的方式： 环境变量的方式： ConfigMap：和Secret一样，拥有大多数共同的特性，但是区别是，configMap保存的是一些不太重要的信息，它保存的数据是以明文的方式存放的，使用describe来查看是，能看到真正的信息 创建ConfigMap资源对象的四种方式和引用ConfigMap的两种方式一摸一样 –from-literal（文字的） –from-file（文件） –from-env-file（环境变量） 通过yaml配置文件 Volumes挂载的方式： 环境变量的方式： 如果引用secret、CongigMap数据的应用，要求会随着secret、ConfigMap资源对象内保存的数据的更新而实时更新，那么应该使用volumes挂载的方式引用资源。因为用环境变量的方式引用不会实时更新数据","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"StatefulSet","slug":"StatefulSet","date":"2020-02-11T16:00:00.000Z","updated":"2020-02-14T03:25:48.428Z","comments":true,"path":"StatefulSet.html","link":"","permalink":"https://pdxblog.top/StatefulSet.html","excerpt":"","text":"StatefulSetRC、RS、Deployment、DS（DaemonSet）这些Pod控制器都是面向无状态的服务，它们所管理的Pod的IP、名字、启停顺序等都是随机的 这些Pod控制器都有一个相同点 ​ template（模板）：根据模板创建出来的Pod，它们的状态都是一摸一样的（除了名称、IP、域名之外） ​ 可以理解为：任何一个Pod都可以被删除，然后用新生成的Pod进行替换 StatefulSet： 顾名思义：有状态的集合，管理所有有状态的服务，比如MySQL、MongoDB集群等 它之前的名字是：PetSet Pet：宠物 把之前按无状态的服务比喻为牛、羊等牲畜。把有状态的服务比喻为：宠物 StatefulSet本质上是Deployment的一种变体，在v1.9版本中已成为GA版本，它为了解决有状态服务的问题，它所管理的Pod拥有固定的Pod名称，启停顺序，在StatefulSet中，Pod名字称为网络标识(hostname)，还必须要用到共享存储 有状态的服务：后端生成的每一个Pod都具有自己的唯一性，不可随意被删除 需要记录前一次或者多次通信中的相关事件，以作为下一次通信的分类标准。比如：mysql等数据库服务。（Pod的名称不能随意变化，数据持久化的目录也是不一样的，每一个Pod都有自己独有的数据持久化存储目录） 一个小实例： 12345678910111213141516171819202122232425262728293031323334353637383940[root@master ~]# vim statefulset.yamlapiVersion: v1kind: Servicemetadata: name: headless-svc labels: app: headless-svcspec: ports: - port: 80 selector: app: headless-pod clusterIP: None---apiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-testspec: serviceName: headless-svc replicas: 3 selector: matchLabels: app: headless-pod template: metadata: labels: app: headless-pod spec: containers: - name: myhttpd image: httpd ports: - containerPort: 80[root@master ~]# kubectl apply -f statefulset.yaml service/headless-svc createdstatefulset.apps/statefulset-test created[root@master ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEheadless-svc ClusterIP None &lt;none&gt; 80/TCP 4m Deployment控制的pod的名称由来： ​ Deployment+RS+随机字符串（Pod的名称），没有顺序的额，可以被随意替代 StategulSet的三个组成部分： 1、headless-svc：无头服务。因为没有IP地址，所以它不具备负载均衡的功能了 作用：为后端的每一个Pod去命名 因为statefulset要求Pod的名称是有顺序的，每一个Pod都不能被随意取代，也就是说即使Pod重建之后，名称依然不变 12345[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEstatefulset-test-0 1/1 Running 0 23mstatefulset-test-1 1/1 Running 0 22mstatefulset-test-2 1/1 Running 0 22m 1234567[root@master ~]# kubectl delete pod statefulset-test-0[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEstatefulset-test-0 0/1 ContainerCreating 0 6sstatefulset-test-1 1/1 Running 0 25mstatefulset-test-2 1/1 Running 0 25m//Pod重建之后名称没有发生变化 2、statefulset：定义具体的应用 3、volumeClaimTeplates：自动创建PVC，为后端的Pod提供专有的存储 存储卷申请模板，创建PVC，指定pvc名称大小，将自动创建pvc，且pvc必须由存储类供应 为什么需要 headless service 无头服务？在用Deployment时，每一个Pod名称是没有顺序的，是随机字符串，因此是Pod名称是无序的，但是在statefulset中要求必须是有序 ，每一个pod不能被随意取代，pod重建后pod名称还是一样的。而pod IP是变化的，所以是以Pod名称来识别。pod名称是pod唯一性的标识符，必须持久稳定有效。这时候要用到无头服务，它可以给每个Pod一个唯一的名称 。为什么需要volumeClaimTemplate？对于有状态的副本集都会用到持久存储，对于分布式系统来讲，它的最大特点是数据是不一样的，所以各个节点不能使用同一存储卷，每个节点有自已的专用存储，但是如果在Deployment中的Pod template里定义的存储卷，是所有副本集共用一个存储卷，数据是相同的，因为是基于模板来的 ，而statefulset中每个Pod都要自已的专有存储卷，所以statefulset的存储卷就不能再用Pod模板来创建了，于是statefulSet使用volumeClaimTemplate，称为卷申请模板，它会为每个Pod生成不同的pvc，并绑定pv， 从而实现各pod有专用存储。这就是为什么要用volumeClaimTemplate的原因 每一个pod—&gt;对应一个pvc—-&gt;每一个pvc对应一个pv ​ storageclass：自动创建PV ​ 需要解决：自动创建PVC—–&gt;volumeClaimTeplates 一、创建StorageClass资源对象 ​ 1、基于NFS服务，创建NFS服务 123[root@master ~]# showmount -eExport list for master:/nfsdata * ​ 2、创建rbac权限 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@master ~]# vim rbac-rolebind.yamlapiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runnerrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: default //这个字段必须要写，不然会报错roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io[root@master ~]# kubectl apply -f rbac-rolebind.yaml serviceaccount/nfs-provisioner unchangedclusterrole.rbac.authorization.k8s.io/nfs-provisioner-runner unchangedclusterrolebinding.rbac.authorization.k8s.io/run-nfs-provisioner created ​ 3、创建Deployment资源对象，用Pod代替真正的NFS服务 1234567891011121314151617181920212223242526272829303132333435363738[root@master ~]# vim nfs-deployment.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: bdqn - name: NFS_SERVER value: 192.168.1.70 - name: NFS_PATH value: /nfsdata volumes: - name: nfs-client-root nfs: server: 192.168.1.70 path: /nfsdata[root@master ~]# kubectl apply -f nfs-deployment.yaml deployment.extensions/nfs-client-provisioner created[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEnfs-client-provisioner-f6cb6688b-5zn8d 1/1 Running 0 7s ​ 4、创建storaclass 123456789101112[root@master ~]# vim test-storageclass.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: sc-nfsprovisioner: bdqnreclaimPolicy: Retain[root@master ~]# kubectl apply -f test-storageclass.yaml storageclass.storage.k8s.io/sc-nfs created[root@master ~]# kubectl get scNAME PROVISIONER AGEsc-nfs bdqn 10s 二、解决自动创建PVC 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@master ~]# vim statefulset.yamlapiVersion: v1kind: Servicemetadata: name: headless-svc labels: app: headless-svcspec: ports: - port: 80 name: myweb selector: app: headless-pod clusterIP: None---apiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-testspec: serviceName: headless-svc replicas: 3 selector: matchLabels: app: headless-pod template: metadata: labels: app: headless-pod spec: containers: - image: httpd name: myhttpd ports: - containerPort: 80 name: httpd volumeMounts: - mountPath: /mnt name: test volumeClaimTemplates: //自动的创建PVC - metadata: name: test annotations: //这是指定storageclass volume.beta.kubernetes.io/storage-class: sc-nfs spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi[root@master ~]# kubectl apply -f statefulset.yaml service/headless-svc createdstatefulset.apps/statefulset-test created[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEnfs-client-provisioner-f6cb6688b-5zn8d 1/1 Running 0 13mstatefulset-test-0 1/1 Running 0 23sstatefulset-test-1 1/1 Running 0 16sstatefulset-test-2 1/1 Running 0 9s 注意： ​ 如果生成的Pod，第一个出现了问题，后面的都不会生成 根据volumeClaimTemplates自动创建的PVC： 12345[root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-statefulset-test-0 Bound pvc-3a105a9d-5892-4080-a993-20fd2540cd3e 100Mi RWO sc-nfs 46mtest-statefulset-test-1 Bound pvc-123bf53d-a72b-4bfa-a901-bb98efcda056 100Mi RWO sc-nfs 45mtest-statefulset-test-2 Bound pvc-8529c668-5b38-4024-93af-b18fa238b0ba 100Mi RWO sc-nfs 45m 如果集群中没有StorageClass的动态供应PVC的机制，也可以提前手动创建多个PV、PVC，手动创建的PVC名称必须符合之后创建的StatefulSet命名规则：(volumeClaimTemplates.name)-(pod_name) Statefulset名称为statefulset-test 三个Pod副本: statefulset-test-0，statefulset-test-1，statefulset-test-2 volumeClaimTemplates名称为：test 那么自动创建出来的PVC名称为test-statefulset-test-[0-2]，为每个Pod创建一个PVC 规律总结： 匹配Pod name(网络标识)的模式为：$(statefulset名称)-$(序号)，比如上面的示例：statefulset-test-0，statefulset-test-1，statefulset-test-2。 StatefulSet为每个Pod副本创建了一个DNS域名，这个域名的格式为： $(podname).(headless server name)，也就意味着服务间是通过Pod域名来通信而非Pod IP，因为当Pod所在Node发生故障时，Pod会被飘移到其它Node上，Pod IP会发生变化，但是Pod域名不会有变化。 StatefulSet使用Headless服务来控制Pod的域名，这个域名的FQDN为：$(service name).$(namespace).svc.cluster.local，其中，“cluster.local”指的是集群的域名。 根据volumeClaimTemplates，为每个Pod创建一个pvc，pvc的命名规则匹配模式：(volumeClaimTemplates.name)-(pod_name)，比如上面的volumeMounts.name=test， Pod name=statefulset-test-[0-2]，因此创建出来的PVC是test-statefulset-test-0，test-statefulset-test-1，test-statefulset-test-2 删除Pod不会删除其pvc，手动删除pvc将自动释放pv。关于Cluster Domain、headless service名称、StatefulSet 名称如何影响StatefulSet的Pod的 StatefulSet的启停顺序： 有序部署：部署StatefulSet时，如果有多个Pod副本，它们会被顺序地创建（从0到N-1）并且，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态。 有序删除：当Pod被删除时，它们被终止的顺序是从N-1到0。 有序扩展：当对Pod执行扩展操作时，与部署一样，它前面的Pod必须都处于Running和Ready状态 StatefulSet Pod管理策略： 在v1.7以后，通过允许修改Pod排序策略，同时通过.spec.podManagementPolicy字段确保其身份的唯一性。 OrderedReady：上述的启停顺序，默认设置。 Parallel：告诉StatefulSet控制器并行启动或终止所有Pod，并且在启动或终止另一个Pod之前不等待前一个Pod变为Running and Ready或完全终止 StatefulSet使用场景： 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现 有序收缩，有序删除（即从N-1到0） 更新策略： 在Kubernetes 1.7及更高版本中，通过.spec.updateStrategy字段允许配置或禁用Pod、labels、source request/limits、annotations自动滚动更新功能。 OnDelete：通过.spec.updateStrategy.type 字段设置为OnDelete，StatefulSet控制器不会自动更新StatefulSet中的Pod。用户必须手动删除Pod，以使控制器创建新的Pod。 RollingUpdate：通过.spec.updateStrategy.type 字段设置为RollingUpdate，实现了Pod的自动滚动更新，如果.spec.updateStrategy未指定，则此为默认策略。StatefulSet控制器将删除并重新创建StatefulSet中的每个Pod。它将以Pod终止（从最大序数到最小序数）的顺序进行，一次更新每个Pod。在更新下一个Pod之前，必须等待这个Pod Running and Ready。 Partitions：通过指定 .spec.updateStrategy.rollingUpdate.partition 来对 RollingUpdate 更新策略进行分区，如果指定了分区，则当 StatefulSet 的 .spec.template 更新时，具有大于或等于分区序数的所有 Pod 将被更新。 具有小于分区的序数的所有 Pod 将不会被更新，即使删除它们也将被重新创建。如果 StatefulSet 的 .spec.updateStrategy.rollingUpdate.partition 大于其 .spec.replicas，则其 .spec.template 的更新将不会传播到 Pod。在大多数情况下，不需要使用分区 StatefulSet注意事项： 还在beta状态，需要kubernetes v1.5版本以上才支持 所有Pod的Volume必须使用PersistentVolume或者是管理员事先创建好 为了保证数据安全，删除StatefulSet时不会删除Volume StatefulSet需要一个Headless Service来定义DNS domain，需要在StatefulSet之前创建好 目前StatefulSet还没有feature complete，比如更新操作还需要手动patch 更多可以参考：https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/ 进入容器，验证持久化是否成功 123456789101112[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEnfs-client-provisioner-f6cb6688b-5zn8d 1/1 Running 0 30mstatefulset-test-0 1/1 Running 0 17mstatefulset-test-1 1/1 Running 0 16mstatefulset-test-2 1/1 Running 0 16m[root@master ~]# kubectl exec -it statefulset-test-0 /bin/sh# cd /mnt# touch testfile# exit[root@master ~]# ls /nfsdata/default-test-statefulset-test-0-pvc-3a105a9d-5892-4080-a993-20fd2540cd3e/testfile 小结Deployment、RC、RS、DS： 这些Pod控制器都是面向无状态的服务，他们管理的Pod的IP、名字、启停顺序等都是随机的 这些根据模板创建出来的Pod，特们的状态都是一摸一样的（除了名称、IP、域名之外） 任何一个Pod都可以被删除，然后用因生成的Pod进项替换 StatefulSet： 顾名思义：有状态的集合，管理所有的有状态服务，比如MySQL集群等 后端生成的每一个Pod都具有自己的唯一性，不可被随意删除 需要记录前一次或者多次通信中的相关事件，以作为下一次通信的分类标准 Pod的名称不能随意变化，数据持久化的目录也是不一样的，每一个Pod又都自己独有的数据持久化存储目录 扩容、缩容：在此过程中，Pod的生成或删除操作也是有顺序性的 12345678[root@master ~]# kubectl get pod -n zhbNAME READY STATUS RESTARTS AGEnfs-client-provisioner-f6cb6688b-x2zls 1/1 Running 1 42hstatefulset-test-0 1/1 Running 1 42hstatefulset-test-1 1/1 Running 1 42hstatefulset-test-2 1/1 Running 1 42hstatefulset-test-3 1/1 Running 0 76sstatefulset-test-4 1/1 Running 0 66s 升级操作： 1[root@master ~]# kubectl explain sts.spec.updateStrategy.rollingUpdate.partition partition：如果partition后面的值等于N，N+的都会更新，默认值为0（所有都会更新） 如果N等于2，那么它会从statefulset-test-2开始更新，以此类推 statefulset-test-0statefulset-test-1statefulset-test-2statefulset-test-3statefulset-test-4","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"k8s的存储类","slug":"k8s的存储类","date":"2020-02-10T16:00:00.000Z","updated":"2020-02-12T07:04:51.200Z","comments":true,"path":"k8s的存储类.html","link":"","permalink":"https://pdxblog.top/k8s%E7%9A%84%E5%AD%98%E5%82%A8%E7%B1%BB.html","excerpt":"","text":"k8s存储类如果，k8s集群中，有很多类似的PV，PVC在去向PV申请空间的时候，不仅会考虑名称以及访问控制模式，还会考虑你申请空间的大小，会分配给你最合适大小的PV 运行一个web服务，采用Deployment资源，基于nginx镜像。数据持久化目录为nginx服务的主访问目录：/usr/share/nginx/html 创建一个PVC，与上述资源进行关联 ​ 先创建两个PV：web-pv1（1G），web-pv2（2G） 123456789101112131415161718192021[root@master ~]# vim web1.yamlapiVersion: v1kind: PersistentVolumemetadata: name: web-pv1spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/web1 server: 192.168.1.70[root@master ~]# mkdir /nfsdata/web1[root@master ~]# kubectl apply -f web1.yaml persistentvolume/web-pv1 created[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEweb-pv1 1Gi RWO Recycle Available nfs 6s 1234567891011121314151617181920[root@master ~]# vim web2.yamlapiVersion: v1kind: PersistentVolumemetadata: name: web-pv2spec: capacity: storage: 2Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfsdata/web2 server: 192.168.1.70[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEweb-pv1 1Gi RWO Recycle Available nfs 97sweb-pv2 2Gi RWO Recycle Available nfs 6s[root@master ~]# mkdir /nfsdata/web2 123456789101112131415161718192021[root@master ~]# vim web.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: test-webspec: template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx volumeMounts: - name: test-web mountPath: /usr/share/nginx/html volumes: - name: test-web persistentVolumeClaim: claimName: web-pvc 123456789101112131415161718192021222324[root@master ~]# vim web-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: web-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs[root@master ~]# kubectl apply -f web-pvc.yaml[root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEweb-pvc Bound web-pv1 1Gi RWO nfs 6s[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEweb-pv1 1Gi RWO Recycle Bound default&#x2F;web-pvc nfs 9m8sweb-pv2 2Gi RWO Recycle Available nfs 7m37s[root@master ~]# kubectl apply -f web.yaml[root@master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-web-67989b6d78-2b774 1&#x2F;1 Running 0 2m1s 10.244.2.5 node02 &lt;none&gt; &lt;none&gt; 如果名称和访问模式都一样，它会考虑空间大小进行分配，分配比较接近的PV进行关联 12345678[root@master ~]# kubectl exec -it test-web-67989b6d78-2b774 /bin/bashroot@test-web-67989b6d78-2b774:/# cd /usr/share/nginx/html/root@test-web-67989b6d78-2b774:/usr/share/nginx/html# echo 12345 &gt; index.htmlroot@test-web-67989b6d78-2b774:/usr/share/nginx/html# exitexitcommand terminated with exit code 127[root@master ~]# curl 10.244.2.512345 很多的服务，很多的资源对象 ​ 如果要去创建服务，做数据持久化，需要预先知道可用PV有哪些？ ​ 如果为了这个服务去提前创建PV，那么我们还需要知道，这个服务大概需要多大的空间？ Storage Class（存储类）：它可以动态的自动的创建所需要的 PV PV是运维人员来创建的，开发操作PVC，可是大规模集群中可能会有很多PV，如果这些PV都需要运维手动来处理这也是一件很繁琐的事情，所以就有了动态供给概念，也就是Dynamic Provisioning。而我们上面的创建的PV都是静态供给方式，也就是Static Provisioning。而动态供给的关键就是StorageClass，它的作用就是创建PV模板。 创建StorageClass里面需要定义PV属性比如存储类型、大小等；另外创建这种PV需要用到存储插件。最终效果是，用户提交PVC，里面指定存储类型，如果符合我们定义的StorageClass，则会为其自动创建PV并进行绑定 存储类（Storage class）是k8s资源类型的一种，它是有管理员为管理PV更加方便创建的一个逻辑组，可以按照存储系统的性能高低，或者综合服务质量，备份策略等分类。不过k8s本身不知道类别到底是什么，它这是作为一个描述 Provisioner（供给方、提供者）： 及提供了存储资源的存储系统。k8s内建有多重供给方，这些供给方的名字都以“kubernetes.io”为前缀。并且还可以自定义 Parmeters（参数）： 存储类使用参数描述要关联到的存储卷，注意不同的供给方参数也不同 ReclaimPolicy： PV的回收策略，可用值有Delete(默认)和Retain 基于StorageClass的动态存储供应整体过程如下图所示： 1）集群管理员预先创建存储类（StorageClass）； 2）用户创建使用存储类的持久化存储声明(PVC：PersistentVolumeClaim)； 3）存储持久化声明通知系统，它需要一个持久化存储(PV: PersistentVolume)； 4）系统读取存储类的信息； 5）系统基于存储类的信息，在后台自动创建PVC需要的PV； 6）用户创建一个使用PVC的Pod； 7）Pod中的应用通过PVC进行数据的持久化； 8）而PVC使用PV进行数据的最终持久化处理。 更多可以参考：https://www.kubernetes.org.cn/4078.html 1）确定基于NFS服务来做的sc，NFS服务需要开启 123[root@master ~]# showmount -eExport list for master:/nfsdata * 2）需要RBAC权限 RBAC： rbac是k8s的API安全策略，是基于用户的访问权限 规定了谁可以有什么样的权限 为了给SC资源操作k8s集群的权限 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@master ~]# vim rbac-rolebind.yamlkind: NamespaceapiVersion: v1metadata: name: bdqn-test---apiVersion: v1kind: ServiceAccountmetadata: name: nfs-provisioner namespace: bdqn-test---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: nfs-provisioner-runner namespace: bdqn-testrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\",\"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-provisionersubjects: - kind: ServiceAccount name: nfs-provisioner namespace: bdqn-testroleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io[root@master ~]# kubectl apply -f rbac-rilebind.yaml namespace/bdqn-test createdserviceaccount/nfs-provisioner createdclusterrole.rbac.authorization.k8s.io/nfs-provisioner-runner createdclusterrolebinding.rbac.authorization.k8s.io/run-nfs-provisioner created 3）nfs-deployment. 作用：其实它是NFS的客户端，但是它通过K8S的内置的NFS驱动挂载远端的NFS服务器到本地目录；然后将自身作为storage provider，关联storage class 123456789101112131415161718192021222324252627282930313233343536[root@master ~]# vim nfs-deployment.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nfs-client-provisioner namespace: bdqn-testspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME //提供者的名称 value: bdqn-test - name: NFS_SERVER //nfs服务器地址 value: 192.168.1.70 - name: NFS_PATH value: /nfsdata volumes: - name: nfs-client-root nfs: server: 192.168.1.70 path: /nfsdata[root@master ~]# kubectl apply -f nfs-deployment.yaml deployment.extensions/nfs-client-provisioner created 4）创建storageclass 12345678910[root@master ~]# vim test-storageclass.yamlapiVersion: storage.k8s.io&#x2F;v1kind: StorageClassmetadata: name: sc-nfs namespace: bdqn-test &#x2F;&#x2F;属于哪个名称空间provisioner: bdqn-test &#x2F;&#x2F;供给着，和nfs-deployment的名称要一样：value: bdqn-testreclaimPolicy: Retain[root@master ~]# kubectl apply -f test-storageclass.yaml storageclass.storage.k8s.io&#x2F;sc-nfs created provisioner: bdqn-test //通过preovisioner字段关联到上述Deployment 5）创建PVC 12345678910111213141516171819[root@master ~]# vim test-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-claim namespace: bdqn-testspec: storageClassName: sc-nfs accessModes: - ReadWriteMany resources: requests: storage: 20Mi[root@master ~]# kubectl apply -f test-pvc.yaml persistentvolumeclaim/test-claim created[root@master ~]# kubectl get pvc -n bdqn-test NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-claim Bound pvc-0c606810-9f93-441b-bc6b-391d7813dcab 20Mi RWX sc-nfs 2m15s 它会为我们自动生成一个pv 12345[root@master ~]# ls /nfsdata/bdqn-test-test-claim-pvc-0c606810-9f93-441b-bc6b-391d7813dcab[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-0c606810-9f93-441b-bc6b-391d7813dcab 20Mi RWX Delete Bound bdqn-test/test-claim sc-nfs 4m24s 6）创建Pod测试 12345678910111213141516171819202122232425262728[root@master ~]# vim test-pod.yamlkind: PodapiVersion: v1metadata: name: test-pod namespace: bdqn-testspec: containers: - name: test-pod image: busybox args: - /bin/sh - -c - sleep 30000 volumeMounts: - name: nfs-pvc mountPath: /test restartPolicy: OnFailure volumes: - name: nfs-pvc persistentVolumeClaim: claimName: test-claim[root@master ~]# kubectl apply -f test-pod.yaml pod/test-pod created[root@master ~]# kubectl get pod -n bdqn-test NAME READY STATUS RESTARTS AGEnfs-client-provisioner-57f49c99c7-lhd8n 1/1 Running 0 27mtest-pod 1/1 Running 0 32s 1234567[root@master ~]# kubectl exec -it test-pod -n bdqn-test /bin/sh/ # cd /test/test # touch test-file/test # echo 123456 &gt; test-file /test # exit[root@master ~]# cat /nfsdata/bdqn-test-test-claim-pvc-0c606810-9f93-441b-bc6b-391d7813dcab/test-file 123456 1234567[root@master ~]# kubectl exec -it -n bdqn-test nfs-client-provisioner-57f49c99c7-lhd8n /bin/sh/ # ls /persistentvolumes / # cd /persistentvolumes//persistentvolumes # lsbdqn-test-test-claim-pvc-0c606810-9f93-441b-bc6b-391d7813dcab#这个目录和/nfsdata下面的目录一样","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"k8s数据持久化","slug":"k8s数据持久化","date":"2020-02-10T08:53:18.320Z","updated":"2020-02-14T03:50:17.919Z","comments":true,"path":"k8s数据持久化.html","link":"","permalink":"https://pdxblog.top/k8s%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96.html","excerpt":"","text":"k8s数据持久化Docker容器是有生命周期的，因此数据卷可以实现数据持久化 数据卷主要解决的问题： 数据持久性：当我们写入数据时，文件都是暂时性的存在，当容器崩溃后，host就会将这个容器杀死，然后重新从镜像创建容器，数据就会丢失 数据共享：在同一个Pod中运行容器，会存在共享文件的需求 Volume：emptyDir（空目录）：使用情况比较少，一般只做临时使用，类似Docker数据 持久化的：docker manager volume，该数据卷初始分配时，是一个空目录，同一个Pod中的容器可以对该目录有执行读写操作，并且共享数据 ​ 使用场景：在同一个Pod里，不同的容器，共享数据卷 ​ 如果容器被删除，数据仍然存在，如果Pod被删除，数据也会被删除 使用实例： 123456789101112131415161718192021222324252627282930313233343536[root@master ~]# vim emptyDir.yamlapiVersion: v1kind: Podmetadata: name: producer-consumerspec: containers: - image: busybox name: producer volumeMounts: - mountPath: /producer_dir //容器内的路径 name: shared-volume //指定本地的目录名 args: - /bin/sh - -c - echo \"hello k8s\" &gt; /producer_dir/hello; sleep 30000 - image: busybox name: consumer volumeMounts: - mountPath: /consumer_dir name: shared-volume args: - /bin/sh - -c - cat /consumer_dir/hello; sleep 30000 volumes: - name: shared-volume //这里的名字必须与上面的Pod的mountPath的name相对应 emptyDir: &#123;&#125; //定义数据持久化类型，即表示空目录[root@master ~]# kubectl apply -f emptyDir.yaml[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGE producer-consumer 2/2 Running 0 14s[root@master ~]# kubectl logs producer-consumer consumer hello k8s 使用inspect查看挂载的目录在哪（查看Mount字段） 123456789101112131415161718192021222324252627282930313233[root@master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESproducer-consumer 2/2 Running 0 69s 10.244.1.2 node01 &lt;none&gt; &lt;none&gt;//可以看到容器运行在node01上，在node01上找到这个容器并查看并查看详细信息[root@node01 ~]# docker psCONTAINER ID IMAGEf117beb235cf busybox13c7a18109a1 busybox[root@node01 ~]# docker inspect 13c7a18109a1 \"Mounts\": [ &#123; \"Type\": \"bind\", \"Source\": \"/var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volume\", \"Destination\": \"/producer_dir\", //容器内的挂载目录 \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\"//再查看另一个容器[root@node01 ~]# docker inspect f117beb235cf \"Mounts\": [ &#123; \"Type\": \"bind\", \"Source\": \"/var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volume\", \"Destination\": \"/consumer_dir\", //容器内的挂载目录 \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\"//可以看到两个容器使用的同一个挂载目录[root@node01 ~]# cd /var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volume[root@node01 shared-volume]# lshello[root@node01 shared-volume]# cat hello hello k8s 将容器删除，验证目录是否存在 1234567[root@node01 ~]# docker rm -f 13c7a18109a1 13c7a18109a1[root@node01 ~]# docker psCONTAINER ID IMAGEa809717b1aa5 busyboxf117beb235cf busybox//它会重新生成一个新的容器，来达到我们用户所期望的状态，所以这个目录还是存在的 删除Pod 1234[root@master ~]# kubectl delete pod producer-consumer[root@master ~]# ls /var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volumels: 无法访问/var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volume: 没有那个文件或目录//Pod删除后数据也会被删除 hostPath Volume（使用场景也比较少）：类似Docker数据持久化的：bind mount 将Pod所在节点的文件系统上某一个文件或目录挂载进容器内 ​ 如果Pod被删除，数据会保留，相比较emptyDir会好一点，不过，一旦host崩溃，hostPath也无法访问 docker或者k8s集群本身的存储会采用hostPath这种方式 k8s集群中会有很多pod，如果都是用hostPath Volume的话管理起来很不方便，所以就用到了PV Persistent Volume | PV（持久卷）提前做好的，数据持久化的数据存放目录 是集群中的一块存储空间，由集群管理员管理或者由Storage class（存储类）自动管理，PV和pod、deployment、Service一样，都是一个资源对象 PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 该API对象捕获存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统 Psesistent Volume Claim | PVC（持久卷使用声明|申请） PVC代表用户使用存储的请求，应用申请PV持久化空间的一个申请、声明。K8s集群可能会有多个PV，你需要不停的为不同的应用创建多个PV 它类似于pod。Pod消耗节点资源，PVC消耗存储资源。 pod可以请求特定级别的资源（CPU和内存）。 权限要求可以请求特定的大小和访问模式 更多可以参考：https://www.kubernetes.org.cn/pvpvcstorageclass 基于NFS服务来做的PV 12345678910[root@master ~]# yum -y install nfs-utils (需要节点全部下载，会报挂载类型错误)[root@master ~]# yum -y install rpcbind[root@master ~]# mkdir &#x2F;nfsdata[root@master ~]# vim &#x2F;etc&#x2F;exports&#x2F;nfsdata *(rw,sync,no_root_squash)[root@master ~]# systemctl start rpcbind[root@master ~]# systemctl start nfs-server[root@master ~]# showmount -eExport list for master:&#x2F;nfsdata * 1.创建PV（实际的存储目录） 2.创建PVC 3.创建pod 创建PV资源对象： 12345678910111213141516171819[root@master ~]# vim nfs-pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: test-pvspec: capacity: //PV容量的大小 storage: 1Gi accessModes: //PV支持的访问模式 - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle //PV的存储空间的回收策略是什么 storageClassName: nfs nfs: path: /nfsdata/pv1 server: 192.168.1.70[root@master ~]# kubectl apply -f nfs-pv.yaml[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtest-pv 1Gi RWO Recycle Available nfs 9m30s accessModes: （PV支持的访问模式） ​ - ReadWriteOnce：能以读-写的方式mount到单个节点 ​ - ReadWriteMany：能以读-写的方式mount到多个节点 ​ - ReadOnlyMany：能以只读的方式mount到多个节点 persistentVolumeReclaimPolicy：（PV的存储空间的回收策略是什么） ​ Recycle：自动清除数据 ​ Retain：需要管理员手动回收 ​ Delete：云存储专用。直接删除数据 PV和PVC相互的关联：通过的是storageClassName &amp;&amp; accessModes 创建PVC 12345678910111213141516[root@master ~]# vim nfs-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-pvcspec: accessModes: //访问模式 - ReadWriteOnce resources: requests: storage: 1Gi //申请的容量大小 storageClassName: nfs //向哪个PV申请[root@master ~]# kubectl apply -f nfs-pvc.yaml[root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-pvc Bound test-pv 1Gi RWO nfs 14s PV的应用：创建一个Pod资源： 123456789101112131415161718192021[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-podspec: containers: - name: pod1 image: busybox args: - /bin/sh - -c - sleep 30000 volumeMounts: - mountPath: \"/mydata\" name: mydata volumes: - name: mydata persistentVolumeClaim: claimName: test-pvc[root@master ~]# kubectl apply -f pod.yaml 之前创建PV的时候指定的挂载目录是/nfsdata/pv1，我们并没有创建pv1这个目录，所以这个pod是运行不成功的。 以下是排错方法： kubectl describe kubectl logs /var/log/messages 查看该节点的kubelet的日志 123//使用kubectl describe[root@master ~]# kubectl describe pod test-podmount.nfs: mounting 192.168.1.70:/nfsdata/pv1 failed, reason given by server: No such file or directory //提示没有文件或目录 创建目录，再查看pod状态： 123[root@master ~]# mkdir /nfsdata/pv1NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-pod 1/1 Running 0 12m 10.244.1.3 node01 &lt;none&gt; &lt;none&gt; 验证是否应用成功： 123456[root@master ~]# kubectl exec test-pod touch /mydata/hello[root@master ~]# ls /nfsdata/pv1/hello[root@master ~]# echo 123 &gt; /nfsdata/pv1/hello [root@master ~]# kubectl exec test-pod cat /mydata/hello123 删除Pod，验证回收策略（Recycle）： 12345678[root@master ~]# kubectl delete pod test-pod[root@master ~]# kubectl delete pvc test-pvc[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtest-pv 1Gi RWO Recycle Available nfs 42h[root@master ~]# ls &#x2F;nfsdata&#x2F;pv1&#x2F;[root@master ~]#&#x2F;&#x2F;验证成功，数据已经回收 通常情况下不会设置为自动删除，不然就和emptyDir就差不多了 删除pv，修改回收策略： 之前是先创建PV—&gt;PVC—&gt;Pod，现在调整一下，先创建PV—&gt;—Pod—&gt;PVC 12345678910111213141516171819202122[root@master ~]# vim nfs-pv.yaml persistentVolumeReclaimPolicy: Retain[root@master ~]# kubectl apply -f nfs-pv.yaml [root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtest-pv 1Gi RWO Retain Available nfs 7s[root@master ~]# kubectl apply -f pod.yaml [root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEtest-pod 0&#x2F;1 Pending 0 5s &#x2F;&#x2F;Pending正在被调度[root@master ~]# kubectl describe pod test-podEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 41s (x2 over 41s) default-scheduler persistentvolumeclaim &quot;test-pvc&quot; not found&#x2F;&#x2F;没有发现对应的pvc创建pvc[root@master ~]# kubectl apply -f nfs-pvc.yaml[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEtest-pod 1&#x2F;1 Running 0 114s 验证Retain（管理员手动删除）回收策略： 1234567891011[root@master ~]# kubectl exec test-pod touch /mydata/k8s[root@master ~]# ls /nfsdata/pv1/k8s[root@master ~]# kubectl delete pod test-pod [root@master ~]# kubectl delete pvc test-pvc[root@master ~]# ls /nfsdata/pv1/k8s//可以看到并没有回收[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtest-pv 1Gi RWO Retain Available nfs 6s mysql对数据持久化的应用： //这里就不再创建PV，PVC了，用之前的就行 1234[root@master ~]# kubectl apply -f nfs-pvc.yaml [root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-pvc Bound test-pv 1Gi RWO nfs 7s 创建Deploment资源对象，mysql容器 123456789101112131415161718192021222324252627282930[root@master ~]# vim mysql.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: test-mysqlspec: selector: matchLabels: //基于等值的标签 app: mysql template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: 123.com volumeMounts: - name: mysql-storage mountPath: /var/lib/mysql volumes: - name: mysql-storage persistentVolumeClaim: claimName: test-pvc[root@master ~]# kubectl get deployments.NAME READY UP-TO-DATE AVAILABLE AGEtest-mysql 1/1 1 1 61s 进入容器创建数据，验证是否应用PV： 123456789101112131415161718[root@master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-mysql-569f8df4db-fnnxc 1/1 Running 0 32m 10.244.1.5 node01 &lt;none&gt; &lt;none&gt;[root@master ~]# kubectl exec -it test-mysql-569f8df4db-fnnxc -- mysql -u root -p123.commysql&gt; create database yun33; //创建数据库mysql&gt; use yun33; //选择使用数据路Database changedmysql&gt; create table my_id( id int(4)); 创建表mysql&gt; insert my_id values(9527); //在表中插入数据mysql&gt; select * from my_id; //查看表中所有数据+------+| id |+------+| 9527 |+------+1 row in set (0.00 sec)[root@master ~]# ls /nfsdata/pv1/auto.cnf ibdata1 ib_logfile0 ib_logfile1 k8s mysql performance_schema yun33 关闭node01节点，模拟节点宕机： 1234567891011121314151617[root@master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSIONmaster Ready master 36d v1.15.0node01 NotReady &lt;none&gt; 36d v1.15.0node02 Ready &lt;none&gt; 36d v1.15.0[root@master ~]# kubectl get pod -o wide -wNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-mysql-569f8df4db-fnnxc 1/1 Running 0 36m 10.244.1.5 node01 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-fnnxc 1/1 Terminating 0 38m 10.244.1.5 node01 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-2m5rd 0/1 Pending 0 0s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-2m5rd 0/1 Pending 0 0s &lt;none&gt; node02 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-2m5rd 0/1 ContainerCreating 0 0s &lt;none&gt; node02 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-2m5rd 1/1 Running 0 2s 10.244.2.4 node02 &lt;none&gt; &lt;none&gt;[root@master ~]# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-mysql-569f8df4db-2m5rd 1/1 Running 0 20s 10.244.2.4 node02 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-fnnxc 1/1 Terminating 0 38m 10.244.1.5 node01 &lt;none&gt; &lt;none&gt; 验证：在node02上新生成的pod，它内部是否有我们创建的数据 12345678910111213141516171819202122232425262728293031323334[root@master ~]# kubectl exec -it test-mysql-569f8df4db-2m5rd -- mysql -u root -p123.commysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || yun33 |+--------------------+4 rows in set (0.01 sec)mysql&gt; use yun33;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+-----------------+| Tables_in_yun33 |+-----------------+| my_id |+-----------------+1 row in set (0.01 sec)mysql&gt; select * from my_id;+------+| id |+------+| 9527 |+------+1 row in set (0.01 sec)[root@master ~]# ls /nfsdata/pv1/auto.cnf ibdata1 ib_logfile0 ib_logfile1 k8s mysql performance_schema yun33 Pod不断的重启： ​ 1.swap，没有关闭。导致集群运行不正常 ​ 2.内存不足，运行服务也会重启 小结负责把PVC绑定到PV的是一个持久化存储卷控制循环，这个控制器也是kube-manager-controller的一部分运行在master上。而真正把目录挂载到容器上的操作是在POD所在主机上发生的，所以通过kubelet来完成。而且创建PV以及PVC的绑定是在POD被调度到某一节点之后进行的，完成这些操作，POD就可以运行了。下面梳理一下挂载一个PV的过程： 用户提交一个包含PVC的POD 调度器把根据各种调度算法把该POD分配到某个节点，比如node01 Node01上的kubelet等待Volume Manager准备存储设备 PV控制器调用存储插件创建PV并与PVC进行绑定 Attach/Detach Controller或Volume Manager通过存储插件实现设备的attach。（这一步是针对块设备存储） Volume Manager等待存储设备变为可用后，挂载该设备到/var/lib/kubelet/pods//volumes/kubernetes.io~/目录上 Kubelet被告知卷已经准备好，开始启动POD，通过映射方式挂载到容器中","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"Docker swarm","slug":"Docker swarm","date":"2020-01-24T16:00:00.000Z","updated":"2020-02-02T09:31:18.463Z","comments":true,"path":"Docker swarm.html","link":"","permalink":"https://pdxblog.top/Docker%20swarm.html","excerpt":"","text":"Docker swarm docker swarm集群：三剑客之一 docker01 192.168.1.70 node1 docker02 192.168.1.50 node2 docker03 192.168.1.40 node3 关闭防火墙、禁用linux、3台dockerhost区别主机名，时间同步 1234[root@localhost ~]# setenforce 0[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# hostnamectl set-hostname node1[root@localhost ~]# su - docker版本必须是：v1.12版本开始 Swarm： 作用docker engin（引擎）的多个主机组成的集群 node： 每一个docker engin都是一个node（节点），分为manager和worker manager node： 负责执行编排和集群管理的工作，保持并维护swarm处于期望的状态，swarm可以有多个manager node，他们会自动协调并选举出一个Leader执行编排任务，但相反不能没有manager node worker node： 接受并执行由manager node派发的任务，并且默认manager node也是一个worker node，不过可以将它设置为manager-noly node，让他只负责编排和管理工作 service： 用来定义worker上执行的命令 1）初始化集群 12345678[root@node1 ~]# docker swarm init --advertise-addr 192.168.1.70Swarm initialized: current node (g26pbaqiozkn99qw9ngtgncke) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-3fp7qnihbzzy8u0esfjmmdfncdud4yh628e7bey5tu8fo3cl5p-4x021jwoh1bryxpwqd4bsj8xd 192.168.1.70:2377To add a manager to this swarm, run &#39;docker swarm join-token manager&#39; and follow the instructions. –advertise-addr：指定与其他Node通信的地址 上边返回的结果告诉我们：初始化成功，并且，如果想要添加work节点运行下面的命令： 1docker swarm join --token SWMTKN-1-3fp7qnihbzzy8u0esfjmmdfncdud4yh628e7bey5tu8fo3cl5p-4x021jwoh1bryxpwqd4bsj8xd 192.168.1.70:2377 PS：这里注意，token只有24小时的有效期 如果想要添加manager节点：运行下边的命令： 1docker swarm join-token manager 当两个节点加入成功之后，我们可以执行docker node ls查看节点详情 12345[root@node1 ~]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONg26pbaqiozkn99qw9ngtgncke * node1 Ready Active Leader 18.09.0w11nz7pzgmhq6wn5b51g6b8ao node2 Ready Active 18.09.0zqi7od9q0v7zo2tkabnseuqdf node3 Ready Active 18.09.0 基本操作命令： docker swarm leave：申请离开一个集群，之后查看节点状态会变成down然后通过manager node将其删除 docker node rm xxx：删除某个节点 docker swarm join-token {manager|worker}：生成令牌，可以是manager身份或worker身份 docker node demote（降级）：将swarm节点的manager降级为work docker node promote（升级）：将swarm节点的work升级为manager 2）部署docker swarm集群网络 overlay：覆盖型网络 1[root@node1 ~]# docker network create -d overlay --attachable docker attacheable：这个参数必须要加，否则不能用于容器 在创建网络的时候，我们并没有部署一个存储 服务，比如consul，那是因为docker swarm自带存储 3）部署一个图形化webUI界面 12[root@node1 ~]# docker run -d -p 8080:8080 -e HOST&#x3D;192.168.1.70 -e PORT&#x3D;8080 \\&gt; -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock --name viswalizer dockersamples&#x2F;visualizer 然后通过浏览器验证：192.168.1.70:8080 如果访问网页访问不到，需要开启路由转发： 12[root@node1 ~]# echo net.ipv4.ip_forward &#x3D; 1 &gt;&gt; &#x2F;etc&#x2F;sysctl.conf [root@node1 ~]# sysctl -p ) 4）创建service（服务） 1[root@node1 ~]# docker service create --replicas 1 --network docker --name web1 -p 80:80 nginx ) PS： 如果node2或node3宕机，这些服务会自动转到节点中没有宕机的host上，并继续运行 –replicas：副本数量 大概可以理解为以个副本等于一个容器 查看service： docker service ls 查看service信息： docker service ps xxx 删除server： docker service rm xxx 设置manager node不参加工作 1[root@node1 ~]# docker node update node1 --availability drain 5)搭建私有仓库 过程：略，详情请查看看https://blog.csdn.net/weixin_45636702/article/details/104002017 6）自定义镜像 要求：基于httpd镜像，更改访问界面内容。镜像tag版本为v1，对应主机页面内容为111，2222，3333 v1，v2，v3目录下的操作一样 12345678910[root@node01 ~]# mkdir &#123;v1,v2,v3&#125;[root@node01 ~]# cd v1[root@node01 v1]# vim index.html[root@node01 v1]# cat index.html111111111111111111111111111.................[root@node01 v1]# vim Dockerfile[root@node01 v1]# cat Dockerfile FROM httpdADD index.html &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs&#x2F;index.html 7）发布一个服务，基于上述镜像 要求：副本数量为3个，服务的名称为：bdqn 1[root@node01 ~]# docker service create --replicas 3 --name bdqn -p 80:80 192.168.1.70:5000&#x2F;httpd:v1 默认的ingress网络，包括创建的自定义overlay网络，为后端真正为用户提供服务的container，提供了一个统一的入口 随机映射的端口范围：30000-32767 8）服务的扩容与缩容 1[root@node01 ~]# docker service scale bdqn&#x3D;6 扩容与缩容可以直接通过scale进行设置副本数量 9）服务的升级与回滚 1[root@node01 ~]# docker service update --image 192.168.1.70:5000&#x2F;httpd:v2 bdqn //平滑的更新 1[root@node01 ~]# docker service update --image 192.168.1.70:5000&#x2F;httpd:v3 --update-parallelism 2 --update-delay 1m bdqn PS： 默认情况下，swarm一次只更新一个副本，并且两个副本之间没有等待时间，我们可以通过 –update-parallelisnm：设置并更新的副本数量 –update-delay：指定滚动更新时间间隔 //回滚操作 1[root@node01 ~]# docker service rollback bdqn PS： docker swarm的回滚操作，默认只能回滚到上一次的操作状态，并不能连续回滚操作","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"}]},{"title":"Docker架构+Docker镜像分层+Dockerfile","slug":"Docker架构+Docker镜像分层+Dockerfile","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.224Z","comments":true,"path":"Docker架构+Docker镜像分层+Dockerfile.html","link":"","permalink":"https://pdxblog.top/Docker%E6%9E%B6%E6%9E%84+Docker%E9%95%9C%E5%83%8F%E5%88%86%E5%B1%82+Dockerfile.html","excerpt":"","text":"Docker架构：) Docker架构总结： Docker是属于C/S架构，用户是使用 Docker Client 与 Docker Daemon 建立通信，并发送请求。请求接收后，Docker server通过http协议与路由，找到相应的 Handler 来执行请求 Docker Engine 是 Docker 架构中的运行引擎，同时也 Docker 运行的核心模块。Docker Engine 执行 Docker 内部的一系列工作，每一项工作都是以一个 Job 的形式的存在 Job 的运行过程中，当需要容器镜像时，则从 Docker Registry 中下载镜像，并通过镜像管理驱动 Graphdriver 将下载镜像以 Graph 的形式存储 当需要为 Docker 创建网络环境时，通过网络管理驱动 Networkdriver 创建并配置 Docker容器网络环境 当需要限制 Docker 容器运行资源或执行用户指令等操作时，则通过 Execdriver 来完成 Libcontainer 是一项独立的容器管理包，Networkdriver 以及 Execdriver 都是通过 Libcontainer 来实现具体对容器进行的操作 Docker镜像分层：Docker的最小镜像： 1[root@localhost ~]# docker pull hello-world 123FROM scratchCORP hello&#x2F;CMD [&quot;&#x2F;hello&quot;] Dockerfile的组成： 1）FROM：scratch（抓、挠） 2）COPY：hello/ 3）CMD：[“/hello”] base镜像(基础镜像)： Centos:7镜像的dockerfile 1234567891011FROM scratch &#x2F;&#x2F;从零开始构建ADD centos-7-x86 64-docker.tar.xz &#x2F;LABEL org. label-schema. schema-version&#x3D;&quot;1.0&quot;\\org. label-schema.namem&quot;centos Base Image&quot;\\org. label-schema.vendore&quot;Centos&quot;\\org. label-schema.Ticenses&quot;GPLv2&quot; \\org. labe1-schema.build-date&quot;20190305CMD [&quot;&#x2F;bin&#x2F;bash&quot;] 1[root@localhost test]# docker build -t centos7-vim-net-tools:12-11 . ) Dockerfile镜像分层总结： 镜像是容器的基石，容器是镜像运行后的实例，当镜像运行为容器之后，对镜像的所有数据仅有只读权限，如果需要对镜像源文件进行修改或删除操作时，此时是在容器层（可写层）进行的，用到了COW（copy on write）写时复制机制 Docker镜像的缓存特性 创建一个新的Dockerfile文件 12345FROM centos:7RUN yum -y install vimRUN yum -y install net-toolsRUN yum -y install wgetCMD [&quot;&#x2F;bin&#x2F;bash&quot;] 1[root@localhost ~]# docker build -t new-centos . 1）如果在相同层，有用到相同的镜像，可以不必再去下载，可以直接使用缓存 创建一个新的Dockefile文件 12345FROM centos:7RUN yum -y install vimRUN yum -y install wgetRUN yum -y install net-toolsCMD [&quot;&#x2F;bin&#x2F;bash&quot;] 1[root@localhost test1]# docker build -t centos-new . 2）即使镜像层里的操作一样，也必须是在同一层才可以使用dockerfile的缓存特性 如果制作镜像过程中，不想使用缓存可以加–no-cache选项 3）如果前面的曾发生改变，即使后边的层操作和顺序一样，也不能使用缓存特性 Dockerfile常用指令： 1）FROM：构建镜像基于哪个镜像 例如：FROM:centos:7 2）MAINTAINER：镜像维护者姓名或邮箱 例如：MAINTAINER admin 3）RUN：构建镜像时运行的shell命令 例如： RUN [“yum”,”install”,”httpd”] RUN yum -y install httpd 4）CMD：运行容器时执行的shell命令 例如： CMD [“/bin/bash”] 5）EXPOSE：声明容器的服务端口 例如：EXPOSE 80 443 6）ENV：设置容器环境变量 例如： ENV MYSQL_ROOT_PASSWORD 123.com 7）ADD：拷贝文件或目录到镜像，如果时URL或压缩包会自动下载或解压 ADD &lt;源文件&gt;… &lt;目标目录&gt; ADD [“源文件”…”目标目录”] 8）COPY：拷贝文件或目录到镜像容器内，跟ADD相似，但不具备自动下载或解压功能 9）ENTRYPOINT：运行容器时执行的shell命令 例如： ENTRYPOINT [“/bin/bash”,”-c”,”command”] ENTRYPOINT /bin/bash -c ‘command’ 10）VOLUME：指定容器挂在点到宿主机自动生成的目录或其他容器 例如： VOLUME [“/va/lib/mysql”] 11）USER：为RUN、CMD、和ENTRYPOINT执行命令指定运行用户 12）WORKDIR：为RUN、CMD、ENTRYPOINT、COPY和ADD设置工作目录，意思为切换目录 例如： WORKDIR：/var/lib/mysql 13）HEALTHCHECK：健康检查 14）ARG：构建时指定的一些参数 例如： FROM centos:7 ARG user USER $user 注意： 1、RUN在building时运行，可以写多条 2、CMD和ENTRYPOINT在运行container时，只能写一条，如果写多条，最后一条生效 3、CMD在run时可以被COMMAND覆盖，ENTRYPOINT不会被不会被COMMAND覆盖，但可以指定–entrypoint覆盖 4、如果在Dockerfile里需要往镜像内导入文件，则此文件必须在dockerfile所在目录或子目录下 小实验： 写一个dockerfile，基于cenyos:7镜像，部署安装NGINX服务 1234567891011121314151617[root@localhost ~]# mkdir web[root@localhost ~]# mv nginx-1.14.0.tar.gz web&#x2F;[root@localhost ~]# cd web&#x2F;[root@localhost web]# vim DockerfileFROM centos:7RUN yum -y install gcc pcre pcre-devel openssl openssl-devel zlib zlib-develCOPY nginx-1.14.0.tar.gz &#x2F;RUN tar -zxf nginx-1.14.0.tar.gz -C &#x2F;usr&#x2F;srcRUN useradd -M -s &#x2F;sbin&#x2F;nologin nginxWORKDIR &#x2F;usr&#x2F;src&#x2F;nginx-1.14.0RUN .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --user&#x3D;nginx --group&#x3D;nginxRUN make &amp;&amp; make installRUN ln -s &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;* &#x2F;usr&#x2F;local&#x2F;sbinRUN nginx -tRUN nginxEXPOSE 80[root@localhost web]# docker build -t test-web . &#x2F;&#x2F;如果Dockerfile在其他路径需要加-f参数来指定Dockerfile文件路径 //如果想要保证容器运行之后，nginx服务就直接开启，不必手动开启，我们可以在命令最后加上：nginx -g “daemon off;”选项 1[root@localhost web]# docker run -itd --name testweb_2 test-web:latest nginx -g &quot;daemon off;&quot; //查看容器的IP： 1[root@localhost web]# docker inspect testweb_2","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"}]},{"title":"Docker的底层原理","slug":"Docker的底层原理","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.240Z","comments":true,"path":"Docker的底层原理.html","link":"","permalink":"https://pdxblog.top/Docker%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86.html","excerpt":"","text":"Docker底层原理如果虚拟机内服务对内核版本有要求，这个服务就不太适合用docker来实现了 Busybox：欺骗层 解耦：解除耦合、解除冲突 耦合：冲突现象 run—–&gt;Centos系统（nginx、web） 对于docker host来说这个系统仅仅是一个进程 Namespace（名称空间）： 用来隔离容器 1234[root@localhost ns]# pwd&#x2F;proc&#x2F;2971&#x2F;ns[root@localhost ns]# lsipc mnt net pid user uts ipc：共享内存、消息列队 mnt：挂载点、文件系统 net：网络栈 pid：进程编号 user：用户、组 uts：主机名、域名 //namespace六项隔离，实现了容器与宿主机、容器与容器之间的隔离 Cgroup（控制组）： 资源的限制 12[root@d9d679199f74 cgroup]# pwd&#x2F;sys&#x2F;fs&#x2F;cgroup 1[root@localhost cpu]# cat tasks PS：task这个文件内的数字，记录的是进程编号。PID 四大功能： 1）资源限制：cgroup可以对进程组使用的资源总额进行限制 2）优先级分配：通过分配的cpu时间片数量以及硬盘IO带宽大小，实际上相当于控制了进程运行的优先级别 3）资源统计：cgroup可以统计西系统资源使用量，比如cpu使用时间，内存使用量等，用于按量计费。同时还支持挂起功能，也就是说用过cgroup把所有的资源限制起来，对资源都不能使用，注意并不算是说我们的程序不能使用了，只是不能使用资源，处于挂起等待状态 4）进程控制：可以对进程组执行挂起、恢复等操作 内存限额： 容器内存包括两个部分：物理内存和swap 可以通过参数控制容器内存的使用量： -m或者–memory：设置内存的使用限额 –memory-swap：设置内存+swap的使用限额 举个例子： 如果运行一个容器，并且限制该容器最多使用200M内存和100M的swap 123[root@localhost ~]# docker run -it -m 200M --memory-swap 300M centos:7[root@5bc0e71faba3 memory]# pwd&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory //内存使用限制 12[root@5bc0e71faba3 memory]# cat memory.limit_in_bytes 209715200（字节） //内存+swap限制 12[root@5bc0e71faba3 memory]# cat memory.memsw.limit_in_bytes314572800（字节） 对比一个没有限制的容器，我们会发现，如果运行容器之后不限制内存的话，意味着没有限制 CPU使用： 通过-c或者–cpu-shares设置容器使用cpu的权重，如果 不设置默认为1024 举个例子： //没有限制：1024 1234[root@localhost ~]# docker run -it --name containerA centos:7[root@e2d88b8f8b87 &#x2F;]# cd &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu[root@e2d88b8f8b87 cpu]# cat cpu.shares 1024 //限制CPU使用权重为512： 1234[root@localhost ~]# docker run -it --name containerB -c 512 centos:7[root@f8165e07c8d7 &#x2F;]# cd &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu[root@f8165e07c8d7 cpu]# cat cpu.shares 512 容器的Block IO（磁盘的读写）： docker中可以通过设置权重，限制bps和iops的方式控制容器读写磁盘的IO bps：每秒的读写的数据量（byte per second） iops：每秒IO的次数 （io per second） 默认情况下，所有容器都能够平等的读写磁盘，也可以通过–blkio-weight参数改变容器的blockIO的优先级 –device-read-bps：显示读取某个设备的bps –device-write-bps：显示写入某个设备的bps –device-read-iops：显示读取某个设备的iops –device-write-iops：显示写入某个设备的iops //限制testA这个容器，写入/dev/sda这块磁盘的bps为30MB 1[root@localhost ~]# docker run -it --name testA --device-write-bps &#x2F;dev&#x2F;sda:30MB centos:7 //从/dev/zero输入，然后输出到test.out文件中，每次大小为1M，总共为800次，oflg=direct用来指定directIO方式写文件，这样才会使–device-write-bps生效 1[root@0e659ca3e85d &#x2F;]# time dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;test.out bs&#x3D;1M count&#x3D;800 oflag&#x3D;direct","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"}]},{"title":"Docker的基本操作命令","slug":"Docker的基本操作命令","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.240Z","comments":true,"path":"Docker的基本操作命令.html","link":"","permalink":"https://pdxblog.top/Docker%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4.html","excerpt":"","text":"Docker的基本操作命令：//查找镜像： 1[root@localhost ~]# docker search mysql &#x2F;&#x2F;默认在docker hub公共仓库进行查找 //拉取镜像，下载镜像: 1[root@localhost ~]# docker pull busybox //导出镜像到本地： 1[root@localhost ~]# docker save -o busybox.tar busybox:latest （docker save &gt; busybox.tar busybox:latest） //查看本地镜像： 1[root@localhost ~]# docker images （docker image ls） PS：虽然我们查看到镜像标签位latest（最新的），但并不表示它一定是最新的，而且镜像如果没有写标签，默认是以latest为标签 //删除镜像： 1[root@localhost ~]# docker rmi busybox:latest //根据本地镜像包导入镜像： 1[root@localhost ~]# docker load -i busybox.tar （docker load &lt; busybox.tar ） //查看容器–正在运行的： 1[root@localhost ~]# docker ps //查看所有的容器： 1[root@localhost ~]# docker ps -a //删除容器： 1[root@localhost ~]# docker rm centos [CONTAINER ID&#x2F;容器名称] //停止容器运行： 1[root@localhost ~]# docker stop centos //启动容器: 1[root@localhost ~]# docker start centos PS:开启容器后记得去验证一下容器是否开启 //强制删除容器： 1[root@localhost ~]# docker rm centos -f //强制删除所有容器（生产环境严禁使用）： 1[root@localhost ~]# docker ps -a -q | xargs docker rm -f -——————————————————————————————— 123[root@localhost ~]# docker ps -a -q | xargs docker start -f &#x2F;&#x2F;开启所有容器[root@localhost ~]# docker ps -a -q | xargs docker stop -f &#x2F;&#x2F;关闭所有容器 //重启一个容器： 1[root@localhost ~]# docker restart test2 //运行一个容器： 1[root@localhost ~]# docker run -it --name test1 centos:7 -i：交互 -t：伪终端 -d（daemon）：后台运行 –name：给容器命名 –restart=always：始终保持运行（随着docker开启而运行） 1[root@localhost ~]# docker create -it --name test3 centos:7 &#x2F;&#x2F;不常用 //进入一个容器： 123[root@localhost ~]# docker exec -it test2 &#x2F;bin&#x2F;bash[root@localhost ~]# docker attach test2 区别： exec进入的方式需要添加-i，-t选项，后面还需要给容器一个shell环境，但attach就不需要这么麻烦 exec进入的方式：如果exit退出，容器仍然保持运行 attach：如果执行exit退出，容器会被关闭，如果想要保持容器不被关闭，可以使用键盘：ctrl+p ctrl+q可以实现 本质上区别： exec进入的方法，会产生新的进程 attach进入的方法，不会产生新的进程 Docker的基本操作逻辑： ) 小实验： 基于centos:7镜像运行一个容器，并且在这个容器内部署nginx服务 1）下载镜像： 1[root@localhost ~]# docker pull centos:7 2）运行容器： 1[root@localhost ~]# docker run -itd --name webapp --restart&#x3D;always centos:7 3）进入容器，开始部署nginx服务：//将nginx包导入到容器内： 1[root@localhost ~]# docker cp nginx-1.14.0.tar.gz 12345678910111213[root@localhost ~]# docker exec -it webapp &#x2F;bin&#x2F;bash[root@01b870908942 ~]# tar zxf nginx-1.14.0.tar.gz [root@01b870908942 ~]# cd nginx-1.14.0[root@01b870908942 nginx-1.14.0]# yum -y install gcc pcre pcre-devel openssl openssl-devevl zlib zlib-devel[root@01b870908942 nginx-1.14.0]# useradd -s &#x2F;sbin&#x2F;nologin nginx[root@01b870908942 nginx-1.14.0]# .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --user&#x3D;nginx --group&#x3D;nginx[root@01b870908942 nginx-1.14.0]# make &amp;&amp; make install[root@01b870908942 nginx-1.14.0]# ln -s &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;* &#x2F;usr&#x2F;local&#x2F;sbin&#x2F;[root@01b870908942 nginx-1.14.0]# nginx[root@01b870908942 nginx-1.14.0]# cd &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;html&#x2F;[root@01b870908942 html]# echo This is a resrweb in container &gt; index.html[root@01b870908942 html]# curl 127.0.0.1This is a resrweb in container //把容器制作成镜像：（可移植性） 1[root@localhost ~]# docker commit webapp myweb:12-10","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"}]},{"title":"Docker的监控","slug":"Docker的监控","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.256Z","comments":true,"path":"Docker的监控.html","link":"","permalink":"https://pdxblog.top/Docker%E7%9A%84%E7%9B%91%E6%8E%A7.html","excerpt":"","text":"Docker的监控docker自带的监控命令 docker top / stats / logs sysdig 12345678910[root@localhost ~]# docker load &lt; sysdig.tar[root@localhost ~]# docker load &lt; scope.1.12.tar[root@localhost ~]# docker run -it --rm --name sysdig \\&gt; --privileged&#x3D;true \\&gt; --volume&#x3D;&#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;host&#x2F;var&#x2F;run&#x2F;docker.sock \\&gt; --volume&#x3D;&#x2F;dev:&#x2F;host&#x2F;dev \\&gt; --volume&#x3D;&#x2F;proc:&#x2F;host&#x2F;proc:ro \\&gt; --volume&#x3D;&#x2F;boot:&#x2F;host&#x2F;boot:ro \\&gt; --volume&#x3D;&#x2F;lib&#x2F;modules:&#x2F;host&#x2F;lib&#x2F;modules:ro \\&gt; --volume&#x3D;&#x2F;usr:&#x2F;host&#x2F;usr:ro sysdig&#x2F;sysdig //下载失败后可以运行下边的命令，重新下载 1root@2fefbfde3db5:&#x2F;# sysdig-probe-loader //下载成功之后，可以运行sysdig命令 1root@2fefbfde3db5:&#x2F;# csysdig scope123[root@localhost ~]# curl -L git.io&#x2F;scope -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scope[root@localhost ~]# chmod a+x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scope[root@localhost ~]# scope launch //访问本机的4040端口 ) //监控两台dockerhost docker01 192.168.1.70 docker02 192.168.1.50 //docker02上也需要同样的操作 123[root@docker02 ~]# curl -L git.io&#x2F;scope -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scope[root@docker02 ~]# chmod a+x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scope[root@docker02 ~]# docker load &lt; scope.1.12.tar 12[root@docker01 ~]# scope launch 192.168.1.70 192.168.1.50[root@docker02 ~]# scope launch 192.168.1.50 192.168.1.70 )","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"}]},{"title":"Docker的私有仓库","slug":"Docker的私有仓库","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.256Z","comments":true,"path":"Docker的私有仓库.html","link":"","permalink":"https://pdxblog.top/Docker%E7%9A%84%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93.html","excerpt":"","text":"Registry用docker容器运行registry私有仓库 下载registry镜像： 1[root@localhost ~]# docker pull registry:2 &#x2F;&#x2F;2版本是使用go语言编写的，而registry是使用python写的 //运行私有仓库： 1[root@localhost ~]# docker run -itd --name registry --restart&#x3D;always -p 5000:5000 -v &#x2F;registry:&#x2F;var&#x2F;lib&#x2F;registry registry:2 -p：端口映射（宿主机端口：容器暴露的端口） -v：挂载目录（宿主机的目录：容器内的目录） 镜像重命名： 1[root@localhost ~]# docker tag test-web:latest 192.168.1.70:5000&#x2F;test 上传镜像到私有仓库： 123456[root@localhost ~]# vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service修改：指定私有仓库地址ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd --insecure-registry 192.168.1.70:5000[root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker[root@localhost ~]# docker push 192.168.1.70:5000&#x2F;test:latest 这里注意，既然是私有仓库，肯定是要考虑多台DockerHost共用的情况，如果有其他的DockerHost想要使用私有仓库，仅需要修改docker的配置文件，指定私有仓库的IP和端口即可。当然别忘了，更改过配置文件之后，daemon-reload ,restart docker服务 企业级私有仓库镜像Harbor下载一个docker-compse工具 //从GitHub上下载方法： 12[root@docker01 ~]# curl -L https:&#x2F;&#x2F;github.com&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.25.0&#x2F;docker-compose-&#96;uname -s&#96;-&#96;uname -m&#96; -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose[root@docker01 ~]# chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose 12[root@docker01 ~]# tar zxf docker-compose.tar.gz -C &#x2F;usr&#x2F;local&#x2F;bin[root@docker01 ~]# chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose //下载依赖包 1[root@docker01 ~]# yum -y install yum-utils device-mapper-persistent-data lvm2 //导入harbo离线安装包，并解压到/usr/local/下 1[root@docker01 ~]# tar zxf harbor-offline-installer-v1.7.4.tgz -C &#x2F;usr&#x2F;local&#x2F; //安装harbor 1234[root@docker01 ~]# cd &#x2F;usr&#x2F;local&#x2F;harbor&#x2F;[root@docker01 harbor]# vim harbor.cfghostname &#x3D; 192.168.1.70[root@docker01 harbor]# .&#x2F;install.sh //浏览器访问：192.168.1.70 用户名：admin 密码：Harbor12345 ) //修改docker配置文件，连接Harbor私有仓库 1234[root@docker01 ~]# vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.serviceExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd --insecure-registry 192.168.1.70[root@docker01 ~]# systemctl daemon-reload [root@docker01 ~]# systemctl restart docker //创建私有仓库 ) //登录仓库上传镜像 123[root@docker01 harbor]# docker login -u admin -p Harbor12345 192.168.1.70[root@docker01 harbor]# docker tag centos:7 192.168.1.70&#x2F;bdqn&#x2F;centos:7[root@docker01 harbor]# docker push 192.168.1.70&#x2F;bdqn&#x2F;centos:7 //从私有仓库下载镜像 1[root@docker03 ~]# docker pull 192.168.1.70&#x2F;bdqn&#x2F;centos:7","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"}]},{"title":"Docker部署LNMP环境","slug":"Docker部署LNMP环境","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.271Z","comments":true,"path":"Docker部署LNMP环境.html","link":"","permalink":"https://pdxblog.top/Docker%E9%83%A8%E7%BD%B2LNMP%E7%8E%AF%E5%A2%83.html","excerpt":"","text":"Docker部署LNMP环境172.16.10.0/24 Nginx：172.16.10.10 Mysql：172.16.10.20 PHP：172.16.10.30 网站的访问主目录：/wwwroot Nginx的配置文件：/docker 123456789101112[root@localhost ~]# docker run -itd --name test nginx:latest[root@localhost ~]# mkdir &#x2F;wwwroot[root@localhost ~]# mkdir &#x2F;docker[root@localhost ~]# docker cp test:&#x2F;etc&#x2F;nginx &#x2F;docker&#x2F;[root@localhost ~]# ls &#x2F;docker&#x2F;nginx[root@localhost ~]# docker cp test:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html &#x2F;wwwroot&#x2F;[root@localhost ~]# ls &#x2F;wwwroot&#x2F;html[root@localhost ~]# vim &#x2F;wwwroot&#x2F;html&#x2F;index.html[root@localhost ~]# cat &#x2F;wwwroot&#x2F;html&#x2F;index.html hello LNMP! 1）创建一个自定义网络 1[root@localhost ~]# docker network create -d bridge --subnet 172.16.10.0&#x2F;24 --gateway 172.16.10.1 lnmp 2）运行nginx容器 1[root@localhost ~]# docker run -itd --name nginx -v &#x2F;docker&#x2F;nginx&#x2F;:&#x2F;etc&#x2F;nginx -v &#x2F;wwwroot&#x2F;html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html -p 80:80 --network lnmp --ip 172.16.10.10 nginx:latest 3）运行mysql容器 123456789101112131415[root@localhost ~]# docker run --name mysql -e MYSQL_ROOT_PASSWORD&#x3D;123.com -d -p 3306:3306 --network lnmp --ip 172.16.10.20 mysql:5.7[root@localhost ~]# yum -y install mysql[root@localhost ~]# mysql -u root -p123.com -h 127.0.0.1 -P 3306MySQL [(none)]&gt; create database name;MySQL [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || name || performance_schema || sys |+--------------------+5 rows in set (0.00 sec) 4）运行php容器 1[root@localhost ~]# docker run -itd --name phpfpm -p 9000:9000 -v &#x2F;wwwroot&#x2F;html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html --network lnmp --ip 172.16.10.30 php:7.2-fpm //添加php测试页面： 123456[root@localhost html]# pwd&#x2F;wwwroot&#x2F;html[root@localhost html]# cat test.php &lt;?phpphpinfo();?&gt; 5）修改nginx配置文件，nginx和php连接 1234567891011121314[root@localhost ~]# cd &#x2F;docker&#x2F;nginx&#x2F;conf.d&#x2F;[root@localhost conf.d]# vim default.conf location &#x2F; &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; index index.html index.htm index.php; &#x2F;&#x2F;添加php解析&#x2F;&#x2F;打开此模块，并更改相应信息 location ~ \\.php$ &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; fastcgi_pass 172.16.10.30:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; //重启nginx 1[root@localhost conf.d]# docker restart nginx //到此，去浏览器验证，nginx服务和php服务界面 ) ) 说明nginx和php的来连接，没有问题，接下来是php和mysql的连接，在这我们使用一个phpMyAdmin的数据库管理工具 1234[root@localhost html]# pwd&#x2F;wwwroot&#x2F;html[root@localhost html]# unzip phpMyAdmin-4.9.1-all-languages.zip[root@localhost html]# mv phpMyAdmin-4.9.1-all-languages phpmyadmin //更改nginx的配置文件 123456789101112131415161718[root@localhost ~]# cd &#x2F;docker&#x2F;nginx&#x2F;conf.d&#x2F;[root@localhost conf.d]# pwd&#x2F;docker&#x2F;nginx&#x2F;conf.d[root@localhost conf.d]# vim default.conf&#x2F;&#x2F;在27行添加 location &#x2F;phpmyadmin &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; index index.html index.htm index.php; &#125;&#x2F;&#x2F;在43行添加 location ~ &#x2F;phpmyadmin&#x2F;(?&lt;after_ali&gt;(.*)\\.(php|php5)?$) &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; fastcgi_pass 172.16.10.30:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; //重启nginx 1[root@localhost conf.d]# docker restart nginx //验证php主界面 ) //需要我们对php镜像做出更改，添加php和mysql连接的模块 写一个Docker 123456789FROM php:7.2-fpmRUN apt-get update &amp;&amp; apt-get install -y \\ libfreetype6-dev \\ libjpeg62-turbo-dev \\ libpng-dev \\ &amp;&amp; docker-php-ext-install -j$(nproc) iconv \\ &amp;&amp; docker-php-ext-configure gd --with-freetype-dir&#x3D;&#x2F;usr&#x2F;include&#x2F; --with-jpeg-dir&#x3D;&#x2F;usr&#x2F;include&#x2F; \\ &amp;&amp; docker-php-ext-install -j$(nproc) gd \\ &amp;&amp; docker-php-ext-install mysqli pdo pdo_mysql 1[root@localhost ~]# docker build -t phpmysql . //删除之前的php容器，并用我们新制作的php镜像重新运行 123[root@localhost ~]# docker stop phpfpm [root@localhost ~]# docker rm phpfpm [root@localhost ~]# docker run -itd --name phpfpm -p 9000:9000 -v &#x2F;wwwroot&#x2F;html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html --network lnmp --ip 172.16.10.30 phpmysql:latest //修改phpmyadmin的配置文件，指定连接的数据库的IP，然后重启php容器 1234567[root@localhost ~]# cd &#x2F;wwwroot&#x2F;html&#x2F;phpmyadmin&#x2F;[root@localhost phpmyadmin]# pwd&#x2F;wwwroot&#x2F;html&#x2F;phpmyadmin[root@localhost phpmyadmin]# cp config.sample.inc.php config.inc.php[root@localhost phpmyadmin]# vim config.inc.php$cfg[&#39;Servers&#39;][$i][&#39;host&#39;] &#x3D; &#39;172.16.10.20&#39;; &#x2F;&#x2F;修改，指定数据库的IP地址[root@localhost ~]# docker restart phpfpm 用户名：root 密码：123.com ) //登录成功之后会看到我们之前创建的数据库 )","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"}]},{"title":"KVM基本操作命令","slug":"KVM基本操作命令","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.334Z","comments":true,"path":"KVM基本操作命令.html","link":"","permalink":"https://pdxblog.top/KVM%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4.html","excerpt":"","text":"基于操作命令1）查看虚拟机列表： 12[root@kvm ~]# virsh list &#x2F;&#x2F;查看正在运行的虚拟机[root@kvm ~]# virsh list --all &#x2F;&#x2F;查看所有虚拟机 //开机的虚拟机才有ID号，而且会随时变化 Id 名称 状态 test01 关闭 2）查看虚拟机的详细信息： 1234567891011121314[root@kvm ~]# virsh dominfo test01 &#x2F;&#x2F;dom全称domain，域的意思Id: -名称： test01UUID: 8ba94166-08dd-4805-962b-c99ed56869bcOS 类型： hvm状态： 关闭CPU： 1最大内存： 1048576 KiB使用的内存： 1048576 KiB持久（peisistent）： 是 &#x2F;&#x2F;数据的持久化自动启动（autostart）： 禁用 &#x2F;&#x2F;是否开机自启管理的保存： 否安全性模式： none安全性 DOI： 0 3）虚拟机域的开关机： 123[root@kvm ~]# virsh start test01 &#x2F;&#x2F;开机[root@kvm ~]# virsh shutdown test01 &#x2F;&#x2F;关机（shutdown：温柔的关机）[root@kvm ~]# virsh shutdown 2 &#x2F;&#x2F;2为ID号 //关机后再开机ID号也会变化 1[root@kvm ~]# virsh destroy test01 &#x2F;&#x2F;强制关机，类似于拔电源 4）导出配置： 1[root@kvm ~]# virsh dumpxml test01 &gt; test01.xml &#x2F;&#x2F;dump备份的意思 vmnet0：桥接 //好处：外网能够访问你的虚拟机vmnet1：主机vmnet8：NAT //缺点：外网访问不了你的虚拟机，好处：可以自己随意指定IP 一个完成的KVM域，生成之后会有两个文件： 1）磁盘文件：在部署之处已经指定 //用来记录它的信息 2）xml配置文件，默认在/etc/libvirt/qemu //qemu模拟硬件，类型为raw 5）删除虚拟机：//删除之前保证虚拟机是关闭状态 1[root@kvm ~]# virsh undefine test01 &#x2F;&#x2F;undefine取消定义 //xml配置文件也会被删除，但是磁盘文件不会被影响 6）根据配置文件恢复虚拟机： 1[root@kvm ~]# virsh define test01.xml &#x2F;&#x2F;define：定义 7）修改配置文件： 1[root@kvm qemu]# virsh edit test01 edit：自带语法检查功能（y：是、n：不、i：忽略、f：强制）vim：不会提示你语法错误 8）虚拟机重命名（7.2版本之前的不支持这条命令） 1[root@kvm ~]# virsh domrename test01 test1 &#x2F;&#x2F;重命名前关闭虚拟机 9）查看虚拟机对应的vnc端口 12[root@localhost ~]# virsh vncdisplay test01:0 :0等于5900:1=5901:2=5902 10)挂起虚拟机 12[root@localhost ~]# virsh suspend test01[root@localhost ~]# virsh resume test01 &#x2F;&#x2F;恢复挂起的虚拟机 11）开机自启 12[root@localhost ~]# virsh autostart test01[root@localhost autostart]# virsh autostart --disable test01 &#x2F;&#x2F;取消开机自启 12）console登录KVM域//在KVM域里添加 1234grubby --update-kernel&#x3D;ALL --args&#x3D;&quot;console&#x3D;ttyS0&quot;rebootvirsh console test01 &#x2F;&#x2F;使用xshell连接kvm退出 ctrl+]","categories":[{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/tags/KVM/"}]},{"title":"KVM磁盘格式","slug":"KVM磁盘格式","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.318Z","comments":true,"path":"KVM磁盘格式.html","link":"","permalink":"https://pdxblog.top/KVM%E7%A3%81%E7%9B%98%E6%A0%BC%E5%BC%8F.html","excerpt":"","text":"磁盘格式：RAW：（裸格式） //占用空间较大，性能较好，但不支持虚拟机快照功能QCOW2：（copy on write） //占用空间较小，支持快照，性能比RAW稍差一些 创建磁盘：（默认是裸格式） 1[root@kvm disk]# qemu-img create 1234.raw 5G 查看磁盘信息： 1[root@kvm disk]# qemu-img info 1234.raw 创建指定格式磁盘： 1[root@kvm disk]# qemu-img create -f qcow2 bdqn.qcow2 5G 转换磁盘格式：qemu-img convert [-f fmt] [-O output_fmt] filename output_filename 1[root@kvm kvm-vm]# qemu-img convert -f raw -O qcow2 centos.raw centos.qcow2 //转换之后原来的磁盘还在 拍摄快照： 1[root@kvm ~]# virsh snapshot-create test01 查看快照信息： 1234[root@kvm ~]# virsh snapshot-list test01 名称 生成时间 状态------------------------------------------------------------ 1575254957 2019-12-02 10:49:17 +0800 running 时间戳：1970年1月1号（计算机C语言诞生了，Linux系统诞生了）32位系统：68年之后你的系统就不能使用了64位系统：使用时间没有限制 根据快照恢复系统： 1[root@kvm ~]# virsh snapshot-revert test01 1575254957 //拍摄的快照是占用磁盘空间的","categories":[{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/tags/KVM/"}]},{"title":"KVM简介","slug":"KVM简介","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.318Z","comments":true,"path":"KVM简介.html","link":"","permalink":"https://pdxblog.top/KVM%E7%AE%80%E4%BB%8B.html","excerpt":"","text":"KVM简介：什么是云计算：云计算:配置各种资源的方式 云计算的分类：基础即服务Lass平台即服务Pass软件即服务Sass如果按照不同的部署方式：公有云、私有云、混合云 KVM介绍：虚拟化的不同的方式 实现虚拟化的技术：基于二进制翻译的全虚拟化：（会报错）解决思路：捕捉报错—-翻译—模拟（会增加服务器的开销） 半虚拟化（Xen）：更改内核，只能用到Linux系统上全虚拟化：KVM、VMware//所依赖的硬件全部准备好就行 KVM的概念：基于内核的虚拟机（Kernel-Based VIrtul mathine） 打开KVM的方式：virt-manager应用程序–系统工具–虚拟系统管理器 命令创建虚拟主机域： 1234[root@localhost iso]# virt-install --os-type&#x3D;linux --os-variant centos7.0--name test01 --ram 1024 --vcpus 1 --disk&#x3D;&#x2F;kvm-vm&#x2F;centos.raw,format&#x3D;raw,size&#x3D;10--location &#x2F;iso&#x2F;CentOS-7-x86_64-DVD-1611.iso --network network&#x3D;default --graphics vnc,listen&#x3D;0.0.0.0 --noautoconsole（不会占用终端） vnc连接KVM虚拟机默认的端口为：5900","categories":[{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/tags/KVM/"}]},{"title":"KVM网络","slug":"KVM网络","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.318Z","comments":true,"path":"KVM网络.html","link":"","permalink":"https://pdxblog.top/KVM%E7%BD%91%E7%BB%9C.html","excerpt":"","text":"NAT模式：KVM默认的网络方式，如果想要应用这种模式，防火墙需要打开，因为需要用到iptables规则 //打开防火墙添加规则，打开5900端口 12345[root@localhost ~]# firewall-cmd --add-port&#x3D;5900&#x2F;tcp --permanent success[root@localhost ~]# firewall-cmd --reloadsuccess[root@localhost ~]# firewall-cmd --list-all //添加路由转发： 123[root@localhost ~]# echo net.ipv4.ip_forward &#x3D; 1 &gt;&gt; &#x2F;etc&#x2F;sysctl.conf [root@localhost ~]# sysctl -pnet.ipv4.ip_forward &#x3D; 1 总结：nat模式支持主机与虚拟机的互访，也支持虚拟机访问互联网，但不支持外网访问虚拟机域 桥接网络：1）创建虚拟桥接网卡br0 1234[root@localhost ~]# systemctl stop NetworkManager[root@localhost ~]# virsh iface-bridge ens33 br0 &#x2F;&#x2F;提示失败不用理会使用附加设备 br0 生成桥接 ens33 失败已启动桥接接口 br0 //查看配置文件会看到，ens33桥接到了br0 12345678[root@localhost network-scripts]# cat ifcfg-ens33DEVICE&#x3D;ens33ONBOOT&#x3D;yesBRIDGE&#x3D;&quot;br0&quot;[root@localhost network-scripts]# brctl showbridge name bridge id STP enabled interfacesbr0 8000.000c2901f11f yes ens33virbr0 8000.525400dc381b yes virbr0-nic 2）修改kvm虚拟机域的xml配置文件： 123&lt;interface type&#x3D;&#39;bridge&#39;&gt;&lt;mac address&#x3D;&#39;52:54:00:f8:a1:c9&#39;&#x2F;&gt;&lt;source bridge&#x3D;&#39;br0&#39;&#x2F;&gt; 3）开启虚拟机，配置IP，验证是否能够联通外网: 1[root@localhost ~]# vi &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth //IP和br0的IP要在同一网段","categories":[{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/tags/KVM/"}]},{"title":"ReplicaSet、DaemonSet","slug":"ReplicaSet、DaemonSet","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.303Z","comments":true,"path":"ReplicaSet、DaemonSet.html","link":"","permalink":"https://pdxblog.top/ReplicaSet%E3%80%81DaemonSet.html","excerpt":"","text":"ReplicaSet RC：ReplicationoController（老一代的Pod控制器） RS：ReplicaSet（新一代的Pod控制器） 用于确保由其管理的控制的Pod对象副本，能够满足用户期望，多则删除，少则通过模板创建 deployment、rs、rc 特点： 确保Pod资源对象的数量精准 确保Pod健康运行 弹性伸缩 同样，它也可以通过yaml或json格式的资源清单来创建，其中spec字段一般嵌套一下字段 replicas：期望的Pod对象副本数量 selector：当前控制器匹配Pod对象副本的标签 template：Pod副本的模板 与RC相比而言，RS不仅支持基于等值的标签选择器，而且还支持集合的标签选择器 标签：解决同类型的资源对象越来越多，为了更好的管理，按照标签分组 常用标签分类： release（版本）：stable（稳定版）、canary（金丝雀）、beta（测试版） environment（环境变量）：dev（开发）、qa（测试）、production（生产） application（应用）：ui、as（application software应用软件）、pc、sc tier（架构层级）：frontend（前端）、backend（后端）、cache（缓存） partition（分区）：customerA（客户A）、customoerB（客户B） track（品控级别）：daily（每天）、weekly（每周） 标签要做到：见名知意 //通过–show-labels显示资源对象的标签 1[root@master ~]# kubectl get pod --show-labels //通过-l选项查看仅含有包含某个标签的资源 1[root@master ~]# kubectl get pod -l env //通过-L显示某个键对应的值 1[root@master ~]# kubectl get pod -L env //给Pod资源添加标签 1[root@master ~]# kubectl label pod label app&#x3D;pc //删除标签 1[root@master ~]# kubectl label pod label app- //修改标签 1[root@master ~]# kubectl label pod label env&#x3D;dev --overwrite 如果标签有多个，标签选择器选择其中一个，也可以关联成功，相反，如果选择器有多个，那标签必须完全满足条件，才可以关联成功 标签选择器：标签的查询过滤条件 基于等值关系的（equality-based）：”=”，”==”，”!=” =” 前面两个都是相等，最后是不等 基于集合关系（set-based）：in、notin、exists三种 例子： 123456selector: matchLables: app: nginx metchExpressions: - &#123;key: name,operator: In,values: [zhangsan,lisi]&#125; - &#123;key: age,operator: Exists,values:&#125; matchLabels：指定键值对来表示的标签选择器 matchExpressions：基于表达式来指定的标签选择器，选择器列表间为”逻辑与”关系；使用In或者Notin操作时，其values不强制要求为非控的字符串，而使用Exists或DosNotExist时，其values必须为空 ) 使用标签选择器的逻辑： 同时指定的多个选择器之间的逻辑关系为”与”操作 使用空值的标签选择器意味着每个资源对象都将被选择中 空的标签选择器无法选中任何资源 DaemonSet 它也是一种Pod控制器 使用场景：如果必须将Pod运行再固定的某个或某几个节点，且要优先其他Pod的启动，通常情况下，默认会每个节点都会运行，并且只能运行一个Pod，这种情况推荐使用DaemonSet资源对象 监控程序： 日志收集程序： 运行一个web服务，在每一个节点都运行一个Pod 1234567891011121314[root@master ~]# vim daemonset.yamlkind: DaemonSetapiVersion: extensions&#x2F;v1beta1metadata: name: test-dsspec: template: metadata: labels: name: test-ns spec: containers: - name: test-ns image: httpd:v1 RC、RS、Deployment、DaemonSet，Pod控制器。statfulSet（有状态）、Ingress。Pod RBAC：基于用户的认证授权机制","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"虚拟机的克隆","slug":"虚拟机的克隆","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.318Z","comments":true,"path":"虚拟机的克隆.html","link":"","permalink":"https://pdxblog.top/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%85%8B%E9%9A%86.html","excerpt":"","text":"克隆：克隆的两种方式：1、手动克隆（完整克隆）：test01———-&gt;test02：（将test01克隆为test02）1）复制xml配置文件： 1234[root@localhost ~]# cd &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;[root@localhost qemu]# cp test01.xml test02.xml或者[root@kvm ~]# virsh dumpxml test01 &gt; test02.xml 2）复制磁盘文件： 12[root@localhost qemu]# cd &#x2F;kvm-vm&#x2F;[root@localhost kvm-vm]# cp centos.raw test02.raw 3）修改配置文件并重新生成一个虚拟机： 12[root@localhost kvm-vm]# cd &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;[root@localhost qemu]# vim test02.xml a:name字段b:删除UUIDc:删除mac addressd:修改磁盘路径以及名称 1[root@localhost qemu]# virsh define test02.xml 链接克隆：//做一个链接的磁盘，然后第二个新的虚拟机更改xml配置文件，磁盘信息指定新的链接磁盘 1[root@localhost kvm-vm]# qemu-img create -f qcow2 -b centos.raw test02.qcow2 自动克隆（完整克隆）: 1[root@localhost ~]# virt-clone --auto-clone -o test02 -n test03 //-o:表示克隆谁，-n：指定名称","categories":[{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/tags/KVM/"}]},{"title":"虚拟机的迁移","slug":"虚拟机的迁移","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.334Z","comments":true,"path":"虚拟机的迁移.html","link":"","permalink":"https://pdxblog.top/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E8%BF%81%E7%A7%BB.html","excerpt":"","text":"虚拟机的迁移： 冷迁移（静态迁移）： //服务器需要关闭kvm01：192.168.1.100kvm02：192.168.1.200 两台机器防火墙全部关闭，禁用selinux 12[root@localhost ~]# lsmod | grep kvm &#x2F;&#x2F;查看是否支持kvm[root@localhost ~]# systemctl status libvirtd &#x2F;&#x2F;查看libvirtd服务是否正常 //迁移和克隆差不多，都是需要对磁盘文件和xml配置文件进行操作 123[root@kvm01 ~]# scp &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;test01.xml root@192.168.1.200:&#x2F;etc&#x2F;libvirt&#x2F;qemu [root@kvm01 ~]# scp &#x2F;kvm-vm&#x2F;centos.raw root@192.168.1.200:&#x2F;kvm-vm&#x2F;[root@kvm02 ~]# virsh define &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;test01.xml 热迁移（动态迁移）：删除所有的KVM虚拟机kvm01：192.168.1.100kvm02：192.168.1.200NFS：192.168.1.129 1）在NFS服务器上面操作： 123456789[root@NFS ~]# yum -y install nfs-utils[root@NFS ~]# mkdir &#x2F;kvmshare &#x2F;&#x2F;创建共享文件夹[root@NFS ~]# vim &#x2F;etc&#x2F;exports &#x2F;&#x2F;编辑共享文件夹权限[root@NFS ~]# cat &#x2F;etc&#x2F;exports&#x2F;kvmshare *(rw,sync,no_root_squash)[root@NFS ~]# systemctl start rpcbind &#x2F;&#x2F;远程传输控制协议[root@NFS ~]# systemctl enable rpcbind[root@NFS ~]# systemctl start nfs-server[root@NFS ~]# systemctl enable nfs-server //确保两台KVM服务器能看到 12[root@kvm01 ~]# showmount -e 192.168.1.129[root@kvm02 ~]# showmount -e 192.168.1.129 2）KVM01上基于NFS服务创建虚拟机添加新的存储池：名称：nfsshare类型：netfs目标路径：/opt/nfsshare（本机挂在的目录，目录默认没有，但会自己创建）主机名:192.168.1.129（nfs-server IP address）源路径：/kvmshare（nfs-server上的共享目录） 验证nfs服务是否正常： 123[root@kvm01 ~]# touch &#x2F;opt&#x2F;nfsshare&#x2F;test[root@NFS ~]# ls &#x2F;kvmshare&#x2F;test 创建存储卷：名称：centos7最大容量：10G //存储池和存储卷完成之后，直接创建虚拟机，并最小化安装选择之前的创建的iso镜像以及刚才创建的存储池和存储卷 配置虚拟机使用bridge桥接网络，使其能够ping通外网，并且在这里我们执行一个ping百度的命令，并让他保持一直是ping着的状态，用来模拟迁移到kvm02上服务不中断： 12345678[root@kvm01 ~]# virsh destroy centos7.0[root@kvm01 ~]# systemctl stop NetworkManager[root@kvm01 ~]# virsh iface-bridge ens33 br0[root@kvm01 ~]# virsh edit centos7.0&lt;interface type&#x3D;&#39;bridge&#39;&gt;&lt;mac address&#x3D;&#39;52:54:00:12:80:97&#39;&#x2F;&gt;&lt;source bridge&#x3D;&#39;br0&#39;&#x2F;&gt;[root@kvm01 ~]# virsh start centos7.0 配置IP为DHCP自动获取 在KVM02上操作，创建存储池：名称：nfsshare类型：netfs目标路径：/opt/nfsshare（本机挂在的目录，目录默认没有，但会自己创建）主机名:192.168.1.129（nfs-server IP address）源路径：/kvmshare（nfs-server上的共享目录）创建完之后会看到之前在KVM01上创建的test文件和centos.qcow2的存储卷 在KVM01上连接KVM02：右上角—文件—添加连接—连接到远程主机—方法：ssh—用户名：root—-主机名：192.168.1.200（KVM02的IP）会提示安装openssh-askpass，直接在KVM01和KVm02上安装： 12[root@kvm01 ~]# yum -y install openssh-askpass[root@kvm02 ~]# yum -y install openssh-askpass //因为KVM01使用的是bridge br0网卡，所以我们需要在KVM02上创建同样的网卡br0，用来支持虚拟机 12[root@kvm02 ~]# systemctl stop NetworkManager[root@kvm02 ~]# virsh iface-bridge ens33 br0 接下来直接在virt-manager管理器中迁移就可以了，迁移完成之后，保证我么的ping命令是不中断的，就表示实验完成了右键centos7.0—迁移—地址：192.168.1.200（KVM02的IP）—高级选项—-勾选允许不可靠—-迁移如果出现错误解决办法：把KVM01和KVM02上挂载的目录给一个777的权限，保证双方root用户有权限调用目录 12[root@kvm01 ~]# chmod 777 &#x2F;opt&#x2F;[root@kvm02 ~]# chmod 777 &#x2F;opt&#x2F; 迁移完成后在KVM02上面查看ping命令是否中断","categories":[{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/tags/KVM/"}]},{"title":"Docker三剑客之docker-compose+wordpress的博客搭建","slug":"Docker三剑客之docker-compose+wordpress的博客搭建","date":"2020-01-23T16:00:00.000Z","updated":"2020-02-03T02:45:12.557Z","comments":true,"path":"Docker三剑客之docker-compose+wordpress的博客搭建.html","link":"","permalink":"https://pdxblog.top/Docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-compose+wordpress%E7%9A%84%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA.html","excerpt":"","text":"Docker三剑客：docker machine：自动化部署多台dockerHostdocker-compose：它可以同时控制多个容器docker swarm：从单个的服务向集群的形式发展为什么要做集群：高可用、高性能、高并发：防止单点故障 Docker三剑客之docker-composedocker容器的编排工具： 解决相互有依赖关系的多个容器的管理 //验证已有docker-compose命令 12[root@localhost ~]# docker-compose -vdocker-compose version 1.25.0, build 0a186604 docker-compose的配置文件实例 通过识别一个docker-compose.yml的配置文件，去管理容器 //设置tab键的空格数量 123[root@localhost ~]# vim .vimrcset tabstop&#x3D;2[root@localhost ~]# source .vimrc 12345678910111213[root@localhost ~]# mkdir compose_test[root@localhost ~]# cd compose_test&#x2F;[root@localhost compose_test]# vim docker-compose.ymlversion: &quot;3&quot;services: nginx: container_name: web-nginx image: nginx restart: always ports: - 90:80 volumes: - .&#x2F;webserver:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html 第一个部分：version：指定格式的版本 第二部分：services：定义服务，（想要运行什么样的容器） //运行docker-compose规定的容器： PS：在执行这条命令的当前目录下，也需要有一个docker-compose.yml的配置文件，并且通常只有一个 12345[root@localhost compose_test]# docker-compose up -d[root@localhost compose_test]# cd webserver&#x2F;[root@localhost webserver]# echo 123456 &gt; index.html[root@localhost webserver]# curl 127.0.0.1:90123456 //停止运行 1[root@localhost compose_test]# docker-compose stop //重启 1[root@localhost compose_test]# docker-compose restart //如果在当前目录没有docker-compose.yml这个文件，可以通过-f来指定docker-compose.yml文件位置 1[root@localhost ~]# docker-compose -f compose_test&#x2F;docker-compose.yml start 并且，在运行container的过程中，还可以支持Dockerfile 1234567891011121314151617181920212223[root@localhost compose_test]# vim Dockerfile[root@localhost compose_test]# cat Dockerfile FROM nginxADD webserver &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;htm[root@localhost compose_test]# vim docker-compose.ymlversion: &quot;3&quot;services: nginx: build: . container_name: web-nginx image: new-nginx:v1.0 restart: always ports: - 90:80[root@localhost compose_test]# docker-compose stop[root@localhost compose_test]# docker-compose rm[root@localhost compose_test]# docker-compose up -d[root@localhost compose_test]# curl 127.0.0.1:90123456[root@localhost compose_test]# cd webserver&#x2F;[root@localhost webserver]# echo 654321 &gt; index.html [root@localhost webserver]# curl 127.0.0.1:90123456 搭建wordpress的博客12345678910111213141516171819202122232425[root@localhost ~]# mkdir wordpress[root@localhost ~]# docker load &lt; wordpress.tar[root@localhost ~]# cd wordpress&#x2F;[root@localhost wordpress]# vim docker-compose.ymlversion: &quot;3.1&quot;services: wordpress: image: wordpress restart: always ports: - 8080:80 environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: 123.com WORDPRESS_DB_NAME: wordpress db: image: mysql:5.7 restart: always environment: MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: 123.com MYSQL_ROOT_PASSWORD: 123.com[root@localhost wordpress]# docker-compose up -d //浏览器访问本机的8080端口：（192.168.1.70:8080） ) ) ) )","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"}]},{"title":"Deployment","slug":"Deployment","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:00:44.044Z","comments":true,"path":"Deployment.html","link":"","permalink":"https://pdxblog.top/Deployment.html","excerpt":"","text":"Deployment 12345678910111213141516apiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: replicas: 4 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v1 ports: - containerPort: 80 PS：注意，在Deployment资源对象中，可以添加Port字段，但此字段仅供用户查看，并不实际生效 service 12345678910111213kind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort selector: app: web ports: - protocol: TCP port: 80 &#x2F;&#x2F;clusterIP的端口 targetPort: 80 &#x2F;&#x2F;Pod的端口 nodePort: 30123 SNAT：Source NAT（源地址转换） DNAT：Destination NAT（目标地址转换） MASQ：动态的源地址转换 service实现的负载均衡：默认使用的是iptables规则 第二种方案：IPVS 回滚到指定版本 准备三个版本所使用的私有镜像，来模拟每次升级到不同的镜像 Deployment1.yaml 1234567891011121314151617[root@master ~]# vim deployment1.yamlapiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v1 ports: Deployment2.yaml 123456789101112131415161718[root@master ~]# vim deployment2.yamlapiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v2 ports: - containerPort: 80 Deployment3.yaml 123456789101112131415161718[root@master ~]# vim deployment3.yamlapiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v3 ports: - containerPort: 80 此处3个yaml文件，指定不同版本的镜像 //运行一个服务，并记录一个版本信息 1[root@master ~]# kubectl apply -f deployment1.yaml --record //查看有哪些版本信息 1[root@master ~]# kubectl rollout history deployment test-web //运行并升级Deployment资源，并记录版本信息 12[root@master ~]# kubectl apply -f deployment2.yaml --record[root@master ~]# kubectl apply -f deployment3.yaml --record //此时可以运行一个关联的Service资源去验证升级是否成功 123[root@master ~]# kubectl apply -f web-svc.yaml[root@master ~]# curl 10.96.179.50&lt;h1&gt;zhb | test-web | httpd | v3&lt;h1&gt; //回滚到指定版本 用label控制Pod的位置 //添加节点你标签 12[root@master ~]# kubectl label nodes node02 disk&#x3D;ssd[root@master ~]# kubectl get nodes --show-labels | grep node02 12345678910111213141516171819apiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: revisionHistoryLimit: 10 &#x2F;&#x2F;版本历史限制 replicas: 3 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v1 ports: - containerPort: 80 nodeSelector: &#x2F;&#x2F;添加节点选择器 disk: ssd &#x2F;&#x2F;和标签内容一致 12345[root@master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-web-d58c9f847-bhswj 1&#x2F;1 Running 0 28s 10.244.2.14 node02 &lt;none&gt; &lt;none&gt;test-web-d58c9f847-k58nj 1&#x2F;1 Running 0 28s 10.244.2.13 node02 &lt;none&gt; &lt;none&gt;test-web-d58c9f847-vt7r5 1&#x2F;1 Running 0 28s 10.244.2.15 node02 &lt;none&gt; &lt;none&gt; //删除节点标签 1[root@master ~]# kubectl label nodes node02 disk-","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"Docker实现服务发现","slug":"Docker实现服务发现","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.209Z","comments":true,"path":"Docker实现服务发现.html","link":"","permalink":"https://pdxblog.top/Docker%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0.html","excerpt":"","text":"Docker实现服务发现Docker + Consul + registrator实现服务发现 Consul：分布式、高可用的，服务发现和配置的工具，数据中心 Registrator：负责收集dockerhost上，容器服务的信息，并且发送给consul Consul-template：根据编辑好的模板生成新的nginx配置文件，并且负责加载nginx配置文件 实验环境 docker01 192.168.1.70 docker02 192.168.1.60 docker03 192.168.1.50 关闭防火墙和selinux，并修改主机名 1234[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# setenforce 0[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - 1）docker01上，启动consul服务 123[root@docker01 ~]# unzip consul_1.5.1_linux_amd64.zip[root@docker01 ~]# mv consul &#x2F;usr&#x2F;local&#x2F;bin&#x2F;[root@docker01 ~]# chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;consul //以二进制的方式部署consul，并启动，身份为leader 12345[root@docker01 ~]# consul agent -server -bootstrap \\&gt; -ui -data-dir&#x3D;&#x2F;var&#x2F;lib&#x2F;consul-data \\&gt; -bind&#x3D;192.168.1.70 \\&gt; -client&#x3D;0.0.0.0 \\&gt; -node&#x3D;master //这时这个命令会占用终端，可以使用nohup命令让它保持后台运行 1[root@docker01 ~]# nohup consul agent -server -bootstrap -ui -data-dir&#x3D;&#x2F;var&#x2F;lib&#x2F;consul-data -bind&#x3D;192.168.1.70 -client&#x3D;0.0.0.0 -node&#x3D;master &amp; PS: -bootstrap：加入这个选项时，一般都在server单节点的时候用，自选举为leader -ui：开启内部web界面 -data-dir：key/volume数据存储位置 -bind：指定开启服务的IP -client：指定访问的客户端 -node：只当集群内通信使用的名称，默认是用主机名命名的 PS：开启的端口 8300：集群节点 8301：集群内部的访问 8302：跨数据中心的通信 8500：web ui界面 8600：使用dns协议查看节点信息的端口 //查看conusl的信息 12[root@docker01 ~]# consul infoleader_addr &#x3D; 192.168.1.70:8300 &#x2F;&#x2F;这个对我们比较有用，其他的都是一些它的算法 //查看集群内成员的信息 123[root@docker01 ~]# consul membersNode Address Status Type Build Protocol DC Segmentmaster 192.168.1.70:8301 alive server 1.5.1 2 dc1 &lt;all&gt; 2）docker02、docker03，加入consul集群 这里我们采用容器的方式去运行consul服务 //docker02 12[root@docker02 ~]# docker load &lt; myprogrium-consul.tar[root@docker02 ~]# docker run -d --name consul -p 8301:8301 -p 8301:8301&#x2F;udp -p 8500:8500 -p 8600:8600 -p 8600:8600&#x2F;udp --restart always progrium&#x2F;consul:latest -join 192.168.1.70 -advertise 192.168.1.60 -client 0.0.0.0 -node&#x3D;node01 //docker03 12[root@docker03 ~]# docker load &lt; myprogrium-consul.tar[root@docker03 ~]# docker run -d --name consul -p 8301:8301 -p 8301:8301&#x2F;udp -p 8500:8500 -p 8600:8600 -p 8600:8600&#x2F;udp --restart always progrium&#x2F;consul:latest -join 192.168.1.70 -advertise 192.168.1.50 -client 0.0.0.0 -node&#x3D;node02 //在docker01上就能看到加入的信息 12342019&#x2F;12&#x2F;26 09:50:25 [INFO] serf: EventMemberJoin: node01 192.168.1.602019&#x2F;12&#x2F;26 09:50:25 [INFO] consul: member &#39;node01&#39; joined, marking health alive2019&#x2F;12&#x2F;26 09:53:06 [INFO] serf: EventMemberJoin: node02 192.168.1.502019&#x2F;12&#x2F;26 09:53:06 [INFO] consul: member &#39;node02&#39; joined, marking health alive 12345[root@docker01 ~]# consul membersNode Address Status Type Build Protocol DC Segmentmaster 192.168.1.70:8301 alive server 1.5.1 2 dc1 &lt;all&gt;node01 192.168.1.60:8301 alive client 0.5.2 2 dc1 &lt;default&gt;node02 192.168.1.50:8301 alive client 0.5.2 2 dc1 &lt;default&gt; //浏览器访问consul服务，验证集群信息 192.168.1.70:8500 ) 3)下载部署consul-template //在docker01上导入consul-template_0.19.5_linux_amd64.zip 123[root@docker01 ~]# unzip consul-template_0.19.5_linux_amd64.zip[root@docker01 ~]# mv consul-template &#x2F;usr&#x2F;local&#x2F;bin&#x2F;[root@docker01 ~]# chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;consul-template 4）docker02、docker03上部署registrator服务 registrator是一个能自动发现docker container提供的服务，并在后端服务注册中心注册服务或取消服务的工具，后端注册中心支持consul、etcd、skydns2、zookeeper等 //docker02 1234567[root@docker02 ~]# docker load &lt; myregistrator.tar[root@docker02 ~]# docker run -d \\&gt; --name registrator \\&gt; -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;tmp&#x2F;docker.sock \\&gt; --restart always \\&gt; gliderlabs&#x2F;registrator \\&gt; consul:&#x2F;&#x2F;192.168.1.60:8500 //运行一个nginx容器 123[root@docker02 ~]# docker run -d -P --name test nginx:latestb0665dcbd6c5 nginx:latest &quot;nginx -g &#39;daemon of…&quot; 10 seconds ago Up 9 seconds 0.0.0.0:32768-&gt;80&#x2F;tcp &#x2F;&#x2F;映射的端口为32768 //回到浏览器 ) ) //docker03 1234567[root@docker03 ~]# docker load &lt; myregistrator.tar[root@docker02 ~]# docker run -d \\&gt; --name registrator \\&gt; -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;tmp&#x2F;docker.sock \\&gt; --restart always \\&gt; gliderlabs&#x2F;registrator \\&gt; consul:&#x2F;&#x2F;192.168.1.50:8500 ) 5）docker01部署一个nginx服务 //依赖环境 [root@docker01 ~]# yum -y install zlib-devel openssl-devel pcre-devel 1234567891011121314[root@docker01 ~]# useradd -M -s &#x2F;sbin&#x2F;nologin nginx[root@docker01 ~]# tar zxf nginx-1.14.0.tar.gz [root@docker01 ~]# cd nginx-1.14.0&#x2F;[root@docker01 nginx-1.14.0]# .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx \\&gt; --user&#x3D;nginx --group&#x3D;nginx \\&gt; --with-http_stub_status_module \\&gt; --with-http_realip_module \\&gt; --with-pcre --with-http_ssl_module[root@docker01 nginx-1.14.0]# make &amp;&amp; make install[root@docker01 nginx-1.14.0]# ln -s &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;* &#x2F;usr&#x2F;local&#x2F;sbin&#x2F;[root@docker01 nginx-1.14.0]# nginx -tnginx: the configuration file &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf syntax is oknginx: configuration file &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf test is successful[root@docker01 nginx-1.14.0]# nginx PS： 这里nginx作为反向代理，代理后端docker02、docker03上nignx的容器服务，所以我们先去docker02、docker03上部署一些服务，为了方便等会看到负载的效果，所以我们运行完成容器之后，做一个主界面内容的区分 docker02：web01 web02 docker03：web03 web04 //docker02 12345678910111213141516[root@docker02 ~]# docker run -itd --name web01 -P nginx:latest [root@docker02 ~]# docker exec -it web01 &#x2F;bin&#x2F;bashroot@dac0cc15f3fe:&#x2F;# cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;root@dac0cc15f3fe:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# echo &quot;This web caontainer in dockek02-web01&quot; &gt; index.html root@dac0cc15f3fe:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# cat index.html This web caontainer in dockek02-web01[root@docker02 ~]# docker run -itd --name web02 -P nginx:latest [root@docker02 ~]# docker exec -it web02 &#x2F;bin&#x2F;bashroot@26d622553e5e:&#x2F;# cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;root@26d622553e5e:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# echo &quot;This web caontainer in dockek02-web02&quot; &gt; index.html root@26d622553e5e:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# cat index.html This web caontainer in dockek02-web02[root@docker02 ~]# curl 127.0.0.1:32769This web caontainer in dockek02-web01[root@docker02 ~]# curl 127.0.0.1:32770This web caontainer in dockek02-web02 //docker03 12345678910111213141516[root@docker03 ~]# docker run -itd --name web03 -P nginx:latest [root@docker03 ~]# docker exec -it web03 &#x2F;bin&#x2F;bashroot@a10f25a91edf:&#x2F;# cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;root@a10f25a91edf:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# echo &quot;This web caontainer in dockek03-web03&quot; &gt; index.html root@a10f25a91edf:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# cat index.html This web caontainer in dockek03-web03[root@docker03 ~]# docker run -itd --name web04 -P nginx:latest [root@docker03 ~]# docker exec -it web04 &#x2F;bin&#x2F;bashroot@6d30a445c9b8:&#x2F;# cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;root@6d30a445c9b8:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# echo &quot;his web caontainer in dockek03-web04&quot; &gt; index.html root@6d30a445c9b8:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# cat index.html This web caontainer in dockek03-web04[root@docker03 ~]# curl 127.0.0.1:32768This web caontainer in dockek03-web03[root@docker03 ~]# curl 127.0.0.1:32769This web caontainer in dockek03-web04 更改nginx的配置文件 12345678910111213141516171819202122232425[root@docker01 ~]# cd &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;[root@docker01 nginx]# mkdir consul[root@docker01 nginx]# cd consul&#x2F;[root@docker01 consul]# pwd&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul[root@docker01 consul]# vim nginx.ctmpl[root@docker01 consul]# cat nginx.ctmpl upstream httpd_backend &#123; &#123;&#123;range service &quot;nginx&quot;&#125;&#125; server &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;; &#123;&#123; end &#125;&#125;&#125;server &#123; listen 8000; server_name localhost; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;http_backend; &#125;&#125;[root@docker01 ~]# vim &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf&#x2F;&#x2F;在文件最后，也就是倒数第二行添加：include &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;*.conf;&#x2F;&#x2F;使nginx的主配置文件能够识别到新产生的配置文件[root@docker01 ~]# nginx -s reload //使用consul-template命令，根据模板生产的配置文件，并重新加载nginx的配置文件 1[root@docker01 consul]# consul-template -consul-addr 192.168.1.70:8500 -template &quot;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;nginx.ctmpl:&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;vhost.conf:&#x2F;usr&#x2F;local&#x2F;sbin&#x2F;nginx -s reload&quot; //这时这个命令会占用终端，可以使用nohup命令让它保持后台运行 1[root@docker01 consul]# nohup consul-template -consul-addr 192.168.1.70:8500 -template &quot;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;nginx.ctmpl:&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;vhost.conf:&#x2F;usr&#x2F;local&#x2F;sbin&#x2F;nginx -s reload&quot; &amp; //此时，应该能够看到，新生产的vhost.conf配置文件已经生效，访问本机的8000端口可以得到不同容器提供的服务 12345678910111213141516171819202122232425[root@docker01 ~]# cd &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;[root@docker01 consul]# lsnginx.ctmpl vhost.conf[root@docker01 consul]# cat vhost.confupstream httpd_backend &#123; server 192.168.1.60:32768; server 192.168.1.60:32769; server 192.168.1.60:32770; server 192.168.1.50:32768; server 192.168.1.50:32769; &#125;server &#123; listen 8000; server_name localhost; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;httpd_backend; &#125;&#125; 12345678[root@docker01 consul]# curl localhost:8000This web caontainer in dockek02-web01[root@docker01 consul]# curl localhost:8000This web caontainer in dockek02-web02[root@docker01 consul]# curl localhost:8000This web caontainer in dockek03-web03[root@docker01 consul]# curl localhost:8000This web caontainer in dockek03-web04 当然，这时不管是添加新的nginx的web容器，或是删除，生产的配置文件都会时时更新，这是我们在运行consul-template这条命令最后添加：/usr/local/sbin/nginx -s reload它的作用 //删除之前的test容器，查看vhost文件 123456789101112[root@docker02 ~]# docker rm -f test[root@docker01 consul]# cat vhost.conf upstream httpd_backend &#123; server 192.168.1.60:32769; server 192.168.1.60:32770; server 192.168.1.50:32768; server 192.168.1.50:32769;&#125; //在运行一个web05，查看vhost文件的变化 12345678910111213[root@docker02 ~]# docker run -itd --name web05 -P nginx:latest upstream httpd_backend &#123; server 192.168.1.60:32769; server 192.168.1.60:32770; server 192.168.1.60:32771; server 192.168.1.50:32768; server 192.168.1.50:32769;&#125; ) 1、docker01主机上以二进制包的方式部署consul服务并后台运行，其身份为leader 2、docker02、docker03以容器的方式运行consul服务，并加入到docker01的consul群集中 3、在主机docker02、docker03上后台运行registrator容器，使其自动发现docker容器提供的服务，并发送给consul 4、在docker01上部署Nginx，提供反向代理服务，docker02、docker03主机上基于Nginx镜像，各运行两个web容器，提供不同的网页文件，以便测试效果 5、在docker01上安装consul-template命令，将收集到的信息（registrator收集到容器的信息）写入template模板中，并且最终写入Nginx的配置文件中 6、至此，实现客户端通过访问Nginx反向代理服务器（docker01），获得docker02、docker03服务器上运行的Nginx容器提供的网页文件 Consul：分布式、高可用的，服务发现和配置的工具，数据中心 Registrator：负责收集dockerhost上，容器服务的信息，并且发送给consul registrator是一个能自动发现docker container提供的服务，并在后端服务注册中心注册服务或取消服务的工具，后端注册中心支持consul、etcd、skydns2、zookeeper等 Consul-template：根据编辑好的模板生成新的nginx配置文件，并且负责加载nginx配置文件","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"}]},{"title":"Docker数据持久化","slug":"Docker数据持久化","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.224Z","comments":true,"path":"Docker数据持久化.html","link":"","permalink":"https://pdxblog.top/Docker%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96.html","excerpt":"","text":"Docker数据持久化为什么要做数据持久化： 因为Docker容器本身就是一个进程，可能会因为某些原因，或某些错误导致进程被杀死，这样数据就会丢失。 Docker容器是有生命周期的，生命周期结束，进程也会被杀死，数据就会丢失，因此需要做数据持久化，保证数据不会丢失 Storage Driver数据存储 Centos7版本的Docker，Storage Driver为：Overlay2； backing filesystem：xfs Data VolumeBind mount持久化存储：本质上是DockerHost文件系统中的目录或文件，能够直接被Mount到容器的文件系统中，在运行容器时，可以通过-v实现 特点： Data Volume是目录或文件，不能是没有格式化的磁盘（块设备） 容器可以读写volume中的数据 volume数据可以永久保存，即使用它的容器已经被销毁 小实验： 运行一个nginx服务，做数据持久化 12345678910[root@docker01 ~]# mkdir html[root@docker01 ~]# cd html&#x2F;[root@docker01 html]# echo &quot;This is a testfile in dockerHost.&quot; &gt; index.html[root@docker01 html]# cat index.html This is a testfile in dockerHost.[root@docker01 ~]# docker run -itd --name testweb -v &#x2F;root&#x2F;html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html nginx:latest[root@docker01 ~]# docker inspect testweb&quot;Gateway&quot;: &quot;172.17.0.1&quot;,[root@docker01 ~]# curl 172.17.0.2This is a testfile in dockerHost. PS:DockerHost上需要挂在的源文件或目录，必须是已经存在的，否则，当做一个目录挂在到容器中 默认挂载到容器内的文件，容器是有读写权限，可以在运行容器时-v后边加”:ro”限制容器的写入权限 并且还可以挂在单独文件到容器内部，一般它的使用场景是：如果不想对整个目录进行覆盖，而只希望添加某个文件，就可以使用挂载单个文件 Docker Manager Volume1[root@docker01 ~]# docker run -itd --name t2 -P -v &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html nginx:latest 删除容器的操作，默认不会对dockerHost上的文件操作，如果想要在删除容器时把源文件也删除，可以在删除容器时添加-v选型（一般不推荐使用这种方式，因为文件有可能被其他容器使用） 容器与容器的数据共享：volume container：给其他容器提供volume存储卷的容器，并且可以提供bind mount，也可以提供docker manager volume //创建一个vc_data容器： 123[root@docker01 ~]# docker create --name vc_data \\&gt; -v ~&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\&gt; -v &#x2F;other&#x2F;useful&#x2F;tools busybox 容器的跨主机数据共享 docker01 dcoker02 docker03 httpd httpd nfs 要求： docker01和docker02的主目录是一样的 //docker03的操作： 1234567891011121314151617[root@docker03 ~]#yum -y install nfs-utils[root@docker03 ~]# mkdir &#x2F;datashare[root@docker03 ~]# vim &#x2F;etc&#x2F;exports[root@docker03 ~]# cat &#x2F;etc&#x2F;exports&#x2F;datashare *(rw,sync,no_root_squash)[root@docker03 ~]# systemctl start rpcbind[root@docker03 ~]# systemctl enable rpcbind[root@docker03 ~]# systemctl start nfs-server[root@docker03 ~]# systemctl enable nfs-server[root@docker03 ~]# vim &#x2F;datashare&#x2F;index.html[root@docker03 ~]# cat &#x2F;datashare&#x2F;index.html &lt;div id&#x3D;&quot;datetime&quot;&gt; &lt;script&gt; setInterval(&quot;document.getElementById(&#39;datetime&#39;).innerHTML&#x3D;new Date().toLocaleString();&quot;, 1000); &lt;&#x2F;script&gt;&lt;&#x2F;div&gt;bdqn-webshare //验证 123456[root@docker01 ~]# showmount -e 192.168.1.60Export list for 192.168.1.60:&#x2F;datashare *[root@docker02 ~]# showmount -e 192.168.1.60Export list for 192.168.1.60:&#x2F;datashare * //docker01的操作 12345678910[root@docker01 ~]# mkdir &#x2F;htdocs[root@docker01 ~]# mount -t nfs 192.168.1.60:&#x2F;datashare &#x2F;htdocs&#x2F;&#x2F;&#x2F;-t：指定类型（type）[root@docker01 ~]# cat &#x2F;htdocs&#x2F;index.html &lt;div id&#x3D;&quot;datetime&quot;&gt; &lt;script&gt; setInterval(&quot;document.getElementById(&#39;datetime&#39;).innerHTML&#x3D;new Date().toLocaleString();&quot;, 1000); &lt;&#x2F;script&gt;&lt;&#x2F;div&gt;bdqn-webshare //docker02的操作 123456789[root@docker02 ~]# mkdir &#x2F;htdocs[root@docker02 ~]# mount -t nfs 192.168.1.60:&#x2F;datashare &#x2F;htdocs&#x2F;[root@docker02 ~]# cat &#x2F;htdocs&#x2F;index.html &lt;div id&#x3D;&quot;datetime&quot;&gt; &lt;script&gt; setInterval(&quot;document.getElementById(&#39;datetime&#39;).innerHTML&#x3D;new Date().toLocaleString();&quot;, 1000); &lt;&#x2F;script&gt;&lt;&#x2F;div&gt;bdqn-webshare 这里先不考虑将代码写入镜像，先以这种方式，分别在docker01和docker02部署httpd服务 //docker01 12[root@docker01 ~]# docker run -itd --name bdqn-web1 -P -v &#x2F;htdocs&#x2F;:&#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs httpd:latestPS：查看端口映射0.0.0.0:32778-&gt;80&#x2F;tcp bdqn-web1 //docker02 12[root@docker02 ~]# docker run -itd --name bdqn-web2 -P -v &#x2F;htdocs&#x2F;:&#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs httpd:latestPS：查看端口映射0.0.0.0:32768-&gt;80&#x2F;tcp bdqn-web2 此时用浏览器访问，两个web服务的主界面是一样的，但如果NFS服务器上源文件丢失，则两个web服务都会异常 想办法将源数据写入镜像内，在基于镜像做一个vc_data容器，这里因为没有接触到docker-compose和docker swarm等docker编排工具，所以我们在docker01和docker02上手动创建镜像 123456789101112[root@docker01 ~]# cd &#x2F;htdocs&#x2F;[root@docker01 htdocs]# vim Dockerfile[root@docker01 htdocs]# cat Dockerfile FROM busyboxADD index.html &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs&#x2F;index.htmlVOLUME &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs[root@docker01 htdocs]# docker build -t back_data .[root@docker01 htdocs]# docker create --name back_container1 back_data:latest [root@docker01 htdocs]# docker run -itd --name web3 -P --volumes-from back_container1 httpd:latest [root@docker01 htdocs]# pwd&#x2F;htdocs[root@docker01 htdocs]# docker save &gt; back_data.tar back_data:latest //docker02上操作： 123456[root@docker02 ~]# cd &#x2F;htdocs&#x2F;[root@docker02 htdocs]# lsback_data.tar Dockerfile index.html[root@docker02 htdocs]# docker load &lt; back_data.tar [root@docker02 htdocs]# docker create --name back_container2 back_data:latest[root@docker02 htdocs]# docker run -itd --name web4 -P --volumes-from back_container2 httpd:latest 测试： 1[root@docker01 htdocs]# rm -rf index.html 通过浏览器访问，一开始运行的web1和web2容器，无法访问了。web3和web4还是可以访问的。 但是数据无法同步了","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"}]},{"title":"Docker网络","slug":"Docker网络","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.256Z","comments":true,"path":"Docker网络.html","link":"","permalink":"https://pdxblog.top/Docker%E7%BD%91%E7%BB%9C.html","excerpt":"","text":"Docker网络：原生网络12345[root@localhost ~]# docker network lsNETWORK ID NAME DRIVER SCOPEfcc280741b01 bridge bridge local9c09e5a698dc host host local03411a6d716c none null local None：什么都没有的网络 12[root@localhost ~]# docker run -itd --name none --network none busybox:latest[root@localhost ~]# docker exec -it none /bin/sh PS：用到None网络的容器，会发现它只有一个LoopBack回环的网络，没有Mac地址、IP等信息，意味着它不能跟外界通信，是被隔离起来的网络 使用场景： 隔离意味着安全，所以，此网络可以运行一些关于安全方面的验证码、校验码等服务 Bridge：桥接网络 1234[root@localhost ~]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.02428e69e324 no virbr0 8000.525400547d41 yes virbr0-nic docker0：在我们安装docker这个服务的时候，默认就会生产一张docker0的网卡，一般默认IP为172.17.0.1/16 12[root@localhost ~]# docker run -itd --name test1 busybox:latest [root@localhost ~]# docker exec -it test1 /bin/sh 容器默认使用的网络是docker0网络，docker0此时相当于一个路由器，基于此网络的容器，网段都是和docker0一致的 自定义网络自带了一个ContainerDNSserver功能（域名解析） bridge //创建一个bridge网络： 1[root@localhost ~]# docker network create -d bridge my_net //创建两个容器，应用自定义网络（my_net）： 12[root@localhost ~]# docker run -itd --name test3 --network my_net busybox:latest[root@localhost ~]# docker run -itd --name test4 --network my_net busybox:latest PS：自定义网络优点，它可以通过容器的名称通信 12[root@localhost ~]# docker exec -it test3 &#x2F;bin&#x2F;sh&#x2F; # ping test4 //创建一个自定义网络，并且指定网关和网段 1[root@localhost ~]# docker network create -d bridge --subnet 172.20.16.0&#x2F;24 --gateway 172.20.16.1 my_net2 //创建两个容器，应用自定义网络(my_net2)，并指定IP 12[root@localhost ~]# docker run -itd --name test5 --network my_net2 --ip 172.20.16.6 busybox:latest[root@localhost ~]# docker run -itd --name test6 --network my_net2 --ip 172.20.16.8 busybox:latest PS：如果想要给容器指定IP地址，那么自定义网络的时候，必须指定网关gateway和subnet网段选项 //实现不同网段之间的通信，在容器里再添加一块网卡： 1[root@localhost ~]# docker network connect my_net2 test4 ) 让外网能够访问容器的端口映射方法：1）手动指定端口映射关系： 1[root@localhost ~]# docker run -itd --name web1 -p 90:80 nginx:latest 2）从宿主机随机映射端口到容器： 1[root@localhost ~]# docker run -itd --name web2 -p 80 nginx:latest 3)从宿主机随机映射端口到容器，容器内所有暴露的端口，都会一一映射 1[root@localhost ~]# docker run -itd --name web4 -P nginx:latest join容器：container（共享网络协议栈）容器和容器之间 12[root@localhost ~]# docker run -itd --name web5 busybox:latest[root@localhost ~]# docker run -itd --name web6 --network container:web5 busybox:latest 123[root@localhost ~]# docker exec -it web6 &#x2F;bin&#x2F;sh&#x2F; # echo 123456 &gt; &#x2F;tmp&#x2F;index.html&#x2F; # httpd -h &#x2F;tmp&#x2F; 123[root@localhost ~]# docker exec -it web5 &#x2F;bin&#x2F;sh&#x2F; # wget -O - -q 127.0.0.1123456 //这时会发现，两个容器的IP地址一样 PS：这种方法的使用场景： 由于这种网络的特殊特性，一般在运行同一个服务，并且合格服务需要做监控，已经日志收集、或者网络监控的时候，可以选择这种网络 docker的跨主机网络解决方案 overlay（覆盖）的解决方案： 实验环境： docker01：192.168.1.70 docker02：192.168.1.60 docker03：192.168.1.50 暂时不考录防火墙和selinux安全问题 将3台dockerhost防火墙和selinux全部关闭，并分别更改主机名称 12345[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su -[root@docker01 ~]# systemctl stop firewalld[root@docker01 ~]# setenforce 0[root@docker01 ~]# systemctl disable firewalld 在docker01上操作： //运行consul服务：（数据中心–分布式的） 123[root@docker01 ~]# docker load &lt; myprogrium-consul.tar[root@docker01 ~]# docker run -d -p 8500:8500 -h consul --name consul \\ &gt; --restart always progrium&#x2F;consul -server -bootstrap PS:容器产生之后我们可以通过浏览器访问consul服务，验证consul服务是否正常，访问dockerHost加映射端口 ) 修改docker02和docker03的docker配置文件： //将IP和端口的映射关系，写入consul 123[root@docker02 ~]# vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service修改：ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd -H unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock -H tcp:&#x2F;&#x2F;0.0.0.0:2376 --cluster-store&#x3D;consul:&#x2F;&#x2F;192.168.1.70:8500 --cluster-advertise&#x3D;ens33:2376 PS:返回浏览器consul服务界面，找到KEY/VALUE—-&gt;Docker—-&gt;NODES，会看到刚刚加入的docker02和docker03的信息 ) ) ) 在docker02上创建一个自定义网络： 1234[root@docker02 ~]# docker network create -d overlay ov_net1[root@docker02 ~]# docker network lsNETWORK ID NAME DRIVER SCOPEfe92a0eff6a4 ov_net1 overlay global 在docker02上创建网络，我们可以看到它的SCOPE定义的时global（全局），意味着加入到consul这个服务的docker服务，都可以看到我们自定义的网络 同理如果是用此网络创建的容器，会有两张网卡，默认这张网卡是10.0.0.0网段，如果想要docker01也可以看到这个网络，那么也只需在docker01的docker配置文件添加相应内容即可 同理，因为是自定义网络，符合自定义网络的特性，可以直接通过docker容器的名称互相通信，当然也可以在自定义网络的时候，指定它的网段，那么使用此网络的容器也可以指定IP地址 )","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"}]},{"title":"Docker跨主机网络方案之MacVlan","slug":"Docker跨主机网络方案之MacVlan","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.271Z","comments":true,"path":"Docker跨主机网络方案之MacVlan.html","link":"","permalink":"https://pdxblog.top/Docker%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%96%B9%E6%A1%88%E4%B9%8BMacVlan.html","excerpt":"","text":"Docker跨主机网络方案之MacVlan实验环境： docker01 192.168.1.70 docker02 192.168.1.50 关闭防火墙和禁用selinux，更该主机名： 1234567[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# systemctl disable firewalld[root@localhost ~]# setenforce 0[root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - macvlan的单网络通信1）打开网卡的混杂模式//需要在docker01和docker02上都进行操作 12[root@docker01 ~]# ip link set ens33 promisc on[root@docker01 ~]# ip link show ens33 2）在docker01上创建macvlan网络 1234在这里插入代码片[root@docker01 ~]# docker network create -d macvlan --subnet 172.22.16.0&#x2F;24 --gateway 172.22.16.1 -o parent&#x3D;ens33 mac_net1[root@docker01 ~]# docker network lsNETWORK ID NAME DRIVER SCOPEe6860af70e90 mac_net1 macvlan local PS:-o parent=绑定在哪张网卡之上3）基于创建的macvlan网络运行一个容器 1[root@docker01 ~]# docker run -itd --name bbox1 --ip 172.22.16.10 --network mac_net1 busybox 4）在docker02上创建macvlan网络，注意与docker01上的macvlan网络一摸一样 1[root@docker02 ~]# docker network create -d macvlan --subnet 172.22.16.0&#x2F;24 --gateway 172.22.16.1 -o parent&#x3D;ens33 mac_net1 5）在docker02上，基于创建的macvlan网络运行一个容器，验证与docker01上容器的通信 1[root@docker02 ~]# docker run -itd --name bbox2 --network mac_net1 --ip 172.22.16.20 busybox macvlan的多网络通信1）docker01和docker02验证内核模块8021q封装macvlan需要解决的问题：基于真实的ens33网卡，生产新的虚拟网卡 123[root@docker01 ~]# modinfo 8021q&#x2F;&#x2F;如果内核模块没有开启，运行下边命令导入一下[root@docker01 ~]# modprobe 8021q 2）基于ens33创建虚拟网卡//修改ens33网卡配置文件： 1234[root@docker01 ~]# cd &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;[root@docker01 network-scripts]# vim ifcfg-ens33&#x2F;&#x2F;修改：BOOTPROTO&#x3D;manual &#x2F;&#x2F;手动模式 //手动添加虚拟网卡配置文件 123456789101112[root@docker01 network-scripts]# cp -p ifcfg-ens33 ifcfg-ens33.10&#x2F;&#x2F;PS：-p 保留源文件或目录的属性[root@docker01 network-scripts]# vim ifcfg-ens33.10BOOTPROTO&#x3D;noneIPADDR&#x3D;192.168.10.10PREFIX&#x3D;24GATEWAY&#x3D;192.168.10.1NAME&#x3D;ens33.10DEVICE&#x3D;ens33.10ONBOOT&#x3D;yesVLAN&#x3D;yes&#x2F;&#x2F;PS：这里注意，IP要和ens33网段做一个区分，保证网关和网段IP的一致性，设备名称和配置文件的一致性，并且打开VLAN支持模式 //创建第二个虚拟网卡配置文件 12345678910[root@docker01 network-scripts]# cp -p ifcfg-ens33.10 ifcfg-ens33.20[root@docker01 network-scripts]# vim ifcfg-ens33.20BOOTPROTO&#x3D;noneIPADDR&#x3D;192.168.20.20PREFIX&#x3D;24GATEWAY&#x3D;192.168.20.1NAME&#x3D;ens33.20DEVICE&#x3D;ens33.20ONBOOT&#x3D;yesVLAN&#x3D;yes 3）docker01上的操作，启用创建的虚拟网卡 12[root@docker01 network-scripts]# ifup ifcfg-ens33.10[root@docker01 network-scripts]# ifup ifcfg-ens33.20 4)基于虚拟网卡，创建macvlan网络 12[root@docker01 ~]# docker network create -d macvlan --subnet 172.16.10.0&#x2F;24 --gateway 172.16.10.1 -o parent&#x3D;ens33.10 mac_net10[root@docker01 ~]# docker network create -d macvlan --subnet 172.16.20.0&#x2F;24 --gateway 172.16.20.1 -o parent&#x3D;ens33.20 mac_net20 5)基于创建的虚拟网卡，创建macvlan网络 12[root@docker02 ~]# docker network create -d macvlan --subnet 172.16.10.0&#x2F;24 --gateway 172.16.10.1 -o parent&#x3D;ens33.10 mac_net10[root@docker02 ~]# docker network create -d macvlan --subnet 172.16.20.0&#x2F;24 --gateway 172.16.20.1 -o parent&#x3D;ens33.20 mac_net20 6)docker02上也创建虚拟网卡，并启用 1234567891011121314151617181920212223[root@docker01 ~]# scp &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33.10 &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33.20 root@192.168.1.50:&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;[root@docker02 network-scripts]# vim ifcfg-ens33BOOTPROTO&#x3D;manual[root@docker02 network-scripts]# vim ifcfg-ens33.10BOOTPROTO&#x3D;noneIPADDR&#x3D;192.168.10.11PREFIX&#x3D;24GATEWAY&#x3D;192.168.10.1NAME&#x3D;ens33.10DEVICE&#x3D;ens33.10ONBOOT&#x3D;yesVLAN&#x3D;yes[root@docker02 network-scripts]# vim ifcfg-ens33.20BOOTPROTO&#x3D;noneIPADDR&#x3D;192.168.20.21PREFIX&#x3D;24GATEWAY&#x3D;192.168.20.1NAME&#x3D;ens33.20DEVICE&#x3D;ens33.20ONBOOT&#x3D;yesVLAN&#x3D;yes[root@docker02 network-scripts]# ifup ifcfg-ens33.10 [root@docker02 network-scripts]# ifup ifcfg-ens33.20 7)基于macvlan网络创建容器，并指定IP地址，不过这里要注意，运行的同期与网络对应的网段相符合，还需要注意IP地址的唯一性 123456&#x2F;&#x2F;docker01[root@docker01 ~]# docker run -itd --name bbox10 --network mac_net10 --ip 172.16.10.10 192.168.1.70:5000&#x2F;busybox:v1 [root@docker01 ~]# docker run -itd --name bbox20 --network mac_net20 --ip 172.16.20.20 192.168.1.70:5000&#x2F;busybox:v1&#x2F;&#x2F;docker02[root@docker02 ~]# docker run -itd --name bbox11 --network mac_net10 --ip 172.16.10.11 192.168.1.70:5000&#x2F;busybox:v1[root@docker02 ~]# docker run -itd --name bbox21 --network mac_net20 --ip 172.16.20.21 192.168.1.70:5000&#x2F;busybox:v1 8)将VMware虚拟机的网络改为桥接9)进入容器测试通信在docker01上进入容器bbox10和docker02上的bbox11进行通信在docker01上进入容器bbox20和docker02上的bbox21进行通信 12345678910[root@docker01 ~]# docker exec -it bbox10 &#x2F;bin&#x2F;sh&#x2F; # ping 172.16.10.11PING 172.16.10.11 (172.16.10.11): 56 data bytes64 bytes from 172.16.10.11: seq&#x3D;0 ttl&#x3D;64 time&#x3D;0.668 ms64 bytes from 172.16.10.11: seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.335 ms[root@docker01 ~]# docker exec -it bbox20 &#x2F;bin&#x2F;sh&#x2F; # ping 172.16.20.21PING 172.16.20.21 (172.16.20.21): 56 data bytes64 bytes from 172.16.20.21: seq&#x3D;0 ttl&#x3D;64 time&#x3D;0.584 ms64 bytes from 172.16.20.21: seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.365 ms","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"}]},{"title":"Kubernetes集群部署","slug":"Kubernetes集群部署","date":"2020-01-23T16:00:00.000Z","updated":"2020-02-03T02:44:38.847Z","comments":true,"path":"Kubernetes集群部署.html","link":"","permalink":"https://pdxblog.top/Kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2.html","excerpt":"","text":"生产级别的容器编排系统 Kubernetes 是用于自动部署，扩展和管理容器化应用程序的开源系统 k8s 最基本的硬件要求 CPU:双核 Mem：2G 3台dockerhost时间必须同步 Kubeadm工具自动部署k8s集群 //给3台docker命名，禁用swap交换分区 12345678910111213[root@localhost ~]# hostnamectl set-hostname master[root@localhost ~]# su -[root@localhost ~]# hostnamectl set-hostname node01[root@localhost ~]# su -[root@localhost ~]# hostnamectl set-hostname node02[root@localhost ~]# su -[root@master ~]# swapoff -a &#x2F;&#x2F;临时禁用[root@master ~]# free total used free shared buff&#x2F;cache availableMem: 1867292 335448 908540 9256 623304 1290100Swap: 0 0 0&#x2F;&#x2F;永久禁用[root@master ~]# vim &#x2F;etc&#x2F;fstab &#x2F;&#x2F;注释掉swap那一行 //禁用selinux，防火墙，并关闭开机自启（三台都需要） 12345[root@master ~]# vim &#x2F;etc&#x2F;selinux&#x2F;configSELINUX&#x3D;disabled[root@master ~]# setenforce 0[root@master ~]# systemctl stop firewalld[root@master ~]# systemctl disable firewalld //编写hosts文件，设置域名解析 12345678[root@master ~]# vim &#x2F;etc&#x2F;hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.1.70 master192.168.1.50 node01192.168.1.40 node02[root@master ~]# scp &#x2F;etc&#x2F;hosts root@192.168.1.50:&#x2F;etc[root@master ~]# scp &#x2F;etc&#x2F;hosts root@192.168.1.40:&#x2F;etc //设置免密登录 123[root@master ~]# ssh-keygen -t rsa[root@master ~]# ssh-copy-id node01[root@master ~]# ssh-copy-id node02 //打开iptables的桥接功能，开启路由转发 1234567891011121314151617181920212223[root@master ~]# vim &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.confnet.bridge.bridge-nf-call-iptables &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1[root@master ~]# echo net.ipv4.ip_forward &#x3D; 1 &gt;&gt; &#x2F;etc&#x2F;sysctl.conf [root@master ~]# sysctl -pnet.ipv4.ip_forward &#x3D; 1[root@master ~]# sysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf &#x2F;&#x2F;如果这条命令不成功则需要添加一个模块[root@master ~]# modprobe br_netfilternet.bridge.bridge-nf-call-iptables &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1[root@master ~]# scp &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf node01:&#x2F;etc&#x2F;sysctl.d [root@master ~]# scp &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf node02:&#x2F;etc&#x2F;sysctl.d [root@master ~]# scp &#x2F;etc&#x2F;sysctl.conf node02:&#x2F;etc&#x2F; [root@master ~]# scp &#x2F;etc&#x2F;sysctl.conf node01:&#x2F;etc&#x2F;[root@node01 ~]# sysctl -pnet.ipv4.ip_forward &#x3D; 1[root@node01 ~]# sysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf net.bridge.bridge-nf-call-iptables &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1[root@node02 ~]# sysctl -pnet.ipv4.ip_forward &#x3D; 1[root@node02 ~]# sysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf net.bridge.bridge-nf-call-iptables &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1 //获取yum源 123456789101112[root@master ~]# cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo&gt; [kubernetes]&gt; name&#x3D;Kubernetes&gt; baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;&gt; enabled&#x3D;1&gt; gpgcheck&#x3D;1&gt; repo_gpgcheck&#x3D;1&gt; gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg&gt; EOF[root@master ~]# yum repolist[root@master ~]# yum makecache&#x2F;&#x2F;三台都需要这个yum源（node01，node02步骤省略） //安装以下三个组件kubectl：k8s客户端kubeadm：自动化快速部署k8s集群工具kubelet：客户端代理 1234[root@master ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0 kubectl-1.15.0-0&#x2F;&#x2F;node01、node02不需要安装kubectl[root@node01 ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0[root@node02 ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0 //加入开机自启（三台全部加入开机自启） 1[root@master ~]# systemctl enable kubelet //导入镜像 1234567891011121314[root@master ~]# mkdir images[root@master ~]# cd images&#x2F;[root@master images]# lscoredns-1-3-1.tar kube-apiserver-1-15.tar kube-proxy-1-15.tar myflannel-11-0.taretcd-3-3-10.tar kube-controller-1-15.tar kube-scheduler-1-15.tar pause-3-1.tar[root@master ~]# cat &gt; images.sh &lt;&lt;EOF&gt; #!&#x2F;bin&#x2F;bash&gt; for i in &#x2F;root&#x2F;images&#x2F;*&gt; do&gt; docker load &lt; $i&gt; done&gt; EOF[root@master ~]# chmod +x images.sh[root@master ~]# sh images.sh //初始化k8s集群 1234[root@master ~]# kubeadm init --kubernetes-version&#x3D;v1.15.0 \\&gt; --pod-network-cidr&#x3D;10.244.0.0&#x2F;16 \\&gt; --service-cidr&#x3D;10.96.0.0&#x2F;12 \\&gt; --ignore-preflight-errors&#x3D;Swap //如果初始化失败，需要重置k8s集群 1[root@master ~]# kubeadm reset //初始化完成后的操作 123[root@master ~]# mkdir -p $HOME&#x2F;.kube[root@master ~]# cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config[root@master ~]# chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config //查看节点信息情况 123[root@master ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONmaster NotReady master 10m v1.15.0 //部署flannel网络，（k8s版本必须是1.7版本以上） 1[root@master ~]# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;coreos&#x2F;flannel&#x2F;master&#x2F;Documentation&#x2F;kube-flannel.yml PS：这里执行不成功的话可能是网络的问题 //在node01、node02上提前导入镜像（不然在加入集群的时候，它会自动下载镜像） 12345[root@node02 ~]# mkdir images[root@node02 ~]# cd images&#x2F;[root@node02 images]# lskube-proxy-1-15.tar myflannel-11-0.tar pause-3-1.tardocker load &lt; kube-proxy-1-15.tar &amp;&amp; docker load &lt; myflannel-11-0.tar &amp;&amp; docker load &lt; pause-3-1.tar //node01、node02加入集群 1234567kubeadm join 192.168.1.70:6443 --token x85ks8.4x5qrhw87zct1vti \\ --discovery-token-ca-cert-hash sha256:227c69c29f16521a7dccb52104710b8cdd449aa0f7cb787affb62514fc8cc9eb[root@master ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONmaster Ready master 25m v1.15.0node01 Ready &lt;none&gt; 82s v1.15.0node02 Ready &lt;none&gt; 76s v1.15.0 //确保是running的状态 1234567891011121314[root@master ~]# kubectl get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-5c98db65d4-fr894 1&#x2F;1 Running 0 28mkube-system coredns-5c98db65d4-qkqh5 1&#x2F;1 Running 0 28mkube-system etcd-master 1&#x2F;1 Running 0 27mkube-system kube-apiserver-master 1&#x2F;1 Running 0 27mkube-system kube-controller-manager-master 1&#x2F;1 Running 0 27mkube-system kube-flannel-ds-amd64-rjnns 1&#x2F;1 Running 0 4m44skube-system kube-flannel-ds-amd64-tpkh5 1&#x2F;1 Running 0 4m50skube-system kube-flannel-ds-amd64-x425t 1&#x2F;1 Running 0 13mkube-system kube-proxy-4qsj2 1&#x2F;1 Running 0 4m44skube-system kube-proxy-gngnx 1&#x2F;1 Running 0 28mkube-system kube-proxy-shkw9 1&#x2F;1 Running 0 4m50skube-system kube-scheduler-master 1&#x2F;1 Running 0 27m //设置tab键的距离 123[root@master ~]# vim .vimrcset tabstop&#x3D;2[root@master ~]# source .vimrc //将kubectl命令加入tab自动补全 123[root@master ~]# source &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;bash_completion [root@master ~]# source &lt;(kubectl completion bash)[root@master ~]# echo &quot; source &lt;(kubectl completion bash)&quot; &gt;&gt; ~&#x2F;.bashrc","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"Pod资源对象+健康检查","slug":"Pod资源对象+健康检查","date":"2020-01-23T16:00:00.000Z","updated":"2020-03-09T01:24:28.655Z","comments":true,"path":"Pod资源对象+健康检查.html","link":"","permalink":"https://pdxblog.top/Pod%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1+%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5.html","excerpt":"","text":"Deployment、Service、Pod是k8s最核心的3个资源对象Deployment： 最常见的无状态的控制器，支持应用的扩容缩容、滚动更新等操作 Service： 为弹性变动且存在生命周期的Pod对象提供了一个固定的访问接口，用户服务发现和服务访问 Pod： 是运行容器以及调度的最小单位，同一个Pod可以同时运行多个容器，这些容器共享NET、UTS、IPC，除此之外还有USER、PID、MOUNT ReplicationController：（rc） 用于确保每个Pod副本在任意时刻都能满足目标数量，简单点来说，它用于保证每个容器或容器组总是运行并且可以访问：老一代无状态的Pod控制器 ReplicaSet：（rs） 新一代无状态的Pod应用控制器，它与rc的不同之处在于支持的标签选择器不同，rc只支持等值选择器，rs还额外支持基于集合的选择器 StatefulSet： 用于管理有状态的持久化应用，如database服务程序，它与Deployment不同之处在于，它会为每一个Pod创建一个独有的持久性标识符，并确保每个Pod之间的顺序性 DamonSet： 确保每一个节点都运行了某个Pod的一个副本，新增的节点一样会被添加此类Pod，在节点移除时Pod会被收回 Job： 用于管理运行完成后即可终止的银应用，例如批量处理作业任务 volume: PV PVC ConfigMap: Secret： Role： ClusterRole： ClusterRoleBinding： Service account： Helm： Namespace：名称空间 默认的名称空间：Default //查看名称空间 123456[root@master ~]# kubectl get nsNAME STATUS AGEdefault Active 6d22hkube-node-lease Active 6d22hkube-public Active 6d22hkube-system Active 6d22h //查看名称空间详细信息 1[root@master ~]# kubectl describe ns default //创建名称空间 1[root@master ~]# kubectl create ns bdqn //使用yaml创建名称空间 123456[root@master ~]# vim test-ns.yamlapiVersion: v1kind: Namespacemetadata: name: test[root@master ~]# kubectl apply -f test-ns.yaml PS： namespace资源对象仅用于资源对象的隔离，并不能隔绝不同名称空间的Pod之间的通信，那是网络策略资源的功能 //删除名称空间： 12[root@master ~]# kubectl delete ns test[root@master ~]# kubectl delete -f test-ns.yaml //查看指定名称空间的资源可以使用–namespace或者-n选项 12[root@master ~]# kubectl get pod --namespace&#x3D;bdqn [root@master ~]# kubectl get pod -n bdqn Pod //通过yaml文件手动创建pod 12345678910111213141516[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-pod namespace: bdqn &#x2F;&#x2F;指定名称空间spec: containers: - name: test-app image: httpd:v1[root@master ~]# kubectl apply -f pod.yaml[root@master ~]# kubectl get pod -n bdqnNAME READY STATUS RESTARTS AGEtest-pod 1&#x2F;1 Running 0 19s&#x2F;&#x2F;删除[root@master ~]# kubectl delete pod -n bdqn test-pod Pod中镜像获取策略： Always： 镜像标签为“lastest”或镜像不存在时，总是从指定的仓库中获取镜像 ifNotPresent： 仅当本地镜像不存在时从目标仓库中下载 Never： 禁止从仓库下载镜像，即只是用本地镜像 123456789101112131415kind: PodapiVersion: v1metadata: name: test-pod namespace: bdqn labels: app: test-webspec: containers: - name: test-app image: httpd:v1 imagePullPolicy: IfNotPresent &#x2F;&#x2F;指定镜像获取策略 ports: - protocol: TCP containerPort: 90 &#x2F;&#x2F;手动创建的Pod，指定容器暴露的端口也不会生效 PS： 对于标签为“latest”或者这标签不存在，其默认镜像下载策略为“Always”,而对于其他标签的镜像，默认策略为“ifNotPresent” 容器的重启策略 Always： 但凡Pod对象终止就将其重启，此为默认设定 OnFailure： 仅在Pod对象出现错误时才将其重启 Never： 从不重启 PS：对于标签为“latest”或者不存在的时候，其默认镜像下载策略为“Always”，而对于其他标签的镜像，默认策略为“IfNotPresent” Pod的默认健康检查 12345678910111213141516171819202122[root@master ~]# vim healcheck.yamlapiVersion: v1kind: Podmetadata: labels: test: healcheck name: healcheckspec: restartPolicy: OnFailure containers: - name: healthcheck image: busybox args: &#x2F;&#x2F;由镜像运行成容器的时候去做的事情 - &#x2F;bin&#x2F;sh - -c - sleep 20; exit 1[root@master ~]# vim healcheck.yaml[root@master ~]# kubectl get pod -w &#x2F;&#x2F;实时查看它的状态[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEhealthcheck 0&#x2F;1 Error 5 8m44s&#x2F;&#x2F;可以看到每过20s就会重启一次 LivenessProbe（活跃度探测） 1234567891011121314151617181920212223[root@master ~]# vim liveness.yamlkind: PodapiVersion: v1metadata: name: liveness labels: test: livenessspec: restartPolicy: OnFailure containers: - name: liveness image: busybox args: &#x2F;&#x2F;由镜像运行成容器的时候去做的事情 - &#x2F;bin&#x2F;sh - -c - touch &#x2F;tmp&#x2F;test; sleep 60; rm -rf &#x2F;tmp&#x2F;test; sleep 300 livenessProbe: &#x2F;&#x2F;活跃度探测 exec: command: - cat - &#x2F;tmp&#x2F;test initialDelaySeconds: 10 &#x2F;&#x2F;pod运行10秒后开始探测 periodSeconds: 5 &#x2F;&#x2F;每5秒探测一次 PS: Liveness活跃度探测，根据某个文件是否存在，来确认某个服务是否正常运行，如果存在则正常，否则，它会根据你设置的Pod的重启策略操作Pod Readiness（敏捷探测、就绪性探测） 1234567891011121314151617181920212223[root@master ~]# vim readiness.yaml kind: PodapiVersion: v1metadata: name: readiness labels: test: readinessspec: restartPolicy: OnFailure containers: - name: readiness image: busybox args: - &#x2F;bin&#x2F;sh - -c - touch &#x2F;tmp&#x2F;test; sleep 60; rm -rf &#x2F;tmp&#x2F;test; sleep 300 readinessProbe: exec: command: - cat - &#x2F;tmp&#x2F;test initialDelaySeconds: 10 periodSeconds: 5 PS：总结liveness和readiness探测 1）leveness和readiness是两种健康检查机制，如果不特意配置，k8s两种探测采取相同的默认行为，即通过判断容器启动进程的返回是否为零，来判断探测是否成功 2）两种探测配置方法完全一样，不同之处在于探测失败后的行为： liveness探测是根据Pod重启策略操作容器，大多数是重启容器 readinesss则是将容器设置为不可用，不接收Service转发的请求 3）两种探测方法可以独立存在，也可以同时使用，用liveness判断容器是否需要实现自愈；用readiness判断容器是否已经准备好对外提供服务 监控检测应用 在scale（扩容、缩容）中的应用 123456789101112131415161718192021222324252627282930313233343536373839404142[root@master ~]# vim hcscal.yamlkind: DeploymentapiVersion: extensions&#x2F;v1beta1metadata: name: webspec: replicas: 3 template: metadata: labels: run: web spec: containers: - name: web image: httpd ports: - containerPort: 80 readinessProbe: httpGet: scheme: HTTP path: &#x2F;healthy port: 80 initialDelaySeconds: 10 periodSeconds: 5---kind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort selector: run: web ports: - protocol: TCP port: 90 targetPort: 80 nodePort: 30321[root@master ~]# kubectl exec web-69d659f974-ktqbz touch &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs&#x2F;healthy[root@master ~]# kubectl describe svcEndpoints: 10.244.1.4:80 在更新过程中的使用 1234567891011121314151617181920212223242526[root@master ~]# vim app.v1.yamlkind: DeploymentapiVersion: extensions&#x2F;v1beta1metadata: name: appspec: replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - &#x2F;bin&#x2F;sh - -c - sleep 10; touch &#x2F;tmp&#x2F;healthy; sleep 3000 readinessProbe: exec: command: - cat - &#x2F;tmp&#x2F;healthy initialDelaySeconds: 10 periodSeconds: 5 //第一次升级 12345678[root@master ~]# kubectl apply -f app.v1.yaml --record[root@master ~]# cp app.v1.yaml app.v2.yaml [root@master ~]# vim app.v2.yaml&#x2F;&#x2F;修改 args: - &#x2F;bin&#x2F;sh - -c - sleep 3000 //第二次升级 123456789101112131415161718192021[root@master ~]# cp app.v1.yaml app.v3.yaml [root@master ~]# vim app.v3.yaml&#x2F;&#x2F;删除探测机制kind: DeploymentapiVersion: extensions&#x2F;v1beta1metadata: name: appspec: replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - &#x2F;bin&#x2F;sh - -c - sleep 3000 maxSurge：此参数控制滚动更新过程中，副本总数超过预期数的值，可以是整数，也可以是百分比，默认为1 maxUnavilable：不可用Pod的值，默认为1","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"Prometheus（普罗米修斯）","slug":"Prometheus（普罗米修斯）","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.287Z","comments":true,"path":"Prometheus（普罗米修斯）.html","link":"","permalink":"https://pdxblog.top/Prometheus%EF%BC%88%E6%99%AE%E7%BD%97%E7%B1%B3%E4%BF%AE%E6%96%AF%EF%BC%89.html","excerpt":"","text":"Prometheus（普罗米修斯）是一个系统和服务的监控平台。它可以自定义时间间隔从已配置的目标收集指标，评估规则表达式，显示结果，并在发现某些情况时触发警报 与其他监视系统相比，Prometheus的主要区别特征是： 一个多维数据模型（时间序列由指标名称定义和设置键/值尺寸） 一个灵活的查询语言来利用这一维度 不依赖于分布式存储；单服务器节点是自治的 时间序列收集通过HTTP 上的拉模型进行 通过中间网关支持推送时间序列 通过服务发现或静态配置发现目标 多种图形和仪表板支持模式 支持分层和水平联合 实验环境： docker01 192.168.1.70 NodeEXporter cAdvisor+Prometheus server+gragana docker02 192.168.1.60 NodeEXporter cAdvisor docker03 192.168.1.50 NodeEXporter cAdvisor 123456[root@localhost ~]# hostnamectl set-hostname docker1[root@localhost ~]# su -[root@localhost ~]# hostnamectl set-hostname docker2[root@localhost ~]# su -[root@localhost ~]# hostnamectl set-hostname docker3[root@localhost ~]# su - 全部关闭防火墙，禁用selinux 123[root@docker01 ~]# systemctl stop firewalld[root@docker01 ~]# systemctl disable firewalld[root@docker01 ~]# setenforce 0 需要部署的组件 Prometheus server（9090）：普罗米修斯的主机服务器 NodeEXporter（9100）：负责收集host硬件信息和操作系统信息 //谷歌开发的监控软件。收集数据，不太直观。有历史保留，方便后期做优化 cAdvisor（8080）：负责收集host上运行的容器信息 Grafana（3000）：负责展示普罗米修斯监控界面 //类似于kibana的图形界面，提供可视化web页面 1）三个节点，全部部署node-exporter和cadvisor //分别在三个节点上导入镜像 1[root@docker01 ~]# docker load &lt; node-exporter.tar &amp;&amp; docker load &lt; mycadvisor.tar //部署node-exporter，收集硬件和系统信息（三台都需要部署） 12345[root@docker01 ~]# docker run -d -p 9100:9100 -v &#x2F;proc:&#x2F;host&#x2F;proc \\&gt; -v &#x2F;sys:&#x2F;host&#x2F;sys -v &#x2F;:&#x2F;rootfs --net&#x3D;host \\&gt; prom&#x2F;node-exporter --path.procfs &#x2F;host&#x2F;proc \\&gt; --path.sysfs &#x2F;host&#x2F;sys \\&gt; --collector.filesystem.ignored-mount-points &quot;^&#x2F;(sys|proc|dev|host|etc)($|&#x2F;)&quot; PS：注意这里使用了–net=host，这样prometheus server可以直接与node-exporter通信 验证，打开浏览器验证结果（192.168.1.70:9100） ) ) //部署安装cAdvisor，收集节点容器信息（三台都需要部署） 1234[root@docker01 ~]# docker run -v &#x2F;:&#x2F;rootfs:ro -v &#x2F;var&#x2F;run:&#x2F;var&#x2F;run&#x2F;:rw \\&gt; -v &#x2F;sys:&#x2F;sys:ro -v &#x2F;var&#x2F;lib&#x2F;docker:&#x2F;var&#x2F;lib&#x2F;docker:ro \\&gt; -p 8080:8080 --detach&#x3D;true --name&#x3D;cadvisor \\&gt; --net&#x3D;host google&#x2F;cadvisor 打开浏览器验证（192.168.1.70:8080） ) 2）在docker01上部署prometheus server服务 在部署prometheus之前，我们需要对它的配置文件进行修改，所以我们先运行一个容器，将其配置文件拷贝出来 123456789[root@docker01 ~]# docker load &lt; prometheus.tar[root@docker01 ~]# docker run -d -p 9090:9090 --name prometheus --net&#x3D;host prom&#x2F;prometheus[root@docker01 ~]# docker exec -it prometheus &#x2F;bin&#x2F;sh&#x2F;prometheus $ cd &#x2F;etc&#x2F;prometheus&#x2F;&#x2F;etc&#x2F;prometheus $ lsconsole_libraries consoles prometheus.yml[root@docker01 ~]# docker cp prometheus:&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml .&#x2F;[root@docker01 ~]# vim prometheus.yml- targets: [&#39;localhost:9090&#39;,&#39;localhost:8080&#39;,&#39;localhost:9100&#39;,&#39;192.168.1.60:8080&#39;,&#39;192.168.1.60:9100&#39;,&#39;192.168.1.50:8080&#39;,&#39;192.168.1.50:9100&#39;] PS：这里制定了prometheus的监控项，包括它也会监控自己收集到的数据 1[root@docker01 ~]# docker rm -f prometheus //重新运行容器： 1[root@docker01 ~]# docker run -d -p 9090:9090 --name prometheus --net&#x3D;host -v &#x2F;root&#x2F;prometheus.yml:&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml prom&#x2F;prometheus //浏览器访问：192.168.1.70:9090 ) ) PS:这里能够看到我们各个监控项 3）在docker01上部署grafana服务，用来展示prometheus收集到的数据 1234[root@docker01 ~]# docker load &lt; grafana.tar[root@docker01 ~]# mkdir grafana-storage[root@docker01 ~]# chmod 777 -R grafana-storage&#x2F;[root@docker01 ~]# docker run -d -p 3000:3000 --name grafana -v &#x2F;root&#x2F;grafana-storage:&#x2F;var&#x2F;lib&#x2F;grafana -e &quot;GF_SECURITY_ADMIN_PASSWORD&#x3D;123.com&quot; grafana&#x2F;grafana 浏览器访问：192.168.1.70:3000 用户名：admin 密码：123.com ) //创建数据源 ) ) ) ) PS：看到这这提示，说明prometheus和grafana服务是正常连接的 此时，虽然grafana收集到了数据，但怎么显示它，仍然是个问题，grafana支持自定义显示信息，不过要自定义起来非常麻烦，不过好在，grafana官方为我们提供了一些模板，来供我们使用 Grafana官网：https://grafana.com ) //可以根据自己的喜好选择模板 ) 选中一款模板后，然后，我们又两种方式可以套用这个模板 第一种方式：通过JSON文件使用模板 ) 下载完成之后，回到grangana控制台 ) ) ) ) 第二种方式： 可以直接通过模板的ID号 ) ) ) ) ) 配置AlertManagerAlertManager：用来接收prometheus发送的报警信息，并且执行设置好的报警方式、报警内容 AlertManager.yml配置文件： global：全部配置，包括报警解决后的超时时间、SMTP相关配置、各种渠道通知的API地址等新消息 route：用来设置报警的分发策略 receivers：配置告警消息接收者信息 inhibit_rules：抑制规则配置，当存在于另一组匹配的报警时，抑制规则将禁用一组匹配的警报 //运行一个容器获取配置文件并进行配置 12345678910111213141516171819202122232425262728[root@docker01 ~]# docker run -d --name alertmanager -p 9093：993 prom&#x2F;alertmanager:latest[root@docker01 ~]# docker cp alertmanager:&#x2F;etc&#x2F;alertmanager&#x2F;alertmanager .&#x2F;[root@docker01 ~]# cat alertmanager.yml global: resolve_timeout: 5m smtp_from: &#39;2960824193@qq.com&#39; smtp_smarthost: &#39;smtp.qq.com:465&#39; smtp_auth_username: &#39;2960824193@qq.com&#39; smtp_auth_password: &#39;aseydtzejqfqdhai&#39; smtp_require_tls: false smtp_hello: &#39;qq.com&#39;route: group_by: [&#39;alertname&#39;] group_wait: 5s group_interval: 5s repeat_interval: 5m receiver: &#39;email&#39;receivers:- name: &#39;email&#39; email_configs: - to: &#39;2960824193@qq.com&#39; send_resolved: trueinhibit_rules: - source_match: severity: &#39;critical&#39; target_match: severity: &#39;warning&#39; equal: [&#39;alertname&#39;, &#39;dev&#39;, &#39;instance&#39;] //删除容器并重新运行 12[root@docker01 ~]# docker rm -f alertmanager[root@docker01 ~]# docker run -d --name alertmanager -p 9093:9093 -v &#x2F;root&#x2F;alertmanager.yml:&#x2F;etc&#x2F;alertmanager&#x2F;alertmanager.yml prom&#x2F;alertmanager:latest //浏览器访问：192.168.1.70:9093 ) Prometheus配置alertmanager报警规则 1234567891011121314151617181920212223[root@docker01 ~]# mkdir -p prometheus&#x2F;rules[root@docker01 ~]# cd prometheus&#x2F;rules&#x2F;[root@docker01 rules]# lsnode-up.rules[root@docker01 rules]# cat node-up.rules groups:- name: node-up rules: - alert: node-up expr: up&#123;job&#x3D;&quot;prometheus&quot;&#125; &#x3D;&#x3D; 0 &#x2F;&#x2F;这个job要和prometheus里的job名称一样 for: 15s labels: severity: 1 team: node annotations: summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 已停止运行超过 15s！&quot;-----------------------------------------------------------------------------------------------&#x2F;etc&#x2F;prometheus $ cat prometheus.yml.......- job_name: &#39;prometheus&#39;&#x2F;&#x2F;监控的内容是： - targets: [&#39;localhost:9090&#39;,&#39;localhost:8080&#39;,&#39;localhost:9100&#39;,&#39;192.168.1.60:9100&#39;,&#39;192.168.1.60:8080&#39;,&#39;192.168.1.50:9100&#39;,&#39;192.168.1.50:8080&#39;]....... //编辑prometheus的配置文件 12345678910[root@docker01 ~]# vim prometheus.yml# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: - 192.168.1.70:9093 &#x2F;&#x2F;目标为alertmanager容器rule_files: - &quot;&#x2F;usr&#x2F;local&#x2F;prometheus&#x2F;rules&#x2F;*.rules&quot; &#x2F;&#x2F;路径是容器内的路径 //删除prometheus容器，重新挂载配置文件 12[root@docker01 ~]# docker rm -f prometheus[root@docker01 ~]# docker run -d -p 9090:9090 --name prometheus --net&#x3D;host -v &#x2F;root&#x2F;prometheus.yml:&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml -v &#x2F;root&#x2F;prometheus&#x2F;rules&#x2F;node-up.rules:&#x2F;usr&#x2F;local&#x2F;prometheus&#x2F;rules&#x2F;node-up.rules prom&#x2F;prometheus //浏览器访问：192.168.1.70:9090 ) //随便关闭一个容器进行测试 ) AlertManager配置自定义邮件模板 1234567891011121314151617[root@docker01 ~]# mkdir prometheus&#x2F;alertmanager-tmpl[root@docker01 ~]# cd prometheus&#x2F;alertmanager-tmpl&#x2F;[root@docker01 alertmanager.tmpl]# cat email.tmpl &#123;&#123; define &quot;email.from&quot; &#125;&#125;2960824193@qq.com&#123;&#123; end &#125;&#125; &#x2F;&#x2F;哪个邮箱来发送信息&#123;&#123; define &quot;email.to&quot; &#125;&#125;2960824193@qq.com&#123;&#123; end &#125;&#125; &#x2F;&#x2F;发送到哪个邮箱&#123;&#123; define &quot;email.to.html&quot; &#125;&#125;&#123;&#123; range .Alerts &#125;&#125;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;start&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&lt;br&gt;告警程序: prometheus_alert&lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; 级&lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125;&lt;br&gt;故障主机: &#123;&#123; .Labels.instance &#125;&#125;&lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125;&lt;br&gt;触发时间: &#123;&#123; .StartsAt.Format &quot;2019-08-04 16:58:15&quot; &#125;&#125; &lt;br&gt;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;end&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125; //修改altermanager的配置文件 1234567[root@docker01 ~]# vim alertmanager.yml&#x2F;&#x2F;在第九行添加templates: - &#39;&#x2F;etc&#x2F;alertmanager-tmpl&#x2F;*.tmpl&#39; &#x2F;&#x2F;容器内的路径&#x2F;&#x2F;修改第20行，和第21行： - to: &#39;&#123;&#123; template &quot;email.to&quot; &#125;&#125;&#39; &#x2F;&#x2F;必须和email.tmpl中的&#123;&#123; define “email.to”&#125;&#125; 2960824193@qq.com&#123;&#123;end&#125;&#125;对应 html: &#39;&#123;&#123; template &quot;email.to.html&quot; . &#125;&#125;&#39; &#x2F;&#x2F;必须和email.tml中的&#123;&#123; define &quot;email.to.html&quot; &#125;&#125;名字对应 //删除alertmanager容器，重新运行并挂载 12[root@docker01 ~]# docker rm -f alertmanager[root@docker01 ~]# docker run -d --name alertmanager -p 9093:9093 -v &#x2F;root&#x2F;alertmanager.yml:&#x2F;etc&#x2F;alertmanager&#x2F;alertmanager.yml -v &#x2F;root&#x2F;prometheus&#x2F;alertmanager-tmpl&#x2F;:&#x2F;etc&#x2F;altermanager-tmpl prom&#x2F;alertmanager:latest //停止容器测试 )","categories":[{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"k8s架构、基本概念","slug":"k8s架构、基本概念","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.303Z","comments":true,"path":"k8s架构、基本概念.html","link":"","permalink":"https://pdxblog.top/k8s%E6%9E%B6%E6%9E%84%E3%80%81%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.html","excerpt":"","text":"k8s总架构：) Master节点：（默认不参加工作） kubectl：k8s是命令端，用来发送客户端的操作指令 k8s的原生组件：（部署k8s比必不可少的组件） API server：是k8s集群的前端接口，各种客户端工具以及k8s的其他组件可以通过它管理k8s集群的各种资源，它提供了HTTP/HTTPS RESTful API，即k8s API Scheduler：负责决定将Pod放在哪个Node上运行，在调度时，会充分考虑集群内的拓扑结构，当前各个节点的负载情况，以及对高可用、性能、数据和亲和性需求 Controller Manager：负责管理集群的各种资源，保证资源处于预期的状态，它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等等 Etcd：负责保存k8s集群的配置信息和各种资源的状态信息，当数据发生变化时，etcd会快速的通知k8s相关组件。第三方组件，意味着它有可替换方案，比如：Consul、zookeeper Pod：k8s集群的最小组成单位，一个Pod内，可以运行一个或多个容器，大多数情况下，一个Pod内只有一个Container容器 Flannel：是k8s集群网络解决方案，可以保证Pod的跨主机通信。第三方解决方案，也有替换方案 Node节点：kubelet：它是Node的agent（代理），当Scheduler确定某个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet，kubelet会根据这些信息创建和运行容器，并向Master报告运行状态 kube-proxy：负责将访问service的TCP/UDP数据流转发到后端容器，如果有多个副本，kube-pory会实现负载均衡 运行一个例子： //创建一个deployment资源对象。Pod控制器 1[root@master ~]# kubectl run test-web --image&#x3D;httpd --replicas&#x3D;2 分析各个组件的作用以及架构流程： kubectl发送部署请求到API server API server通知Controller Manager创建一个Deployment资源 Scheduler执行调度任务，将两个副本Pod分发到node01和node02上 node1和node02上的kubelet在各个节点上创建并运行Pod 补充： 应用的配置和当前的状态信息报错在etcd中，执行kubectl get pod时API server会从etc中读取这 些数据 flannel会为每个Pod分配一个IP，但此时没有创建Service资源，目前kube-pory还没有参与进来","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]},{"title":"创建资源的两种方式","slug":"创建资源的两种方式","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.303Z","comments":true,"path":"创建资源的两种方式.html","link":"","permalink":"https://pdxblog.top/%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F.html","excerpt":"","text":"创建资源的两种方式 用命令行的方式创建：//创建Pod控制器，deployments 1[root@master ~]# kubectl run web --image&#x3D;nginx --replicas&#x3D;5 //查看控制器情况 1[root@master ~]# kubectl get deployments. //查看资源详细信息 1[root@master ~]# kubectl describe deployments. web PS：查看某种资源对象，没有指名称空间，默认是在default名称空间，可以加上-n选项，查看指定名称空间 1[root@master ~]# kubectl get pod -n&#x3D;kube-system 注意：直接运行创建的Deployment资源对象，是经常使用的一个控制器类型，除了deployment，还有rc，rs等Pod控制器，Deployment是一个高级的Pod控制器 //创建Service资源类型 1[root@master ~]# kubectl expose deployment web --name&#x3D;web-svc --port&#x3D;80 --type&#x3D;NodePort PS：如果想要外网能够访问服务，可以暴露deployment资源，得到service资源，但svc资源的类型必须为NodePoet 映射端口范围：30000-32767 服务的扩容与缩容： 1[root@master ~]# kubectl scale deployment web --replicas&#x3D;8 //通过修改yuml文件进行扩容与缩容 1[root@master ~]# kubectl edit deployments. web 服务的升级与回滚: 1[root@master ~]# kubectl set image deployment web web&#x3D;nginx:1.15 //通过修改配置文件进行升级 1[root@master ~]# kubectl edit deployments. web //回滚 1[root@master ~]# kubectl rollout undo deployment web 配置清单（yml、yaml）： 常见yaml文件写法，以及字段的作用： 五个一级字段： apiVersion: api版本信息 kind: 资源对象的类别 metadata: 元数据。名称字段必须写 spec: 用户期望的状态 status: 资源现在处于什么样的状态 Deployment 12345678910111213141516[root@master ~]# vim web.yamlkind: DeploymentapiVersion: extensions&#x2F;v1beta1metadata: name: webspec: replicas: 2 template: metadata: labels: app: web_server spec: containers: - name: nginx image: nginx[root@master ~]# kubectl apply -f web.yaml service 123456789101112kind: ServiceapiVersion: v1metadata: name: web-svcspec: selector: &#x2F;&#x2F;标签选择器，和Deployment里的标签要一样 app: web_server ports: - protocol: TCP port: 80 targetPort: 80[root@master ~]# kubectl apply -f web-svc.yml 使用相同的标签和标签选择器，使两个资源对象相互关联 PS：（本质的意义：提供一个统一的接口） 创建的Service资源对象，默认的type为ClusterIP,意味着集群内任何节点都可以访问，它的作用是为后端真正提供服务的Pod提供一个统一的访问接口,如果想要外网访问服务，应该把type改为NodePort 12345678910111213kind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort &#x2F;&#x2F;指定类型，让外网来访问 selector: app: web_server ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30033 &#x2F;&#x2F;指定集群映射端口，范围是30000-32767","categories":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"}]}],"categories":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/categories/Python/"},{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/categories/k8s/"},{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/categories/Docker/"},{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/categories/KVM/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://pdxblog.top/tags/Python/"},{"name":"k8s","slug":"k8s","permalink":"https://pdxblog.top/tags/k8s/"},{"name":"Docker","slug":"Docker","permalink":"https://pdxblog.top/tags/Docker/"},{"name":"KVM","slug":"KVM","permalink":"https://pdxblog.top/tags/KVM/"}]}