{"meta":{"title":"派大星の博客","subtitle":"记录生活中的点点滴滴","description":"直到这一刻微笑着说话为止，我至少留下了一公升眼泪","author":"派大星","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2020-01-24T14:29:44.000Z","updated":"2020-01-24T14:30:26.053Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-01-25T12:57:30.000Z","updated":"2020-01-25T12:58:43.644Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"派大星","date":"2020-01-25T13:20:10.000Z","updated":"2020-01-25T13:22:20.493Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"扎实的专业知识是我最大的财富；认真踏实是我做事的原则；不断超越创新是我追求的目标；"}],"posts":[{"title":"k8s数据持久化","slug":"k8s数据持久化","date":"2020-02-10T08:53:18.320Z","updated":"2020-02-10T08:54:54.062Z","comments":true,"path":"2020/02/10/k8s数据持久化/","link":"","permalink":"http://yoursite.com/2020/02/10/k8s%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/","excerpt":"","text":"k8s数据持久化Docker容器是有生命周期的，因此数据卷可以实现数据持久化 数据卷主要解决的问题： 数据持久性：当我们写入数据时，文件都是暂时性的存在，当容器崩溃后，host就会将这个容器杀死，然后重新从镜像创建容器，数据就会丢失 数据共享：在同一个Pod中运行容器，会存在共享文件的需求 Volume：emptyDir（空目录）：使用情况比较少，一般只做临时使用，类似Docker数据 持久化的：docker manager volume，该数据卷初始分配时，是一个空目录，同一个Pod中的容器可以对该目录有执行读写操作，并且共享数据 ​ 使用场景：在同一个Pod里，不同的容器，共享数据卷 ​ 如果容器被删除，数据仍然存在，如果Pod被删除，数据也会被删除 使用实例： 123456789101112131415161718192021222324252627282930313233343536[root@master ~]# vim emptyDir.yamlapiVersion: v1kind: Podmetadata: name: producer-consumerspec: containers: - image: busybox name: producer volumeMounts: - mountPath: /producer_dir //容器内的路径 name: shared-volume //指定本地的目录名 args: - /bin/sh - -c - echo \"hello k8s\" &gt; /producer_dir/hello; sleep 30000 - image: busybox name: consumer volumeMounts: - mountPath: /consumer_dir name: shared-volume args: - /bin/sh - -c - cat /consumer_dir/hello; sleep 30000 volumes: - name: shared-volume //这里的名字必须与上面的Pod的mountPath的name相对应 emptyDir: &#123;&#125; //定义数据持久化类型，即表示空目录[root@master ~]# kubectl apply -f emptyDir.yaml[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGE producer-consumer 2/2 Running 0 14s[root@master ~]# kubectl logs producer-consumer consumer hello k8s 使用inspect查看挂载的目录在哪（查看Mount字段） 123456789101112131415161718192021222324252627282930313233[root@master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESproducer-consumer 2/2 Running 0 69s 10.244.1.2 node01 &lt;none&gt; &lt;none&gt;//可以看到容器运行在node01上，在node01上找到这个容器并查看并查看详细信息[root@node01 ~]# docker psCONTAINER ID IMAGEf117beb235cf busybox13c7a18109a1 busybox[root@node01 ~]# docker inspect 13c7a18109a1 \"Mounts\": [ &#123; \"Type\": \"bind\", \"Source\": \"/var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volume\", \"Destination\": \"/producer_dir\", //容器内的挂载目录 \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\"//再查看另一个容器[root@node01 ~]# docker inspect f117beb235cf \"Mounts\": [ &#123; \"Type\": \"bind\", \"Source\": \"/var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volume\", \"Destination\": \"/consumer_dir\", //容器内的挂载目录 \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\"//可以看到两个容器使用的同一个挂载目录[root@node01 ~]# cd /var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volume[root@node01 shared-volume]# lshello[root@node01 shared-volume]# cat hello hello k8s 将容器删除，验证目录是否存在 1234567[root@node01 ~]# docker rm -f 13c7a18109a1 13c7a18109a1[root@node01 ~]# docker psCONTAINER ID IMAGEa809717b1aa5 busyboxf117beb235cf busybox//它会重新生成一个新的容器，来达到我们用户所期望的状态，所以这个目录还是存在的 删除Pod 1234[root@master ~]# kubectl delete pod producer-consumer[root@master ~]# ls /var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volumels: 无法访问/var/lib/kubelet/pods/5225f542-0859-4a6a-8d99-1f23b9781807/volumes/kubernetes.io~empty-dir/shared-volume: 没有那个文件或目录//Pod删除后数据也会被删除 hostPath Volume（使用场景也比较少）：类似Docker数据持久化的：bind mount 将Pod所在节点的文件系统上某一个文件或目录挂载进容器内 ​ 如果Pod被删除，数据会保留，相比较emptyDir会好一点，不过，一旦host崩溃，hostPath也无法访问 docker或者k8s集群本身的存储会采用hostPath这种方式 k8s集群中会有很多pod，如果都是用hostPath Volume的话管理起来很不方便，所以就用到了PV Persistent Volume | PV（持久卷）提前做好的，数据持久化的数据存放目录 是集群中的一块存储空间，由集群管理员管理或者由Storage class（存储类）自动管理，PV和pod、deployment、Service一样，都是一个资源对象 PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 该API对象捕获存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统 Psesistent Volume Claim | PVC（持久卷使用声明|申请） PVC代表用户使用存储的请求，应用申请PV持久化空间的一个申请、声明。K8s集群可能会有多个PV，你需要不停的为不同的应用创建多个PV 它类似于pod。Pod消耗节点资源，PVC消耗存储资源。 pod可以请求特定级别的资源（CPU和内存）。 权限要求可以请求特定的大小和访问模式 官方文档有更详细的说明：https://www.kubernetes.org.cn/pvpvcstorageclass 基于NFS服务来做的PV 12345678910[root@master ~]# yum -y install nfs-utils (需要节点全部下载，会报挂载类型错误)[root@master ~]# yum -y install rpcbind[root@master ~]# mkdir &#x2F;nfsdata[root@master ~]# vim &#x2F;etc&#x2F;exports&#x2F;nfsdata *(rw,sync,no_root_squash)[root@master ~]# systemctl start rpcbind[root@master ~]# systemctl start nfs-server[root@master ~]# showmount -eExport list for master:&#x2F;nfsdata * 1.创建PV（实际的存储目录） 2.创建PVC 3.创建pod 创建PV资源对象： 12345678910111213141516171819[root@master ~]# vim nfs-pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: test-pvspec: capacity: //PV容量的大小 storage: 1Gi accessModes: //PV支持的访问模式 - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle //PV的存储空间的回收策略是什么 storageClassName: nfs nfs: path: /nfsdata/pv1 server: 192.168.1.70[root@master ~]# kubectl apply -f nfs-pv.yaml[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtest-pv 1Gi RWO Recycle Available nfs 9m30s accessModes: （PV支持的访问模式） ​ - ReadWriteOnce：能以读-写的方式mount到单个节点 ​ - ReadWriteMany：能以读-写的方式mount到多个节点 ​ - ReadOnlyOnce：能以只读的方式mount到单个节点 persistentVolumeReclaimPolicy：（PV的存储空间的回收策略是什么） ​ Recycle：自动清除数据 ​ Retain：需要管理员手动回收 ​ Delete：云存储专用。直接删除数据 PV和PVC相互的关联：通过的是storageClassName &amp;&amp; accessModes 创建PVC 12345678910111213141516[root@master ~]# vim nfs-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-pvcspec: accessModes: //访问模式 - ReadWriteOnce resources: requests: storage: 1Gi //申请的容量大小 storageClassName: nfs //向哪个PV申请[root@master ~]# kubectl apply -f nfs-pvc.yaml[root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-pvc Bound test-pv 1Gi RWO nfs 14s PV的应用：创建一个Pod资源： 123456789101112131415161718192021[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-podspec: containers: - name: pod1 image: busybox args: - /bin/sh - -c - sleep 30000 volumeMounts: - mountPath: \"/mydata\" name: mydata volumes: - name: mydata persistentVolumeClaim: claimName: test-pvc[root@master ~]# kubectl apply -f pod.yaml 之前创建PV的时候指定的挂载目录是/nfsdata/pv1，我们并没有创建pv1这个目录，所以这个pod是运行不成功的。 以下是排错方法： kubectl describe kubectl logs /var/log/messages 查看该节点的kubelet的日志 123//使用kubectl describe[root@master ~]# kubectl describe pod test-podmount.nfs: mounting 192.168.1.70:/nfsdata/pv1 failed, reason given by server: No such file or directory //提示没有文件或目录 创建目录，再查看pod状态： 123[root@master ~]# mkdir /nfsdata/pv1NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-pod 1/1 Running 0 12m 10.244.1.3 node01 &lt;none&gt; &lt;none&gt; 验证是否应用成功： 123456[root@master ~]# kubectl exec test-pod touch /mydata/hello[root@master ~]# ls /nfsdata/pv1/hello[root@master ~]# echo 123 &gt; /nfsdata/pv1/hello [root@master ~]# kubectl exec test-pod cat /mydata/hello123 删除Pod，验证回收策略（Recycle）： 12345678[root@master ~]# kubectl delete pod test-pod[root@master ~]# kubectl delete pvc test-pvc[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtest-pv 1Gi RWO Recycle Available nfs 42h[root@master ~]# ls &#x2F;nfsdata&#x2F;pv1&#x2F;[root@master ~]#&#x2F;&#x2F;验证成功，数据已经回收 通常情况下不会设置为自动删除，不然就和emptyDir就差不多了 删除pv，修改回收策略： 之前是先创建PV—&gt;PVC—&gt;Pod，现在调整一下，先创建PV—&gt;—Pod—&gt;PVC 12345678910111213141516171819202122[root@master ~]# vim nfs-pv.yaml persistentVolumeReclaimPolicy: Retain[root@master ~]# kubectl apply -f nfs-pv.yaml [root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtest-pv 1Gi RWO Retain Available nfs 7s[root@master ~]# kubectl apply -f pod.yaml [root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEtest-pod 0&#x2F;1 Pending 0 5s &#x2F;&#x2F;Pending正在被调度[root@master ~]# kubectl describe pod test-podEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 41s (x2 over 41s) default-scheduler persistentvolumeclaim &quot;test-pvc&quot; not found&#x2F;&#x2F;没有发现对应的pvc创建pvc[root@master ~]# kubectl apply -f nfs-pvc.yaml[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEtest-pod 1&#x2F;1 Running 0 114s 验证Retain（管理员手动删除）回收策略： 1234567891011[root@master ~]# kubectl exec test-pod touch /mydata/k8s[root@master ~]# ls /nfsdata/pv1/k8s[root@master ~]# kubectl delete pod test-pod [root@master ~]# kubectl delete pvc test-pvc[root@master ~]# ls /nfsdata/pv1/k8s//可以看到并没有回收[root@master ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEtest-pv 1Gi RWO Retain Available nfs 6s mysql对数据持久化的应用： //这里就不再创建PV，PVC了，用之前的就行 1234[root@master ~]# kubectl apply -f nfs-pvc.yaml [root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEtest-pvc Bound test-pv 1Gi RWO nfs 7s 创建Deploment资源对象，mysql容器 123456789101112131415161718192021222324252627282930[root@master ~]# vim mysql.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: test-mysqlspec: selector: matchLabels: //基于等值的标签 app: mysql template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: 123.com volumeMounts: - name: mysql-storage mountPath: /var/lib/mysql volumes: - name: mysql-storage persistentVolumeClaim: claimName: test-pvc[root@master ~]# kubectl get deployments.NAME READY UP-TO-DATE AVAILABLE AGEtest-mysql 1/1 1 1 61s 进入容器创建数据，验证是否应用PV： 123456789101112131415161718[root@master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-mysql-569f8df4db-fnnxc 1/1 Running 0 32m 10.244.1.5 node01 &lt;none&gt; &lt;none&gt;[root@master ~]# kubectl exec -it test-mysql-569f8df4db-fnnxc -- mysql -u root -p123.commysql&gt; create database yun33; //创建数据库mysql&gt; use yun33; //选择使用数据路Database changedmysql&gt; create table my_id( id int(4)); 创建表mysql&gt; insert my_id values(9527); //在表中插入数据mysql&gt; select * from my_id; //查看表中所有数据+------+| id |+------+| 9527 |+------+1 row in set (0.00 sec)[root@master ~]# ls /nfsdata/pv1/auto.cnf ibdata1 ib_logfile0 ib_logfile1 k8s mysql performance_schema yun33 关闭node01节点，模拟节点宕机： 1234567891011121314151617[root@master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSIONmaster Ready master 36d v1.15.0node01 NotReady &lt;none&gt; 36d v1.15.0node02 Ready &lt;none&gt; 36d v1.15.0[root@master ~]# kubectl get pod -o wide -wNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-mysql-569f8df4db-fnnxc 1/1 Running 0 36m 10.244.1.5 node01 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-fnnxc 1/1 Terminating 0 38m 10.244.1.5 node01 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-2m5rd 0/1 Pending 0 0s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-2m5rd 0/1 Pending 0 0s &lt;none&gt; node02 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-2m5rd 0/1 ContainerCreating 0 0s &lt;none&gt; node02 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-2m5rd 1/1 Running 0 2s 10.244.2.4 node02 &lt;none&gt; &lt;none&gt;[root@master ~]# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-mysql-569f8df4db-2m5rd 1/1 Running 0 20s 10.244.2.4 node02 &lt;none&gt; &lt;none&gt;test-mysql-569f8df4db-fnnxc 1/1 Terminating 0 38m 10.244.1.5 node01 &lt;none&gt; &lt;none&gt; 验证：在node02上新生成的pod，它内部是否有我们创建的数据 12345678910111213141516171819202122232425262728293031323334[root@master ~]# kubectl exec -it test-mysql-569f8df4db-2m5rd -- mysql -u root -p123.commysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || yun33 |+--------------------+4 rows in set (0.01 sec)mysql&gt; use yun33;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+-----------------+| Tables_in_yun33 |+-----------------+| my_id |+-----------------+1 row in set (0.01 sec)mysql&gt; select * from my_id;+------+| id |+------+| 9527 |+------+1 row in set (0.01 sec)[root@master ~]# ls /nfsdata/pv1/auto.cnf ibdata1 ib_logfile0 ib_logfile1 k8s mysql performance_schema yun33 Pod不断的重启： ​ 1.swap，没有关闭。导致集群运行不正常 ​ 2.内存不足，运行服务也会重启 小结负责把PVC绑定到PV的是一个持久化存储卷控制循环，这个控制器也是kube-manager-controller的一部分运行在master上。而真正把目录挂载到容器上的操作是在POD所在主机上发生的，所以通过kubelet来完成。而且创建PV以及PVC的绑定是在POD被调度到某一节点之后进行的，完成这些操作，POD就可以运行了。下面梳理一下挂载一个PV的过程： 用户提交一个包含PVC的POD 调度器把根据各种调度算法把该POD分配到某个节点，比如node01 Node01上的kubelet等待Volume Manager准备存储设备 PV控制器调用存储插件创建PV并与PVC进行绑定 Attach/Detach Controller或Volume Manager通过存储插件实现设备的attach。（这一步是针对块设备存储） Volume Manager等待存储设备变为可用后，挂载该设备到/var/lib/kubelet/pods//volumes/kubernetes.io~/目录上 Kubelet被告知卷已经准备好，开始启动POD，通过映射方式挂载到容器中","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"}]},{"title":"Docker swarm","slug":"Docker swarm","date":"2020-01-24T16:00:00.000Z","updated":"2020-02-02T09:31:18.463Z","comments":true,"path":"2020/01/25/Docker swarm/","link":"","permalink":"http://yoursite.com/2020/01/25/Docker%20swarm/","excerpt":"","text":"Docker swarm docker swarm集群：三剑客之一 docker01 192.168.1.70 node1 docker02 192.168.1.50 node2 docker03 192.168.1.40 node3 关闭防火墙、禁用linux、3台dockerhost区别主机名，时间同步 1234[root@localhost ~]# setenforce 0[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# hostnamectl set-hostname node1[root@localhost ~]# su - docker版本必须是：v1.12版本开始 Swarm： 作用docker engin（引擎）的多个主机组成的集群 node： 每一个docker engin都是一个node（节点），分为manager和worker manager node： 负责执行编排和集群管理的工作，保持并维护swarm处于期望的状态，swarm可以有多个manager node，他们会自动协调并选举出一个Leader执行编排任务，但相反不能没有manager node worker node： 接受并执行由manager node派发的任务，并且默认manager node也是一个worker node，不过可以将它设置为manager-noly node，让他只负责编排和管理工作 service： 用来定义worker上执行的命令 1）初始化集群 12345678[root@node1 ~]# docker swarm init --advertise-addr 192.168.1.70Swarm initialized: current node (g26pbaqiozkn99qw9ngtgncke) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-3fp7qnihbzzy8u0esfjmmdfncdud4yh628e7bey5tu8fo3cl5p-4x021jwoh1bryxpwqd4bsj8xd 192.168.1.70:2377To add a manager to this swarm, run &#39;docker swarm join-token manager&#39; and follow the instructions. –advertise-addr：指定与其他Node通信的地址 上边返回的结果告诉我们：初始化成功，并且，如果想要添加work节点运行下面的命令： 1docker swarm join --token SWMTKN-1-3fp7qnihbzzy8u0esfjmmdfncdud4yh628e7bey5tu8fo3cl5p-4x021jwoh1bryxpwqd4bsj8xd 192.168.1.70:2377 PS：这里注意，token只有24小时的有效期 如果想要添加manager节点：运行下边的命令： 1docker swarm join-token manager 当两个节点加入成功之后，我们可以执行docker node ls查看节点详情 12345[root@node1 ~]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONg26pbaqiozkn99qw9ngtgncke * node1 Ready Active Leader 18.09.0w11nz7pzgmhq6wn5b51g6b8ao node2 Ready Active 18.09.0zqi7od9q0v7zo2tkabnseuqdf node3 Ready Active 18.09.0 基本操作命令： docker swarm leave：申请离开一个集群，之后查看节点状态会变成down然后通过manager node将其删除 docker node rm xxx：删除某个节点 docker swarm join-token {manager|worker}：生成令牌，可以是manager身份或worker身份 docker node demote（降级）：将swarm节点的manager降级为work docker node promote（升级）：将swarm节点的work升级为manager 2）部署docker swarm集群网络 overlay：覆盖型网络 1[root@node1 ~]# docker network create -d overlay --attachable docker attacheable：这个参数必须要加，否则不能用于容器 在创建网络的时候，我们并没有部署一个存储 服务，比如consul，那是因为docker swarm自带存储 3）部署一个图形化webUI界面 12[root@node1 ~]# docker run -d -p 8080:8080 -e HOST&#x3D;192.168.1.70 -e PORT&#x3D;8080 \\&gt; -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock --name viswalizer dockersamples&#x2F;visualizer 然后通过浏览器验证：192.168.1.70:8080 如果访问网页访问不到，需要开启路由转发： 12[root@node1 ~]# echo net.ipv4.ip_forward &#x3D; 1 &gt;&gt; &#x2F;etc&#x2F;sysctl.conf [root@node1 ~]# sysctl -p ) 4）创建service（服务） 1[root@node1 ~]# docker service create --replicas 1 --network docker --name web1 -p 80:80 nginx ) PS： 如果node2或node3宕机，这些服务会自动转到节点中没有宕机的host上，并继续运行 –replicas：副本数量 大概可以理解为以个副本等于一个容器 查看service： docker service ls 查看service信息： docker service ps xxx 删除server： docker service rm xxx 设置manager node不参加工作 1[root@node1 ~]# docker node update node1 --availability drain 5)搭建私有仓库 过程：略，详情请查看看https://blog.csdn.net/weixin_45636702/article/details/104002017 6）自定义镜像 要求：基于httpd镜像，更改访问界面内容。镜像tag版本为v1，对应主机页面内容为111，2222，3333 v1，v2，v3目录下的操作一样 12345678910[root@node01 ~]# mkdir &#123;v1,v2,v3&#125;[root@node01 ~]# cd v1[root@node01 v1]# vim index.html[root@node01 v1]# cat index.html111111111111111111111111111.................[root@node01 v1]# vim Dockerfile[root@node01 v1]# cat Dockerfile FROM httpdADD index.html &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs&#x2F;index.html 7）发布一个服务，基于上述镜像 要求：副本数量为3个，服务的名称为：bdqn 1[root@node01 ~]# docker service create --replicas 3 --name bdqn -p 80:80 192.168.1.70:5000&#x2F;httpd:v1 默认的ingress网络，包括创建的自定义overlay网络，为后端真正为用户提供服务的container，提供了一个统一的入口 随机映射的端口范围：30000-32767 8）服务的扩容与缩容 1[root@node01 ~]# docker service scale bdqn&#x3D;6 扩容与缩容可以直接通过scale进行设置副本数量 9）服务的升级与回滚 1[root@node01 ~]# docker service update --image 192.168.1.70:5000&#x2F;httpd:v2 bdqn //平滑的更新 1[root@node01 ~]# docker service update --image 192.168.1.70:5000&#x2F;httpd:v3 --update-parallelism 2 --update-delay 1m bdqn PS： 默认情况下，swarm一次只更新一个副本，并且两个副本之间没有等待时间，我们可以通过 –update-parallelisnm：设置并更新的副本数量 –update-delay：指定滚动更新时间间隔 //回滚操作 1[root@node01 ~]# docker service rollback bdqn PS： docker swarm的回滚操作，默认只能回滚到上一次的操作状态，并不能连续回滚操作","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Docker架构+Docker镜像分层+Dockerfile","slug":"Docker架构+Docker镜像分层+Dockerfile","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.224Z","comments":true,"path":"2020/01/25/Docker架构+Docker镜像分层+Dockerfile/","link":"","permalink":"http://yoursite.com/2020/01/25/Docker%E6%9E%B6%E6%9E%84+Docker%E9%95%9C%E5%83%8F%E5%88%86%E5%B1%82+Dockerfile/","excerpt":"","text":"Docker架构：) Docker架构总结： Docker是属于C/S架构，用户是使用 Docker Client 与 Docker Daemon 建立通信，并发送请求。请求接收后，Docker server通过http协议与路由，找到相应的 Handler 来执行请求 Docker Engine 是 Docker 架构中的运行引擎，同时也 Docker 运行的核心模块。Docker Engine 执行 Docker 内部的一系列工作，每一项工作都是以一个 Job 的形式的存在 Job 的运行过程中，当需要容器镜像时，则从 Docker Registry 中下载镜像，并通过镜像管理驱动 Graphdriver 将下载镜像以 Graph 的形式存储 当需要为 Docker 创建网络环境时，通过网络管理驱动 Networkdriver 创建并配置 Docker容器网络环境 当需要限制 Docker 容器运行资源或执行用户指令等操作时，则通过 Execdriver 来完成 Libcontainer 是一项独立的容器管理包，Networkdriver 以及 Execdriver 都是通过 Libcontainer 来实现具体对容器进行的操作 Docker镜像分层：Docker的最小镜像： 1[root@localhost ~]# docker pull hello-world 123FROM scratchCORP hello&#x2F;CMD [&quot;&#x2F;hello&quot;] Dockerfile的组成： 1）FROM：scratch（抓、挠） 2）COPY：hello/ 3）CMD：[“/hello”] base镜像(基础镜像)： Centos:7镜像的dockerfile 1234567891011FROM scratch &#x2F;&#x2F;从零开始构建ADD centos-7-x86 64-docker.tar.xz &#x2F;LABEL org. label-schema. schema-version&#x3D;&quot;1.0&quot;\\org. label-schema.namem&quot;centos Base Image&quot;\\org. label-schema.vendore&quot;Centos&quot;\\org. label-schema.Ticenses&quot;GPLv2&quot; \\org. labe1-schema.build-date&quot;20190305CMD [&quot;&#x2F;bin&#x2F;bash&quot;] 1[root@localhost test]# docker build -t centos7-vim-net-tools:12-11 . ) Dockerfile镜像分层总结： 镜像是容器的基石，容器是镜像运行后的实例，当镜像运行为容器之后，对镜像的所有数据仅有只读权限，如果需要对镜像源文件进行修改或删除操作时，此时是在容器层（可写层）进行的，用到了COW（copy on write）写时复制机制 Docker镜像的缓存特性 创建一个新的Dockerfile文件 12345FROM centos:7RUN yum -y install vimRUN yum -y install net-toolsRUN yum -y install wgetCMD [&quot;&#x2F;bin&#x2F;bash&quot;] 1[root@localhost ~]# docker build -t new-centos . 1）如果在相同层，有用到相同的镜像，可以不必再去下载，可以直接使用缓存 创建一个新的Dockefile文件 12345FROM centos:7RUN yum -y install vimRUN yum -y install wgetRUN yum -y install net-toolsCMD [&quot;&#x2F;bin&#x2F;bash&quot;] 1[root@localhost test1]# docker build -t centos-new . 2）即使镜像层里的操作一样，也必须是在同一层才可以使用dockerfile的缓存特性 如果制作镜像过程中，不想使用缓存可以加–no-cache选项 3）如果前面的曾发生改变，即使后边的层操作和顺序一样，也不能使用缓存特性 Dockerfile常用指令： 1）FROM：构建镜像基于哪个镜像 例如：FROM:centos:7 2）MAINTAINER：镜像维护者姓名或邮箱 例如：MAINTAINER admin 3）RUN：构建镜像时运行的shell命令 例如： RUN [“yum”,”install”,”httpd”] RUN yum -y install httpd 4）CMD：运行容器时执行的shell命令 例如： CMD [“/bin/bash”] 5）EXPOSE：声明容器的服务端口 例如：EXPOSE 80 443 6）ENV：设置容器环境变量 例如： ENV MYSQL_ROOT_PASSWORD 123.com 7）ADD：拷贝文件或目录到镜像，如果时URL或压缩包会自动下载或解压 ADD &lt;源文件&gt;… &lt;目标目录&gt; ADD [“源文件”…”目标目录”] 8）COPY：拷贝文件或目录到镜像容器内，跟ADD相似，但不具备自动下载或解压功能 9）ENTRYPOINT：运行容器时执行的shell命令 例如： ENTRYPOINT [“/bin/bash”,”-c”,”command”] ENTRYPOINT /bin/bash -c ‘command’ 10）VOLUME：指定容器挂在点到宿主机自动生成的目录或其他容器 例如： VOLUME [“/va/lib/mysql”] 11）USER：为RUN、CMD、和ENTRYPOINT执行命令指定运行用户 12）WORKDIR：为RUN、CMD、ENTRYPOINT、COPY和ADD设置工作目录，意思为切换目录 例如： WORKDIR：/var/lib/mysql 13）HEALTHCHECK：健康检查 14）ARG：构建时指定的一些参数 例如： FROM centos:7 ARG user USER $user 注意： 1、RUN在building时运行，可以写多条 2、CMD和ENTRYPOINT在运行container时，只能写一条，如果写多条，最后一条生效 3、CMD在run时可以被COMMAND覆盖，ENTRYPOINT不会被不会被COMMAND覆盖，但可以指定–entrypoint覆盖 4、如果在Dockerfile里需要往镜像内导入文件，则此文件必须在dockerfile所在目录或子目录下 小实验： 写一个dockerfile，基于cenyos:7镜像，部署安装NGINX服务 1234567891011121314151617[root@localhost ~]# mkdir web[root@localhost ~]# mv nginx-1.14.0.tar.gz web&#x2F;[root@localhost ~]# cd web&#x2F;[root@localhost web]# vim DockerfileFROM centos:7RUN yum -y install gcc pcre pcre-devel openssl openssl-devel zlib zlib-develCOPY nginx-1.14.0.tar.gz &#x2F;RUN tar -zxf nginx-1.14.0.tar.gz -C &#x2F;usr&#x2F;srcRUN useradd -M -s &#x2F;sbin&#x2F;nologin nginxWORKDIR &#x2F;usr&#x2F;src&#x2F;nginx-1.14.0RUN .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --user&#x3D;nginx --group&#x3D;nginxRUN make &amp;&amp; make installRUN ln -s &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;* &#x2F;usr&#x2F;local&#x2F;sbinRUN nginx -tRUN nginxEXPOSE 80[root@localhost web]# docker build -t test-web . &#x2F;&#x2F;如果Dockerfile在其他路径需要加-f参数来指定Dockerfile文件路径 //如果想要保证容器运行之后，nginx服务就直接开启，不必手动开启，我们可以在命令最后加上：nginx -g “daemon off;”选项 1[root@localhost web]# docker run -itd --name testweb_2 test-web:latest nginx -g &quot;daemon off;&quot; //查看容器的IP： 1[root@localhost web]# docker inspect testweb_2","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Docker的基本操作命令","slug":"Docker的基本操作命令","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.240Z","comments":true,"path":"2020/01/25/Docker的基本操作命令/","link":"","permalink":"http://yoursite.com/2020/01/25/Docker%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/","excerpt":"","text":"Docker的基本操作命令：//查找镜像： 1[root@localhost ~]# docker search mysql &#x2F;&#x2F;默认在docker hub公共仓库进行查找 //拉取镜像，下载镜像: 1[root@localhost ~]# docker pull busybox //导出镜像到本地： 1[root@localhost ~]# docker save -o busybox.tar busybox:latest （docker save &gt; busybox.tar busybox:latest） //查看本地镜像： 1[root@localhost ~]# docker images （docker image ls） PS：虽然我们查看到镜像标签位latest（最新的），但并不表示它一定是最新的，而且镜像如果没有写标签，默认是以latest为标签 //删除镜像： 1[root@localhost ~]# docker rmi busybox:latest //根据本地镜像包导入镜像： 1[root@localhost ~]# docker load -i busybox.tar （docker load &lt; busybox.tar ） //查看容器–正在运行的： 1[root@localhost ~]# docker ps //查看所有的容器： 1[root@localhost ~]# docker ps -a //删除容器： 1[root@localhost ~]# docker rm centos [CONTAINER ID&#x2F;容器名称] //停止容器运行： 1[root@localhost ~]# docker stop centos //启动容器: 1[root@localhost ~]# docker start centos PS:开启容器后记得去验证一下容器是否开启 //强制删除容器： 1[root@localhost ~]# docker rm centos -f //强制删除所有容器（生产环境严禁使用）： 1[root@localhost ~]# docker ps -a -q | xargs docker rm -f -——————————————————————————————— 123[root@localhost ~]# docker ps -a -q | xargs docker start -f &#x2F;&#x2F;开启所有容器[root@localhost ~]# docker ps -a -q | xargs docker stop -f &#x2F;&#x2F;关闭所有容器 //重启一个容器： 1[root@localhost ~]# docker restart test2 //运行一个容器： 1[root@localhost ~]# docker run -it --name test1 centos:7 -i：交互 -t：伪终端 -d（daemon）：后台运行 –name：给容器命名 –restart=always：始终保持运行（随着docker开启而运行） 1[root@localhost ~]# docker create -it --name test3 centos:7 &#x2F;&#x2F;不常用 //进入一个容器： 123[root@localhost ~]# docker exec -it test2 &#x2F;bin&#x2F;bash[root@localhost ~]# docker attach test2 区别： exec进入的方式需要添加-i，-t选项，后面还需要给容器一个shell环境，但attach就不需要这么麻烦 exec进入的方式：如果exit退出，容器仍然保持运行 attach：如果执行exit退出，容器会被关闭，如果想要保持容器不被关闭，可以使用键盘：ctrl+p ctrl+q可以实现 本质上区别： exec进入的方法，会产生新的进程 attach进入的方法，不会产生新的进程 Docker的基本操作逻辑： ) 小实验： 基于centos:7镜像运行一个容器，并且在这个容器内部署nginx服务 1）下载镜像： 1[root@localhost ~]# docker pull centos:7 2）运行容器： 1[root@localhost ~]# docker run -itd --name webapp --restart&#x3D;always centos:7 3）进入容器，开始部署nginx服务：//将nginx包导入到容器内： 1[root@localhost ~]# docker cp nginx-1.14.0.tar.gz 12345678910111213[root@localhost ~]# docker exec -it webapp &#x2F;bin&#x2F;bash[root@01b870908942 ~]# tar zxf nginx-1.14.0.tar.gz [root@01b870908942 ~]# cd nginx-1.14.0[root@01b870908942 nginx-1.14.0]# yum -y install gcc pcre pcre-devel openssl openssl-devevl zlib zlib-devel[root@01b870908942 nginx-1.14.0]# useradd -s &#x2F;sbin&#x2F;nologin nginx[root@01b870908942 nginx-1.14.0]# .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --user&#x3D;nginx --group&#x3D;nginx[root@01b870908942 nginx-1.14.0]# make &amp;&amp; make install[root@01b870908942 nginx-1.14.0]# ln -s &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;* &#x2F;usr&#x2F;local&#x2F;sbin&#x2F;[root@01b870908942 nginx-1.14.0]# nginx[root@01b870908942 nginx-1.14.0]# cd &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;html&#x2F;[root@01b870908942 html]# echo This is a resrweb in container &gt; index.html[root@01b870908942 html]# curl 127.0.0.1This is a resrweb in container //把容器制作成镜像：（可移植性） 1[root@localhost ~]# docker commit webapp myweb:12-10","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Docker的底层原理","slug":"Docker的底层原理","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.240Z","comments":true,"path":"2020/01/25/Docker的底层原理/","link":"","permalink":"http://yoursite.com/2020/01/25/Docker%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/","excerpt":"","text":"Docker底层原理如果虚拟机内服务对内核版本有要求，这个服务就不太适合用docker来实现了 Busybox：欺骗层 解耦：解除耦合、解除冲突 耦合：冲突现象 run—–&gt;Centos系统（nginx、web） 对于docker host来说这个系统仅仅是一个进程 Namespace（名称空间）： 用来隔离容器 1234[root@localhost ns]# pwd&#x2F;proc&#x2F;2971&#x2F;ns[root@localhost ns]# lsipc mnt net pid user uts ipc：共享内存、消息列队 mnt：挂载点、文件系统 net：网络栈 pid：进程编号 user：用户、组 uts：主机名、域名 //namespace六项隔离，实现了容器与宿主机、容器与容器之间的隔离 Cgroup（控制组）： 资源的限制 12[root@d9d679199f74 cgroup]# pwd&#x2F;sys&#x2F;fs&#x2F;cgroup 1[root@localhost cpu]# cat tasks PS：task这个文件内的数字，记录的是进程编号。PID 四大功能： 1）资源限制：cgroup可以对进程组使用的资源总额进行限制 2）优先级分配：通过分配的cpu时间片数量以及硬盘IO带宽大小，实际上相当于控制了进程运行的优先级别 3）资源统计：cgroup可以统计西系统资源使用量，比如cpu使用时间，内存使用量等，用于按量计费。同时还支持挂起功能，也就是说用过cgroup把所有的资源限制起来，对资源都不能使用，注意并不算是说我们的程序不能使用了，只是不能使用资源，处于挂起等待状态 4）进程控制：可以对进程组执行挂起、恢复等操作 内存限额： 容器内存包括两个部分：物理内存和swap 可以通过参数控制容器内存的使用量： -m或者–memory：设置内存的使用限额 –memory-swap：设置内存+swap的使用限额 举个例子： 如果运行一个容器，并且限制该容器最多使用200M内存和100M的swap 123[root@localhost ~]# docker run -it -m 200M --memory-swap 300M centos:7[root@5bc0e71faba3 memory]# pwd&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory //内存使用限制 12[root@5bc0e71faba3 memory]# cat memory.limit_in_bytes 209715200（字节） //内存+swap限制 12[root@5bc0e71faba3 memory]# cat memory.memsw.limit_in_bytes314572800（字节） 对比一个没有限制的容器，我们会发现，如果运行容器之后不限制内存的话，意味着没有限制 CPU使用： 通过-c或者–cpu-shares设置容器使用cpu的权重，如果 不设置默认为1024 举个例子： //没有限制：1024 1234[root@localhost ~]# docker run -it --name containerA centos:7[root@e2d88b8f8b87 &#x2F;]# cd &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu[root@e2d88b8f8b87 cpu]# cat cpu.shares 1024 //限制CPU使用权重为512： 1234[root@localhost ~]# docker run -it --name containerB -c 512 centos:7[root@f8165e07c8d7 &#x2F;]# cd &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu[root@f8165e07c8d7 cpu]# cat cpu.shares 512 容器的Block IO（磁盘的读写）： docker中可以通过设置权重，限制bps和iops的方式控制容器读写磁盘的IO bps：每秒的读写的数据量（byte per second） iops：每秒IO的次数 （io per second） 默认情况下，所有容器都能够平等的读写磁盘，也可以通过–blkio-weight参数改变容器的blockIO的优先级 –device-read-bps：显示读取某个设备的bps –device-write-bps：显示写入某个设备的bps –device-read-iops：显示读取某个设备的iops –device-write-iops：显示写入某个设备的iops //限制testA这个容器，写入/dev/sda这块磁盘的bps为30MB 1[root@localhost ~]# docker run -it --name testA --device-write-bps &#x2F;dev&#x2F;sda:30MB centos:7 //从/dev/zero输入，然后输出到test.out文件中，每次大小为1M，总共为800次，oflg=direct用来指定directIO方式写文件，这样才会使–device-write-bps生效 1[root@0e659ca3e85d &#x2F;]# time dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;test.out bs&#x3D;1M count&#x3D;800 oflag&#x3D;direct","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Docker的监控","slug":"Docker的监控","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.256Z","comments":true,"path":"2020/01/25/Docker的监控/","link":"","permalink":"http://yoursite.com/2020/01/25/Docker%E7%9A%84%E7%9B%91%E6%8E%A7/","excerpt":"","text":"Docker的监控docker自带的监控命令 docker top / stats / logs sysdig 12345678910[root@localhost ~]# docker load &lt; sysdig.tar[root@localhost ~]# docker load &lt; scope.1.12.tar[root@localhost ~]# docker run -it --rm --name sysdig \\&gt; --privileged&#x3D;true \\&gt; --volume&#x3D;&#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;host&#x2F;var&#x2F;run&#x2F;docker.sock \\&gt; --volume&#x3D;&#x2F;dev:&#x2F;host&#x2F;dev \\&gt; --volume&#x3D;&#x2F;proc:&#x2F;host&#x2F;proc:ro \\&gt; --volume&#x3D;&#x2F;boot:&#x2F;host&#x2F;boot:ro \\&gt; --volume&#x3D;&#x2F;lib&#x2F;modules:&#x2F;host&#x2F;lib&#x2F;modules:ro \\&gt; --volume&#x3D;&#x2F;usr:&#x2F;host&#x2F;usr:ro sysdig&#x2F;sysdig //下载失败后可以运行下边的命令，重新下载 1root@2fefbfde3db5:&#x2F;# sysdig-probe-loader //下载成功之后，可以运行sysdig命令 1root@2fefbfde3db5:&#x2F;# csysdig scope123[root@localhost ~]# curl -L git.io&#x2F;scope -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scope[root@localhost ~]# chmod a+x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scope[root@localhost ~]# scope launch //访问本机的4040端口 ) //监控两台dockerhost docker01 192.168.1.70 docker02 192.168.1.50 //docker02上也需要同样的操作 123[root@docker02 ~]# curl -L git.io&#x2F;scope -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scope[root@docker02 ~]# chmod a+x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;scope[root@docker02 ~]# docker load &lt; scope.1.12.tar 12[root@docker01 ~]# scope launch 192.168.1.70 192.168.1.50[root@docker02 ~]# scope launch 192.168.1.50 192.168.1.70 )","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Docker的私有仓库","slug":"Docker的私有仓库","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.256Z","comments":true,"path":"2020/01/25/Docker的私有仓库/","link":"","permalink":"http://yoursite.com/2020/01/25/Docker%E7%9A%84%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/","excerpt":"","text":"Registry用docker容器运行registry私有仓库 下载registry镜像： 1[root@localhost ~]# docker pull registry:2 &#x2F;&#x2F;2版本是使用go语言编写的，而registry是使用python写的 //运行私有仓库： 1[root@localhost ~]# docker run -itd --name registry --restart&#x3D;always -p 5000:5000 -v &#x2F;registry:&#x2F;var&#x2F;lib&#x2F;registry registry:2 -p：端口映射（宿主机端口：容器暴露的端口） -v：挂载目录（宿主机的目录：容器内的目录） 镜像重命名： 1[root@localhost ~]# docker tag test-web:latest 192.168.1.70:5000&#x2F;test 上传镜像到私有仓库： 123456[root@localhost ~]# vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service修改：指定私有仓库地址ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd --insecure-registry 192.168.1.70:5000[root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker[root@localhost ~]# docker push 192.168.1.70:5000&#x2F;test:latest 这里注意，既然是私有仓库，肯定是要考虑多台DockerHost共用的情况，如果有其他的DockerHost想要使用私有仓库，仅需要修改docker的配置文件，指定私有仓库的IP和端口即可。当然别忘了，更改过配置文件之后，daemon-reload ,restart docker服务 企业级私有仓库镜像Harbor下载一个docker-compse工具 //从GitHub上下载方法： 12[root@docker01 ~]# curl -L https:&#x2F;&#x2F;github.com&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.25.0&#x2F;docker-compose-&#96;uname -s&#96;-&#96;uname -m&#96; -o &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose[root@docker01 ~]# chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose 12[root@docker01 ~]# tar zxf docker-compose.tar.gz -C &#x2F;usr&#x2F;local&#x2F;bin[root@docker01 ~]# chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose //下载依赖包 1[root@docker01 ~]# yum -y install yum-utils device-mapper-persistent-data lvm2 //导入harbo离线安装包，并解压到/usr/local/下 1[root@docker01 ~]# tar zxf harbor-offline-installer-v1.7.4.tgz -C &#x2F;usr&#x2F;local&#x2F; //安装harbor 1234[root@docker01 ~]# cd &#x2F;usr&#x2F;local&#x2F;harbor&#x2F;[root@docker01 harbor]# vim harbor.cfghostname &#x3D; 192.168.1.70[root@docker01 harbor]# .&#x2F;install.sh //浏览器访问：192.168.1.70 用户名：admin 密码：Harbor12345 ) //修改docker配置文件，连接Harbor私有仓库 1234[root@docker01 ~]# vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.serviceExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd --insecure-registry 192.168.1.70[root@docker01 ~]# systemctl daemon-reload [root@docker01 ~]# systemctl restart docker //创建私有仓库 ) //登录仓库上传镜像 123[root@docker01 harbor]# docker login -u admin -p Harbor12345 192.168.1.70[root@docker01 harbor]# docker tag centos:7 192.168.1.70&#x2F;bdqn&#x2F;centos:7[root@docker01 harbor]# docker push 192.168.1.70&#x2F;bdqn&#x2F;centos:7 //从私有仓库下载镜像 1[root@docker03 ~]# docker pull 192.168.1.70&#x2F;bdqn&#x2F;centos:7","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Docker部署LNMP环境","slug":"Docker部署LNMP环境","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.271Z","comments":true,"path":"2020/01/25/Docker部署LNMP环境/","link":"","permalink":"http://yoursite.com/2020/01/25/Docker%E9%83%A8%E7%BD%B2LNMP%E7%8E%AF%E5%A2%83/","excerpt":"","text":"Docker部署LNMP环境172.16.10.0/24 Nginx：172.16.10.10 Mysql：172.16.10.20 PHP：172.16.10.30 网站的访问主目录：/wwwroot Nginx的配置文件：/docker 123456789101112[root@localhost ~]# docker run -itd --name test nginx:latest[root@localhost ~]# mkdir &#x2F;wwwroot[root@localhost ~]# mkdir &#x2F;docker[root@localhost ~]# docker cp test:&#x2F;etc&#x2F;nginx &#x2F;docker&#x2F;[root@localhost ~]# ls &#x2F;docker&#x2F;nginx[root@localhost ~]# docker cp test:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html &#x2F;wwwroot&#x2F;[root@localhost ~]# ls &#x2F;wwwroot&#x2F;html[root@localhost ~]# vim &#x2F;wwwroot&#x2F;html&#x2F;index.html[root@localhost ~]# cat &#x2F;wwwroot&#x2F;html&#x2F;index.html hello LNMP! 1）创建一个自定义网络 1[root@localhost ~]# docker network create -d bridge --subnet 172.16.10.0&#x2F;24 --gateway 172.16.10.1 lnmp 2）运行nginx容器 1[root@localhost ~]# docker run -itd --name nginx -v &#x2F;docker&#x2F;nginx&#x2F;:&#x2F;etc&#x2F;nginx -v &#x2F;wwwroot&#x2F;html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html -p 80:80 --network lnmp --ip 172.16.10.10 nginx:latest 3）运行mysql容器 123456789101112131415[root@localhost ~]# docker run --name mysql -e MYSQL_ROOT_PASSWORD&#x3D;123.com -d -p 3306:3306 --network lnmp --ip 172.16.10.20 mysql:5.7[root@localhost ~]# yum -y install mysql[root@localhost ~]# mysql -u root -p123.com -h 127.0.0.1 -P 3306MySQL [(none)]&gt; create database name;MySQL [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || name || performance_schema || sys |+--------------------+5 rows in set (0.00 sec) 4）运行php容器 1[root@localhost ~]# docker run -itd --name phpfpm -p 9000:9000 -v &#x2F;wwwroot&#x2F;html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html --network lnmp --ip 172.16.10.30 php:7.2-fpm //添加php测试页面： 123456[root@localhost html]# pwd&#x2F;wwwroot&#x2F;html[root@localhost html]# cat test.php &lt;?phpphpinfo();?&gt; 5）修改nginx配置文件，nginx和php连接 1234567891011121314[root@localhost ~]# cd &#x2F;docker&#x2F;nginx&#x2F;conf.d&#x2F;[root@localhost conf.d]# vim default.conf location &#x2F; &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; index index.html index.htm index.php; &#x2F;&#x2F;添加php解析&#x2F;&#x2F;打开此模块，并更改相应信息 location ~ \\.php$ &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; fastcgi_pass 172.16.10.30:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; //重启nginx 1[root@localhost conf.d]# docker restart nginx //到此，去浏览器验证，nginx服务和php服务界面 ) ) 说明nginx和php的来连接，没有问题，接下来是php和mysql的连接，在这我们使用一个phpMyAdmin的数据库管理工具 1234[root@localhost html]# pwd&#x2F;wwwroot&#x2F;html[root@localhost html]# unzip phpMyAdmin-4.9.1-all-languages.zip[root@localhost html]# mv phpMyAdmin-4.9.1-all-languages phpmyadmin //更改nginx的配置文件 123456789101112131415161718[root@localhost ~]# cd &#x2F;docker&#x2F;nginx&#x2F;conf.d&#x2F;[root@localhost conf.d]# pwd&#x2F;docker&#x2F;nginx&#x2F;conf.d[root@localhost conf.d]# vim default.conf&#x2F;&#x2F;在27行添加 location &#x2F;phpmyadmin &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; index index.html index.htm index.php; &#125;&#x2F;&#x2F;在43行添加 location ~ &#x2F;phpmyadmin&#x2F;(?&lt;after_ali&gt;(.*)\\.(php|php5)?$) &#123; root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html; fastcgi_pass 172.16.10.30:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; //重启nginx 1[root@localhost conf.d]# docker restart nginx //验证php主界面 ) //需要我们对php镜像做出更改，添加php和mysql连接的模块 写一个Docker 123456789FROM php:7.2-fpmRUN apt-get update &amp;&amp; apt-get install -y \\ libfreetype6-dev \\ libjpeg62-turbo-dev \\ libpng-dev \\ &amp;&amp; docker-php-ext-install -j$(nproc) iconv \\ &amp;&amp; docker-php-ext-configure gd --with-freetype-dir&#x3D;&#x2F;usr&#x2F;include&#x2F; --with-jpeg-dir&#x3D;&#x2F;usr&#x2F;include&#x2F; \\ &amp;&amp; docker-php-ext-install -j$(nproc) gd \\ &amp;&amp; docker-php-ext-install mysqli pdo pdo_mysql 1[root@localhost ~]# docker build -t phpmysql . //删除之前的php容器，并用我们新制作的php镜像重新运行 123[root@localhost ~]# docker stop phpfpm [root@localhost ~]# docker rm phpfpm [root@localhost ~]# docker run -itd --name phpfpm -p 9000:9000 -v &#x2F;wwwroot&#x2F;html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html --network lnmp --ip 172.16.10.30 phpmysql:latest //修改phpmyadmin的配置文件，指定连接的数据库的IP，然后重启php容器 1234567[root@localhost ~]# cd &#x2F;wwwroot&#x2F;html&#x2F;phpmyadmin&#x2F;[root@localhost phpmyadmin]# pwd&#x2F;wwwroot&#x2F;html&#x2F;phpmyadmin[root@localhost phpmyadmin]# cp config.sample.inc.php config.inc.php[root@localhost phpmyadmin]# vim config.inc.php$cfg[&#39;Servers&#39;][$i][&#39;host&#39;] &#x3D; &#39;172.16.10.20&#39;; &#x2F;&#x2F;修改，指定数据库的IP地址[root@localhost ~]# docker restart phpfpm 用户名：root 密码：123.com ) //登录成功之后会看到我们之前创建的数据库 )","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"ReplicaSet、DaemonSet","slug":"ReplicaSet、DaemonSet","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.303Z","comments":true,"path":"2020/01/25/ReplicaSet、DaemonSet/","link":"","permalink":"http://yoursite.com/2020/01/25/ReplicaSet%E3%80%81DaemonSet/","excerpt":"","text":"ReplicaSet RC：ReplicationoController（老一代的Pod控制器） RS：ReplicaSet（新一代的Pod控制器） 用于确保由其管理的控制的Pod对象副本，能够满足用户期望，多则删除，少则通过模板创建 deployment、rs、rc 特点： 确保Pod资源对象的数量精准 确保Pod健康运行 弹性伸缩 同样，它也可以通过yaml或json格式的资源清单来创建，其中spec字段一般嵌套一下字段 replicas：期望的Pod对象副本数量 selector：当前控制器匹配Pod对象副本的标签 template：Pod副本的模板 与RC相比而言，RS不仅支持基于等值的标签选择器，而且还支持集合的标签选择器 标签：解决同类型的资源对象越来越多，为了更好的管理，按照标签分组 常用标签分类： release（版本）：stable（稳定版）、canary（金丝雀）、beta（测试版） environment（环境变量）：dev（开发）、qa（测试）、production（生产） application（应用）：ui、as（application software应用软件）、pc、sc tier（架构层级）：frontend（前端）、backend（后端）、cache（缓存） partition（分区）：customerA（客户A）、customoerB（客户B） track（品控级别）：daily（每天）、weekly（每周） 标签要做到：见名知意 //通过–show-labels显示资源对象的标签 1[root@master ~]# kubectl get pod --show-labels //通过-l选项查看仅含有包含某个标签的资源 1[root@master ~]# kubectl get pod -l env //通过-L显示某个键对应的值 1[root@master ~]# kubectl get pod -L env //给Pod资源添加标签 1[root@master ~]# kubectl label pod label app&#x3D;pc //删除标签 1[root@master ~]# kubectl label pod label app- //修改标签 1[root@master ~]# kubectl label pod label env&#x3D;dev --overwrite 如果标签有多个，标签选择器选择其中一个，也可以关联成功，相反，如果选择器有多个，那标签必须完全满足条件，才可以关联成功 标签选择器：标签的查询过滤条件 基于等值关系的（equality-based）：”=”，”==”，”!=” =” 前面两个都是相等，最后是不等 基于集合关系（set-based）：in、notin、exists三种 例子： 123456selector: matchLables: app: nginx metchExpressions: - &#123;key: name,operator: In,values: [zhangsan,lisi]&#125; - &#123;key: age,operator: Exists,values:&#125; matchLabels：指定键值对来表示的标签选择器 matchExpressions：基于表达式来指定的标签选择器，选择器列表间为”逻辑与”关系；使用In或者Notin操作时，其values不强制要求为非控的字符串，而使用Exists或DosNotExist时，其values必须为空 ) 使用标签选择器的逻辑： 同时指定的多个选择器之间的逻辑关系为”与”操作 使用空值的标签选择器意味着每个资源对象都将被选择中 空的标签选择器无法选中任何资源 DaemonSet 它也是一种Pod控制器 使用场景：如果必须将Pod运行再固定的某个或某几个节点，且要优先其他Pod的启动，通常情况下，默认会每个节点都会运行，并且只能运行一个Pod，这种情况推荐使用DaemonSet资源对象 监控程序： 日志收集程序： 运行一个web服务，在每一个节点都运行一个Pod 1234567891011121314[root@master ~]# vim daemonset.yamlkind: DaemonSetapiVersion: extensions&#x2F;v1beta1metadata: name: test-dsspec: template: metadata: labels: name: test-ns spec: containers: - name: test-ns image: httpd:v1 RC、RS、Deployment、DaemonSet，Pod控制器。statfulSet（有状态）、Ingress。Pod RBAC：基于用户的认证授权机制","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"}]},{"title":"KVM磁盘格式","slug":"KVM磁盘格式","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.318Z","comments":true,"path":"2020/01/25/KVM磁盘格式/","link":"","permalink":"http://yoursite.com/2020/01/25/KVM%E7%A3%81%E7%9B%98%E6%A0%BC%E5%BC%8F/","excerpt":"","text":"磁盘格式：RAW：（裸格式） //占用空间较大，性能较好，但不支持虚拟机快照功能QCOW2：（copy on write） //占用空间较小，支持快照，性能比RAW稍差一些 创建磁盘：（默认是裸格式） 1[root@kvm disk]# qemu-img create 1234.raw 5G 查看磁盘信息： 1[root@kvm disk]# qemu-img info 1234.raw 创建指定格式磁盘： 1[root@kvm disk]# qemu-img create -f qcow2 bdqn.qcow2 5G 转换磁盘格式：qemu-img convert [-f fmt] [-O output_fmt] filename output_filename 1[root@kvm kvm-vm]# qemu-img convert -f raw -O qcow2 centos.raw centos.qcow2 //转换之后原来的磁盘还在 拍摄快照： 1[root@kvm ~]# virsh snapshot-create test01 查看快照信息： 1234[root@kvm ~]# virsh snapshot-list test01 名称 生成时间 状态------------------------------------------------------------ 1575254957 2019-12-02 10:49:17 +0800 running 时间戳：1970年1月1号（计算机C语言诞生了，Linux系统诞生了）32位系统：68年之后你的系统就不能使用了64位系统：使用时间没有限制 根据快照恢复系统： 1[root@kvm ~]# virsh snapshot-revert test01 1575254957 //拍摄的快照是占用磁盘空间的","categories":[{"name":"KVM","slug":"KVM","permalink":"http://yoursite.com/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://yoursite.com/tags/KVM/"}]},{"title":"KVM简介","slug":"KVM简介","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.318Z","comments":true,"path":"2020/01/25/KVM简介/","link":"","permalink":"http://yoursite.com/2020/01/25/KVM%E7%AE%80%E4%BB%8B/","excerpt":"","text":"KVM简介：什么是云计算：云计算:配置各种资源的方式 云计算的分类：基础即服务Lass平台即服务Pass软件即服务Sass如果按照不同的部署方式：公有云、私有云、混合云 KVM介绍：虚拟化的不同的方式 实现虚拟化的技术：基于二进制翻译的全虚拟化：（会报错）解决思路：捕捉报错—-翻译—模拟（会增加服务器的开销） 半虚拟化（Xen）：更改内核，只能用到Linux系统上全虚拟化：KVM、VMware//所依赖的硬件全部准备好就行 KVM的概念：基于内核的虚拟机（Kernel-Based VIrtul mathine） 打开KVM的方式：virt-manager应用程序–系统工具–虚拟系统管理器 命令创建虚拟主机域： 1234[root@localhost iso]# virt-install --os-type&#x3D;linux --os-variant centos7.0--name test01 --ram 1024 --vcpus 1 --disk&#x3D;&#x2F;kvm-vm&#x2F;centos.raw,format&#x3D;raw,size&#x3D;10--location &#x2F;iso&#x2F;CentOS-7-x86_64-DVD-1611.iso --network network&#x3D;default --graphics vnc,listen&#x3D;0.0.0.0 --noautoconsole（不会占用终端） vnc连接KVM虚拟机默认的端口为：5900","categories":[{"name":"KVM","slug":"KVM","permalink":"http://yoursite.com/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://yoursite.com/tags/KVM/"}]},{"title":"KVM网络","slug":"KVM网络","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.318Z","comments":true,"path":"2020/01/25/KVM网络/","link":"","permalink":"http://yoursite.com/2020/01/25/KVM%E7%BD%91%E7%BB%9C/","excerpt":"","text":"NAT模式：KVM默认的网络方式，如果想要应用这种模式，防火墙需要打开，因为需要用到iptables规则 //打开防火墙添加规则，打开5900端口 12345[root@localhost ~]# firewall-cmd --add-port&#x3D;5900&#x2F;tcp --permanent success[root@localhost ~]# firewall-cmd --reloadsuccess[root@localhost ~]# firewall-cmd --list-all //添加路由转发： 123[root@localhost ~]# echo net.ipv4.ip_forward &#x3D; 1 &gt;&gt; &#x2F;etc&#x2F;sysctl.conf [root@localhost ~]# sysctl -pnet.ipv4.ip_forward &#x3D; 1 总结：nat模式支持主机与虚拟机的互访，也支持虚拟机访问互联网，但不支持外网访问虚拟机域 桥接网络：1）创建虚拟桥接网卡br0 1234[root@localhost ~]# systemctl stop NetworkManager[root@localhost ~]# virsh iface-bridge ens33 br0 &#x2F;&#x2F;提示失败不用理会使用附加设备 br0 生成桥接 ens33 失败已启动桥接接口 br0 //查看配置文件会看到，ens33桥接到了br0 12345678[root@localhost network-scripts]# cat ifcfg-ens33DEVICE&#x3D;ens33ONBOOT&#x3D;yesBRIDGE&#x3D;&quot;br0&quot;[root@localhost network-scripts]# brctl showbridge name bridge id STP enabled interfacesbr0 8000.000c2901f11f yes ens33virbr0 8000.525400dc381b yes virbr0-nic 2）修改kvm虚拟机域的xml配置文件： 123&lt;interface type&#x3D;&#39;bridge&#39;&gt;&lt;mac address&#x3D;&#39;52:54:00:f8:a1:c9&#39;&#x2F;&gt;&lt;source bridge&#x3D;&#39;br0&#39;&#x2F;&gt; 3）开启虚拟机，配置IP，验证是否能够联通外网: 1[root@localhost ~]# vi &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth //IP和br0的IP要在同一网段","categories":[{"name":"KVM","slug":"KVM","permalink":"http://yoursite.com/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://yoursite.com/tags/KVM/"}]},{"title":"虚拟机的克隆","slug":"虚拟机的克隆","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.318Z","comments":true,"path":"2020/01/25/虚拟机的克隆/","link":"","permalink":"http://yoursite.com/2020/01/25/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%85%8B%E9%9A%86/","excerpt":"","text":"克隆：克隆的两种方式：1、手动克隆（完整克隆）：test01———-&gt;test02：（将test01克隆为test02）1）复制xml配置文件： 1234[root@localhost ~]# cd &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;[root@localhost qemu]# cp test01.xml test02.xml或者[root@kvm ~]# virsh dumpxml test01 &gt; test02.xml 2）复制磁盘文件： 12[root@localhost qemu]# cd &#x2F;kvm-vm&#x2F;[root@localhost kvm-vm]# cp centos.raw test02.raw 3）修改配置文件并重新生成一个虚拟机： 12[root@localhost kvm-vm]# cd &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;[root@localhost qemu]# vim test02.xml a:name字段b:删除UUIDc:删除mac addressd:修改磁盘路径以及名称 1[root@localhost qemu]# virsh define test02.xml 链接克隆：//做一个链接的磁盘，然后第二个新的虚拟机更改xml配置文件，磁盘信息指定新的链接磁盘 1[root@localhost kvm-vm]# qemu-img create -f qcow2 -b centos.raw test02.qcow2 自动克隆（完整克隆）: 1[root@localhost ~]# virt-clone --auto-clone -o test02 -n test03 //-o:表示克隆谁，-n：指定名称","categories":[{"name":"KVM","slug":"KVM","permalink":"http://yoursite.com/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://yoursite.com/tags/KVM/"}]},{"title":"虚拟机的迁移","slug":"虚拟机的迁移","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.334Z","comments":true,"path":"2020/01/25/虚拟机的迁移/","link":"","permalink":"http://yoursite.com/2020/01/25/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E8%BF%81%E7%A7%BB/","excerpt":"","text":"虚拟机的迁移： 冷迁移（静态迁移）： //服务器需要关闭kvm01：192.168.1.100kvm02：192.168.1.200 两台机器防火墙全部关闭，禁用selinux 12[root@localhost ~]# lsmod | grep kvm &#x2F;&#x2F;查看是否支持kvm[root@localhost ~]# systemctl status libvirtd &#x2F;&#x2F;查看libvirtd服务是否正常 //迁移和克隆差不多，都是需要对磁盘文件和xml配置文件进行操作 123[root@kvm01 ~]# scp &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;test01.xml root@192.168.1.200:&#x2F;etc&#x2F;libvirt&#x2F;qemu [root@kvm01 ~]# scp &#x2F;kvm-vm&#x2F;centos.raw root@192.168.1.200:&#x2F;kvm-vm&#x2F;[root@kvm02 ~]# virsh define &#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;test01.xml 热迁移（动态迁移）：删除所有的KVM虚拟机kvm01：192.168.1.100kvm02：192.168.1.200NFS：192.168.1.129 1）在NFS服务器上面操作： 123456789[root@NFS ~]# yum -y install nfs-utils[root@NFS ~]# mkdir &#x2F;kvmshare &#x2F;&#x2F;创建共享文件夹[root@NFS ~]# vim &#x2F;etc&#x2F;exports &#x2F;&#x2F;编辑共享文件夹权限[root@NFS ~]# cat &#x2F;etc&#x2F;exports&#x2F;kvmshare *(rw,sync,no_root_squash)[root@NFS ~]# systemctl start rpcbind &#x2F;&#x2F;远程传输控制协议[root@NFS ~]# systemctl enable rpcbind[root@NFS ~]# systemctl start nfs-server[root@NFS ~]# systemctl enable nfs-server //确保两台KVM服务器能看到 12[root@kvm01 ~]# showmount -e 192.168.1.129[root@kvm02 ~]# showmount -e 192.168.1.129 2）KVM01上基于NFS服务创建虚拟机添加新的存储池：名称：nfsshare类型：netfs目标路径：/opt/nfsshare（本机挂在的目录，目录默认没有，但会自己创建）主机名:192.168.1.129（nfs-server IP address）源路径：/kvmshare（nfs-server上的共享目录） 验证nfs服务是否正常： 123[root@kvm01 ~]# touch &#x2F;opt&#x2F;nfsshare&#x2F;test[root@NFS ~]# ls &#x2F;kvmshare&#x2F;test 创建存储卷：名称：centos7最大容量：10G //存储池和存储卷完成之后，直接创建虚拟机，并最小化安装选择之前的创建的iso镜像以及刚才创建的存储池和存储卷 配置虚拟机使用bridge桥接网络，使其能够ping通外网，并且在这里我们执行一个ping百度的命令，并让他保持一直是ping着的状态，用来模拟迁移到kvm02上服务不中断： 12345678[root@kvm01 ~]# virsh destroy centos7.0[root@kvm01 ~]# systemctl stop NetworkManager[root@kvm01 ~]# virsh iface-bridge ens33 br0[root@kvm01 ~]# virsh edit centos7.0&lt;interface type&#x3D;&#39;bridge&#39;&gt;&lt;mac address&#x3D;&#39;52:54:00:12:80:97&#39;&#x2F;&gt;&lt;source bridge&#x3D;&#39;br0&#39;&#x2F;&gt;[root@kvm01 ~]# virsh start centos7.0 配置IP为DHCP自动获取 在KVM02上操作，创建存储池：名称：nfsshare类型：netfs目标路径：/opt/nfsshare（本机挂在的目录，目录默认没有，但会自己创建）主机名:192.168.1.129（nfs-server IP address）源路径：/kvmshare（nfs-server上的共享目录）创建完之后会看到之前在KVM01上创建的test文件和centos.qcow2的存储卷 在KVM01上连接KVM02：右上角—文件—添加连接—连接到远程主机—方法：ssh—用户名：root—-主机名：192.168.1.200（KVM02的IP）会提示安装openssh-askpass，直接在KVM01和KVm02上安装： 12[root@kvm01 ~]# yum -y install openssh-askpass[root@kvm02 ~]# yum -y install openssh-askpass //因为KVM01使用的是bridge br0网卡，所以我们需要在KVM02上创建同样的网卡br0，用来支持虚拟机 12[root@kvm02 ~]# systemctl stop NetworkManager[root@kvm02 ~]# virsh iface-bridge ens33 br0 接下来直接在virt-manager管理器中迁移就可以了，迁移完成之后，保证我么的ping命令是不中断的，就表示实验完成了右键centos7.0—迁移—地址：192.168.1.200（KVM02的IP）—高级选项—-勾选允许不可靠—-迁移如果出现错误解决办法：把KVM01和KVM02上挂载的目录给一个777的权限，保证双方root用户有权限调用目录 12[root@kvm01 ~]# chmod 777 &#x2F;opt&#x2F;[root@kvm02 ~]# chmod 777 &#x2F;opt&#x2F; 迁移完成后在KVM02上面查看ping命令是否中断","categories":[{"name":"KVM","slug":"KVM","permalink":"http://yoursite.com/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://yoursite.com/tags/KVM/"}]},{"title":"KVM基本操作命令","slug":"KVM基本操作命令","date":"2020-01-24T16:00:00.000Z","updated":"2020-01-25T13:07:32.334Z","comments":true,"path":"2020/01/25/KVM基本操作命令/","link":"","permalink":"http://yoursite.com/2020/01/25/KVM%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/","excerpt":"","text":"基于操作命令1）查看虚拟机列表： 12[root@kvm ~]# virsh list &#x2F;&#x2F;查看正在运行的虚拟机[root@kvm ~]# virsh list --all &#x2F;&#x2F;查看所有虚拟机 //开机的虚拟机才有ID号，而且会随时变化 Id 名称 状态 test01 关闭 2）查看虚拟机的详细信息： 1234567891011121314[root@kvm ~]# virsh dominfo test01 &#x2F;&#x2F;dom全称domain，域的意思Id: -名称： test01UUID: 8ba94166-08dd-4805-962b-c99ed56869bcOS 类型： hvm状态： 关闭CPU： 1最大内存： 1048576 KiB使用的内存： 1048576 KiB持久（peisistent）： 是 &#x2F;&#x2F;数据的持久化自动启动（autostart）： 禁用 &#x2F;&#x2F;是否开机自启管理的保存： 否安全性模式： none安全性 DOI： 0 3）虚拟机域的开关机： 123[root@kvm ~]# virsh start test01 &#x2F;&#x2F;开机[root@kvm ~]# virsh shutdown test01 &#x2F;&#x2F;关机（shutdown：温柔的关机）[root@kvm ~]# virsh shutdown 2 &#x2F;&#x2F;2为ID号 //关机后再开机ID号也会变化 1[root@kvm ~]# virsh destroy test01 &#x2F;&#x2F;强制关机，类似于拔电源 4）导出配置： 1[root@kvm ~]# virsh dumpxml test01 &gt; test01.xml &#x2F;&#x2F;dump备份的意思 vmnet0：桥接 //好处：外网能够访问你的虚拟机vmnet1：主机vmnet8：NAT //缺点：外网访问不了你的虚拟机，好处：可以自己随意指定IP 一个完成的KVM域，生成之后会有两个文件： 1）磁盘文件：在部署之处已经指定 //用来记录它的信息 2）xml配置文件，默认在/etc/libvirt/qemu //qemu模拟硬件，类型为raw 5）删除虚拟机：//删除之前保证虚拟机是关闭状态 1[root@kvm ~]# virsh undefine test01 &#x2F;&#x2F;undefine取消定义 //xml配置文件也会被删除，但是磁盘文件不会被影响 6）根据配置文件恢复虚拟机： 1[root@kvm ~]# virsh define test01.xml &#x2F;&#x2F;define：定义 7）修改配置文件： 1[root@kvm qemu]# virsh edit test01 edit：自带语法检查功能（y：是、n：不、i：忽略、f：强制）vim：不会提示你语法错误 8）虚拟机重命名（7.2版本之前的不支持这条命令） 1[root@kvm ~]# virsh domrename test01 test1 &#x2F;&#x2F;重命名前关闭虚拟机 9）查看虚拟机对应的vnc端口 12[root@localhost ~]# virsh vncdisplay test01:0 :0等于5900:1=5901:2=5902 10)挂起虚拟机 12[root@localhost ~]# virsh suspend test01[root@localhost ~]# virsh resume test01 &#x2F;&#x2F;恢复挂起的虚拟机 11）开机自启 12[root@localhost ~]# virsh autostart test01[root@localhost autostart]# virsh autostart --disable test01 &#x2F;&#x2F;取消开机自启 12）console登录KVM域//在KVM域里添加 1234grubby --update-kernel&#x3D;ALL --args&#x3D;&quot;console&#x3D;ttyS0&quot;rebootvirsh console test01 &#x2F;&#x2F;使用xshell连接kvm退出 ctrl+]","categories":[{"name":"KVM","slug":"KVM","permalink":"http://yoursite.com/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://yoursite.com/tags/KVM/"}]},{"title":"Deployment","slug":"Deployment","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:00:44.044Z","comments":true,"path":"2020/01/24/Deployment/","link":"","permalink":"http://yoursite.com/2020/01/24/Deployment/","excerpt":"","text":"Deployment 12345678910111213141516apiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: replicas: 4 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v1 ports: - containerPort: 80 PS：注意，在Deployment资源对象中，可以添加Port字段，但此字段仅供用户查看，并不实际生效 service 12345678910111213kind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort selector: app: web ports: - protocol: TCP port: 80 &#x2F;&#x2F;clusterIP的端口 targetPort: 80 &#x2F;&#x2F;Pod的端口 nodePort: 30123 SNAT：Source NAT（源地址转换） DNAT：Destination NAT（目标地址转换） MASQ：动态的源地址转换 service实现的负载均衡：默认使用的是iptables规则 第二种方案：IPVS 回滚到指定版本 准备三个版本所使用的私有镜像，来模拟每次升级到不同的镜像 Deployment1.yaml 1234567891011121314151617[root@master ~]# vim deployment1.yamlapiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v1 ports: Deployment2.yaml 123456789101112131415161718[root@master ~]# vim deployment2.yamlapiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v2 ports: - containerPort: 80 Deployment3.yaml 123456789101112131415161718[root@master ~]# vim deployment3.yamlapiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: revisionHistoryLimit: 10 replicas: 3 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v3 ports: - containerPort: 80 此处3个yaml文件，指定不同版本的镜像 //运行一个服务，并记录一个版本信息 1[root@master ~]# kubectl apply -f deployment1.yaml --record //查看有哪些版本信息 1[root@master ~]# kubectl rollout history deployment test-web //运行并升级Deployment资源，并记录版本信息 12[root@master ~]# kubectl apply -f deployment2.yaml --record[root@master ~]# kubectl apply -f deployment3.yaml --record //此时可以运行一个关联的Service资源去验证升级是否成功 123[root@master ~]# kubectl apply -f web-svc.yaml[root@master ~]# curl 10.96.179.50&lt;h1&gt;zhb | test-web | httpd | v3&lt;h1&gt; //回滚到指定版本 用label控制Pod的位置 //添加节点你标签 12[root@master ~]# kubectl label nodes node02 disk&#x3D;ssd[root@master ~]# kubectl get nodes --show-labels | grep node02 12345678910111213141516171819apiVersion: extensions&#x2F;v1beta1kind: Deploymentmetadata: name: test-webspec: revisionHistoryLimit: 10 &#x2F;&#x2F;版本历史限制 replicas: 3 template: metadata: labels: app: web spec: containers: - name: test-web image: 192.168.1.70:5000&#x2F;httpd:v1 ports: - containerPort: 80 nodeSelector: &#x2F;&#x2F;添加节点选择器 disk: ssd &#x2F;&#x2F;和标签内容一致 12345[root@master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-web-d58c9f847-bhswj 1&#x2F;1 Running 0 28s 10.244.2.14 node02 &lt;none&gt; &lt;none&gt;test-web-d58c9f847-k58nj 1&#x2F;1 Running 0 28s 10.244.2.13 node02 &lt;none&gt; &lt;none&gt;test-web-d58c9f847-vt7r5 1&#x2F;1 Running 0 28s 10.244.2.15 node02 &lt;none&gt; &lt;none&gt; //删除节点标签 1[root@master ~]# kubectl label nodes node02 disk-","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"}]},{"title":"Docker三剑客之docker-compose+wordpress的博客搭建","slug":"Docker三剑客之docker-compose+wordpress的博客搭建","date":"2020-01-23T16:00:00.000Z","updated":"2020-02-03T02:45:12.557Z","comments":true,"path":"2020/01/24/Docker三剑客之docker-compose+wordpress的博客搭建/","link":"","permalink":"http://yoursite.com/2020/01/24/Docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-compose+wordpress%E7%9A%84%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","excerpt":"","text":"Docker三剑客：docker machine：自动化部署多台dockerHostdocker-compose：它可以同时控制多个容器docker swarm：从单个的服务向集群的形式发展为什么要做集群：高可用、高性能、高并发：防止单点故障 Docker三剑客之docker-composedocker容器的编排工具： 解决相互有依赖关系的多个容器的管理 //验证已有docker-compose命令 12[root@localhost ~]# docker-compose -vdocker-compose version 1.25.0, build 0a186604 docker-compose的配置文件实例 通过识别一个docker-compose.yml的配置文件，去管理容器 //设置tab键的空格数量 123[root@localhost ~]# vim .vimrcset tabstop&#x3D;2[root@localhost ~]# source .vimrc 12345678910111213[root@localhost ~]# mkdir compose_test[root@localhost ~]# cd compose_test&#x2F;[root@localhost compose_test]# vim docker-compose.ymlversion: &quot;3&quot;services: nginx: container_name: web-nginx image: nginx restart: always ports: - 90:80 volumes: - .&#x2F;webserver:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html 第一个部分：version：指定格式的版本 第二部分：services：定义服务，（想要运行什么样的容器） //运行docker-compose规定的容器： PS：在执行这条命令的当前目录下，也需要有一个docker-compose.yml的配置文件，并且通常只有一个 12345[root@localhost compose_test]# docker-compose up -d[root@localhost compose_test]# cd webserver&#x2F;[root@localhost webserver]# echo 123456 &gt; index.html[root@localhost webserver]# curl 127.0.0.1:90123456 //停止运行 1[root@localhost compose_test]# docker-compose stop //重启 1[root@localhost compose_test]# docker-compose restart //如果在当前目录没有docker-compose.yml这个文件，可以通过-f来指定docker-compose.yml文件位置 1[root@localhost ~]# docker-compose -f compose_test&#x2F;docker-compose.yml start 并且，在运行container的过程中，还可以支持Dockerfile 1234567891011121314151617181920212223[root@localhost compose_test]# vim Dockerfile[root@localhost compose_test]# cat Dockerfile FROM nginxADD webserver &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;htm[root@localhost compose_test]# vim docker-compose.ymlversion: &quot;3&quot;services: nginx: build: . container_name: web-nginx image: new-nginx:v1.0 restart: always ports: - 90:80[root@localhost compose_test]# docker-compose stop[root@localhost compose_test]# docker-compose rm[root@localhost compose_test]# docker-compose up -d[root@localhost compose_test]# curl 127.0.0.1:90123456[root@localhost compose_test]# cd webserver&#x2F;[root@localhost webserver]# echo 654321 &gt; index.html [root@localhost webserver]# curl 127.0.0.1:90123456 搭建wordpress的博客12345678910111213141516171819202122232425[root@localhost ~]# mkdir wordpress[root@localhost ~]# docker load &lt; wordpress.tar[root@localhost ~]# cd wordpress&#x2F;[root@localhost wordpress]# vim docker-compose.ymlversion: &quot;3.1&quot;services: wordpress: image: wordpress restart: always ports: - 8080:80 environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: 123.com WORDPRESS_DB_NAME: wordpress db: image: mysql:5.7 restart: always environment: MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: 123.com MYSQL_ROOT_PASSWORD: 123.com[root@localhost wordpress]# docker-compose up -d //浏览器访问本机的8080端口：（192.168.1.70:8080） ) ) ) )","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Docker实现服务发现","slug":"Docker实现服务发现","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.209Z","comments":true,"path":"2020/01/24/Docker实现服务发现/","link":"","permalink":"http://yoursite.com/2020/01/24/Docker%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/","excerpt":"","text":"Docker实现服务发现Docker + Consul + registrator实现服务发现 Consul：分布式、高可用的，服务发现和配置的工具，数据中心 Registrator：负责收集dockerhost上，容器服务的信息，并且发送给consul Consul-template：根据编辑好的模板生成新的nginx配置文件，并且负责加载nginx配置文件 实验环境 docker01 192.168.1.70 docker02 192.168.1.60 docker03 192.168.1.50 关闭防火墙和selinux，并修改主机名 1234[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# setenforce 0[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - 1）docker01上，启动consul服务 123[root@docker01 ~]# unzip consul_1.5.1_linux_amd64.zip[root@docker01 ~]# mv consul &#x2F;usr&#x2F;local&#x2F;bin&#x2F;[root@docker01 ~]# chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;consul //以二进制的方式部署consul，并启动，身份为leader 12345[root@docker01 ~]# consul agent -server -bootstrap \\&gt; -ui -data-dir&#x3D;&#x2F;var&#x2F;lib&#x2F;consul-data \\&gt; -bind&#x3D;192.168.1.70 \\&gt; -client&#x3D;0.0.0.0 \\&gt; -node&#x3D;master //这时这个命令会占用终端，可以使用nohup命令让它保持后台运行 1[root@docker01 ~]# nohup consul agent -server -bootstrap -ui -data-dir&#x3D;&#x2F;var&#x2F;lib&#x2F;consul-data -bind&#x3D;192.168.1.70 -client&#x3D;0.0.0.0 -node&#x3D;master &amp; PS: -bootstrap：加入这个选项时，一般都在server单节点的时候用，自选举为leader -ui：开启内部web界面 -data-dir：key/volume数据存储位置 -bind：指定开启服务的IP -client：指定访问的客户端 -node：只当集群内通信使用的名称，默认是用主机名命名的 PS：开启的端口 8300：集群节点 8301：集群内部的访问 8302：跨数据中心的通信 8500：web ui界面 8600：使用dns协议查看节点信息的端口 //查看conusl的信息 12[root@docker01 ~]# consul infoleader_addr &#x3D; 192.168.1.70:8300 &#x2F;&#x2F;这个对我们比较有用，其他的都是一些它的算法 //查看集群内成员的信息 123[root@docker01 ~]# consul membersNode Address Status Type Build Protocol DC Segmentmaster 192.168.1.70:8301 alive server 1.5.1 2 dc1 &lt;all&gt; 2）docker02、docker03，加入consul集群 这里我们采用容器的方式去运行consul服务 //docker02 12[root@docker02 ~]# docker load &lt; myprogrium-consul.tar[root@docker02 ~]# docker run -d --name consul -p 8301:8301 -p 8301:8301&#x2F;udp -p 8500:8500 -p 8600:8600 -p 8600:8600&#x2F;udp --restart always progrium&#x2F;consul:latest -join 192.168.1.70 -advertise 192.168.1.60 -client 0.0.0.0 -node&#x3D;node01 //docker03 12[root@docker03 ~]# docker load &lt; myprogrium-consul.tar[root@docker03 ~]# docker run -d --name consul -p 8301:8301 -p 8301:8301&#x2F;udp -p 8500:8500 -p 8600:8600 -p 8600:8600&#x2F;udp --restart always progrium&#x2F;consul:latest -join 192.168.1.70 -advertise 192.168.1.50 -client 0.0.0.0 -node&#x3D;node02 //在docker01上就能看到加入的信息 12342019&#x2F;12&#x2F;26 09:50:25 [INFO] serf: EventMemberJoin: node01 192.168.1.602019&#x2F;12&#x2F;26 09:50:25 [INFO] consul: member &#39;node01&#39; joined, marking health alive2019&#x2F;12&#x2F;26 09:53:06 [INFO] serf: EventMemberJoin: node02 192.168.1.502019&#x2F;12&#x2F;26 09:53:06 [INFO] consul: member &#39;node02&#39; joined, marking health alive 12345[root@docker01 ~]# consul membersNode Address Status Type Build Protocol DC Segmentmaster 192.168.1.70:8301 alive server 1.5.1 2 dc1 &lt;all&gt;node01 192.168.1.60:8301 alive client 0.5.2 2 dc1 &lt;default&gt;node02 192.168.1.50:8301 alive client 0.5.2 2 dc1 &lt;default&gt; //浏览器访问consul服务，验证集群信息 192.168.1.70:8500 ) 3)下载部署consul-template //在docker01上导入consul-template_0.19.5_linux_amd64.zip 123[root@docker01 ~]# unzip consul-template_0.19.5_linux_amd64.zip[root@docker01 ~]# mv consul-template &#x2F;usr&#x2F;local&#x2F;bin&#x2F;[root@docker01 ~]# chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;consul-template 4）docker02、docker03上部署registrator服务 registrator是一个能自动发现docker container提供的服务，并在后端服务注册中心注册服务或取消服务的工具，后端注册中心支持consul、etcd、skydns2、zookeeper等 //docker02 1234567[root@docker02 ~]# docker load &lt; myregistrator.tar[root@docker02 ~]# docker run -d \\&gt; --name registrator \\&gt; -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;tmp&#x2F;docker.sock \\&gt; --restart always \\&gt; gliderlabs&#x2F;registrator \\&gt; consul:&#x2F;&#x2F;192.168.1.60:8500 //运行一个nginx容器 123[root@docker02 ~]# docker run -d -P --name test nginx:latestb0665dcbd6c5 nginx:latest &quot;nginx -g &#39;daemon of…&quot; 10 seconds ago Up 9 seconds 0.0.0.0:32768-&gt;80&#x2F;tcp &#x2F;&#x2F;映射的端口为32768 //回到浏览器 ) ) //docker03 1234567[root@docker03 ~]# docker load &lt; myregistrator.tar[root@docker02 ~]# docker run -d \\&gt; --name registrator \\&gt; -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;tmp&#x2F;docker.sock \\&gt; --restart always \\&gt; gliderlabs&#x2F;registrator \\&gt; consul:&#x2F;&#x2F;192.168.1.50:8500 ) 5）docker01部署一个nginx服务 //依赖环境 [root@docker01 ~]# yum -y install zlib-devel openssl-devel pcre-devel 1234567891011121314[root@docker01 ~]# useradd -M -s &#x2F;sbin&#x2F;nologin nginx[root@docker01 ~]# tar zxf nginx-1.14.0.tar.gz [root@docker01 ~]# cd nginx-1.14.0&#x2F;[root@docker01 nginx-1.14.0]# .&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx \\&gt; --user&#x3D;nginx --group&#x3D;nginx \\&gt; --with-http_stub_status_module \\&gt; --with-http_realip_module \\&gt; --with-pcre --with-http_ssl_module[root@docker01 nginx-1.14.0]# make &amp;&amp; make install[root@docker01 nginx-1.14.0]# ln -s &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;* &#x2F;usr&#x2F;local&#x2F;sbin&#x2F;[root@docker01 nginx-1.14.0]# nginx -tnginx: the configuration file &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf syntax is oknginx: configuration file &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf test is successful[root@docker01 nginx-1.14.0]# nginx PS： 这里nginx作为反向代理，代理后端docker02、docker03上nignx的容器服务，所以我们先去docker02、docker03上部署一些服务，为了方便等会看到负载的效果，所以我们运行完成容器之后，做一个主界面内容的区分 docker02：web01 web02 docker03：web03 web04 //docker02 12345678910111213141516[root@docker02 ~]# docker run -itd --name web01 -P nginx:latest [root@docker02 ~]# docker exec -it web01 &#x2F;bin&#x2F;bashroot@dac0cc15f3fe:&#x2F;# cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;root@dac0cc15f3fe:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# echo &quot;This web caontainer in dockek02-web01&quot; &gt; index.html root@dac0cc15f3fe:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# cat index.html This web caontainer in dockek02-web01[root@docker02 ~]# docker run -itd --name web02 -P nginx:latest [root@docker02 ~]# docker exec -it web02 &#x2F;bin&#x2F;bashroot@26d622553e5e:&#x2F;# cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;root@26d622553e5e:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# echo &quot;This web caontainer in dockek02-web02&quot; &gt; index.html root@26d622553e5e:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# cat index.html This web caontainer in dockek02-web02[root@docker02 ~]# curl 127.0.0.1:32769This web caontainer in dockek02-web01[root@docker02 ~]# curl 127.0.0.1:32770This web caontainer in dockek02-web02 //docker03 12345678910111213141516[root@docker03 ~]# docker run -itd --name web03 -P nginx:latest [root@docker03 ~]# docker exec -it web03 &#x2F;bin&#x2F;bashroot@a10f25a91edf:&#x2F;# cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;root@a10f25a91edf:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# echo &quot;This web caontainer in dockek03-web03&quot; &gt; index.html root@a10f25a91edf:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# cat index.html This web caontainer in dockek03-web03[root@docker03 ~]# docker run -itd --name web04 -P nginx:latest [root@docker03 ~]# docker exec -it web04 &#x2F;bin&#x2F;bashroot@6d30a445c9b8:&#x2F;# cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;root@6d30a445c9b8:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# echo &quot;his web caontainer in dockek03-web04&quot; &gt; index.html root@6d30a445c9b8:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html# cat index.html This web caontainer in dockek03-web04[root@docker03 ~]# curl 127.0.0.1:32768This web caontainer in dockek03-web03[root@docker03 ~]# curl 127.0.0.1:32769This web caontainer in dockek03-web04 更改nginx的配置文件 12345678910111213141516171819202122232425[root@docker01 ~]# cd &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;[root@docker01 nginx]# mkdir consul[root@docker01 nginx]# cd consul&#x2F;[root@docker01 consul]# pwd&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul[root@docker01 consul]# vim nginx.ctmpl[root@docker01 consul]# cat nginx.ctmpl upstream httpd_backend &#123; &#123;&#123;range service &quot;nginx&quot;&#125;&#125; server &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;; &#123;&#123; end &#125;&#125;&#125;server &#123; listen 8000; server_name localhost; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;http_backend; &#125;&#125;[root@docker01 ~]# vim &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf&#x2F;&#x2F;在文件最后，也就是倒数第二行添加：include &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;*.conf;&#x2F;&#x2F;使nginx的主配置文件能够识别到新产生的配置文件[root@docker01 ~]# nginx -s reload //使用consul-template命令，根据模板生产的配置文件，并重新加载nginx的配置文件 1[root@docker01 consul]# consul-template -consul-addr 192.168.1.70:8500 -template &quot;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;nginx.ctmpl:&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;vhost.conf:&#x2F;usr&#x2F;local&#x2F;sbin&#x2F;nginx -s reload&quot; //这时这个命令会占用终端，可以使用nohup命令让它保持后台运行 1[root@docker01 consul]# nohup consul-template -consul-addr 192.168.1.70:8500 -template &quot;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;nginx.ctmpl:&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;vhost.conf:&#x2F;usr&#x2F;local&#x2F;sbin&#x2F;nginx -s reload&quot; &amp; //此时，应该能够看到，新生产的vhost.conf配置文件已经生效，访问本机的8000端口可以得到不同容器提供的服务 12345678910111213141516171819202122232425[root@docker01 ~]# cd &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;consul&#x2F;[root@docker01 consul]# lsnginx.ctmpl vhost.conf[root@docker01 consul]# cat vhost.confupstream httpd_backend &#123; server 192.168.1.60:32768; server 192.168.1.60:32769; server 192.168.1.60:32770; server 192.168.1.50:32768; server 192.168.1.50:32769; &#125;server &#123; listen 8000; server_name localhost; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;httpd_backend; &#125;&#125; 12345678[root@docker01 consul]# curl localhost:8000This web caontainer in dockek02-web01[root@docker01 consul]# curl localhost:8000This web caontainer in dockek02-web02[root@docker01 consul]# curl localhost:8000This web caontainer in dockek03-web03[root@docker01 consul]# curl localhost:8000This web caontainer in dockek03-web04 当然，这时不管是添加新的nginx的web容器，或是删除，生产的配置文件都会时时更新，这是我们在运行consul-template这条命令最后添加：/usr/local/sbin/nginx -s reload它的作用 //删除之前的test容器，查看vhost文件 123456789101112[root@docker02 ~]# docker rm -f test[root@docker01 consul]# cat vhost.conf upstream httpd_backend &#123; server 192.168.1.60:32769; server 192.168.1.60:32770; server 192.168.1.50:32768; server 192.168.1.50:32769;&#125; //在运行一个web05，查看vhost文件的变化 12345678910111213[root@docker02 ~]# docker run -itd --name web05 -P nginx:latest upstream httpd_backend &#123; server 192.168.1.60:32769; server 192.168.1.60:32770; server 192.168.1.60:32771; server 192.168.1.50:32768; server 192.168.1.50:32769;&#125; ) 1、docker01主机上以二进制包的方式部署consul服务并后台运行，其身份为leader 2、docker02、docker03以容器的方式运行consul服务，并加入到docker01的consul群集中 3、在主机docker02、docker03上后台运行registrator容器，使其自动发现docker容器提供的服务，并发送给consul 4、在docker01上部署Nginx，提供反向代理服务，docker02、docker03主机上基于Nginx镜像，各运行两个web容器，提供不同的网页文件，以便测试效果 5、在docker01上安装consul-template命令，将收集到的信息（registrator收集到容器的信息）写入template模板中，并且最终写入Nginx的配置文件中 6、至此，实现客户端通过访问Nginx反向代理服务器（docker01），获得docker02、docker03服务器上运行的Nginx容器提供的网页文件 Consul：分布式、高可用的，服务发现和配置的工具，数据中心 Registrator：负责收集dockerhost上，容器服务的信息，并且发送给consul registrator是一个能自动发现docker container提供的服务，并在后端服务注册中心注册服务或取消服务的工具，后端注册中心支持consul、etcd、skydns2、zookeeper等 Consul-template：根据编辑好的模板生成新的nginx配置文件，并且负责加载nginx配置文件","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Docker数据持久化","slug":"Docker数据持久化","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.224Z","comments":true,"path":"2020/01/24/Docker数据持久化/","link":"","permalink":"http://yoursite.com/2020/01/24/Docker%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/","excerpt":"","text":"Docker数据持久化为什么要做数据持久化： 因为Docker容器本身就是一个进程，可能会因为某些原因，或某些错误导致进程被杀死，这样数据就会丢失。 Docker容器是有生命周期的，生命周期结束，进程也会被杀死，数据就会丢失，因此需要做数据持久化，保证数据不会丢失 Storage Driver数据存储 Centos7版本的Docker，Storage Driver为：Overlay2； backing filesystem：xfs Data VolumeBind mount持久化存储：本质上是DockerHost文件系统中的目录或文件，能够直接被Mount到容器的文件系统中，在运行容器时，可以通过-v实现 特点： Data Volume是目录或文件，不能是没有格式化的磁盘（块设备） 容器可以读写volume中的数据 volume数据可以永久保存，即使用它的容器已经被销毁 小实验： 运行一个nginx服务，做数据持久化 12345678910[root@docker01 ~]# mkdir html[root@docker01 ~]# cd html&#x2F;[root@docker01 html]# echo &quot;This is a testfile in dockerHost.&quot; &gt; index.html[root@docker01 html]# cat index.html This is a testfile in dockerHost.[root@docker01 ~]# docker run -itd --name testweb -v &#x2F;root&#x2F;html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html nginx:latest[root@docker01 ~]# docker inspect testweb&quot;Gateway&quot;: &quot;172.17.0.1&quot;,[root@docker01 ~]# curl 172.17.0.2This is a testfile in dockerHost. PS:DockerHost上需要挂在的源文件或目录，必须是已经存在的，否则，当做一个目录挂在到容器中 默认挂载到容器内的文件，容器是有读写权限，可以在运行容器时-v后边加”:ro”限制容器的写入权限 并且还可以挂在单独文件到容器内部，一般它的使用场景是：如果不想对整个目录进行覆盖，而只希望添加某个文件，就可以使用挂载单个文件 Docker Manager Volume1[root@docker01 ~]# docker run -itd --name t2 -P -v &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html nginx:latest 删除容器的操作，默认不会对dockerHost上的文件操作，如果想要在删除容器时把源文件也删除，可以在删除容器时添加-v选型（一般不推荐使用这种方式，因为文件有可能被其他容器使用） 容器与容器的数据共享：volume container：给其他容器提供volume存储卷的容器，并且可以提供bind mount，也可以提供docker manager volume //创建一个vc_data容器： 123[root@docker01 ~]# docker create --name vc_data \\&gt; -v ~&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\&gt; -v &#x2F;other&#x2F;useful&#x2F;tools busybox 容器的跨主机数据共享 docker01 dcoker02 docker03 httpd httpd nfs 要求： docker01和docker02的主目录是一样的 //docker03的操作： 1234567891011121314151617[root@docker03 ~]#yum -y install nfs-utils[root@docker03 ~]# mkdir &#x2F;datashare[root@docker03 ~]# vim &#x2F;etc&#x2F;exports[root@docker03 ~]# cat &#x2F;etc&#x2F;exports&#x2F;datashare *(rw,sync,no_root_squash)[root@docker03 ~]# systemctl start rpcbind[root@docker03 ~]# systemctl enable rpcbind[root@docker03 ~]# systemctl start nfs-server[root@docker03 ~]# systemctl enable nfs-server[root@docker03 ~]# vim &#x2F;datashare&#x2F;index.html[root@docker03 ~]# cat &#x2F;datashare&#x2F;index.html &lt;div id&#x3D;&quot;datetime&quot;&gt; &lt;script&gt; setInterval(&quot;document.getElementById(&#39;datetime&#39;).innerHTML&#x3D;new Date().toLocaleString();&quot;, 1000); &lt;&#x2F;script&gt;&lt;&#x2F;div&gt;bdqn-webshare //验证 123456[root@docker01 ~]# showmount -e 192.168.1.60Export list for 192.168.1.60:&#x2F;datashare *[root@docker02 ~]# showmount -e 192.168.1.60Export list for 192.168.1.60:&#x2F;datashare * //docker01的操作 12345678910[root@docker01 ~]# mkdir &#x2F;htdocs[root@docker01 ~]# mount -t nfs 192.168.1.60:&#x2F;datashare &#x2F;htdocs&#x2F;&#x2F;&#x2F;-t：指定类型（type）[root@docker01 ~]# cat &#x2F;htdocs&#x2F;index.html &lt;div id&#x3D;&quot;datetime&quot;&gt; &lt;script&gt; setInterval(&quot;document.getElementById(&#39;datetime&#39;).innerHTML&#x3D;new Date().toLocaleString();&quot;, 1000); &lt;&#x2F;script&gt;&lt;&#x2F;div&gt;bdqn-webshare //docker02的操作 123456789[root@docker02 ~]# mkdir &#x2F;htdocs[root@docker02 ~]# mount -t nfs 192.168.1.60:&#x2F;datashare &#x2F;htdocs&#x2F;[root@docker02 ~]# cat &#x2F;htdocs&#x2F;index.html &lt;div id&#x3D;&quot;datetime&quot;&gt; &lt;script&gt; setInterval(&quot;document.getElementById(&#39;datetime&#39;).innerHTML&#x3D;new Date().toLocaleString();&quot;, 1000); &lt;&#x2F;script&gt;&lt;&#x2F;div&gt;bdqn-webshare 这里先不考虑将代码写入镜像，先以这种方式，分别在docker01和docker02部署httpd服务 //docker01 12[root@docker01 ~]# docker run -itd --name bdqn-web1 -P -v &#x2F;htdocs&#x2F;:&#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs httpd:latestPS：查看端口映射0.0.0.0:32778-&gt;80&#x2F;tcp bdqn-web1 //docker02 12[root@docker02 ~]# docker run -itd --name bdqn-web2 -P -v &#x2F;htdocs&#x2F;:&#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs httpd:latestPS：查看端口映射0.0.0.0:32768-&gt;80&#x2F;tcp bdqn-web2 此时用浏览器访问，两个web服务的主界面是一样的，但如果NFS服务器上源文件丢失，则两个web服务都会异常 想办法将源数据写入镜像内，在基于镜像做一个vc_data容器，这里因为没有接触到docker-compose和docker swarm等docker编排工具，所以我们在docker01和docker02上手动创建镜像 123456789101112[root@docker01 ~]# cd &#x2F;htdocs&#x2F;[root@docker01 htdocs]# vim Dockerfile[root@docker01 htdocs]# cat Dockerfile FROM busyboxADD index.html &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs&#x2F;index.htmlVOLUME &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs[root@docker01 htdocs]# docker build -t back_data .[root@docker01 htdocs]# docker create --name back_container1 back_data:latest [root@docker01 htdocs]# docker run -itd --name web3 -P --volumes-from back_container1 httpd:latest [root@docker01 htdocs]# pwd&#x2F;htdocs[root@docker01 htdocs]# docker save &gt; back_data.tar back_data:latest //docker02上操作： 123456[root@docker02 ~]# cd &#x2F;htdocs&#x2F;[root@docker02 htdocs]# lsback_data.tar Dockerfile index.html[root@docker02 htdocs]# docker load &lt; back_data.tar [root@docker02 htdocs]# docker create --name back_container2 back_data:latest[root@docker02 htdocs]# docker run -itd --name web4 -P --volumes-from back_container2 httpd:latest 测试： 1[root@docker01 htdocs]# rm -rf index.html 通过浏览器访问，一开始运行的web1和web2容器，无法访问了。web3和web4还是可以访问的。 但是数据无法同步了","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Docker网络","slug":"Docker网络","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.256Z","comments":true,"path":"2020/01/24/Docker网络/","link":"","permalink":"http://yoursite.com/2020/01/24/Docker%E7%BD%91%E7%BB%9C/","excerpt":"","text":"Docker网络：原生网络12345[root@localhost ~]# docker network lsNETWORK ID NAME DRIVER SCOPEfcc280741b01 bridge bridge local9c09e5a698dc host host local03411a6d716c none null local None：什么都没有的网络 12[root@localhost ~]# docker run -itd --name none --network none busybox:latest[root@localhost ~]# docker exec -it none /bin/sh PS：用到None网络的容器，会发现它只有一个LoopBack回环的网络，没有Mac地址、IP等信息，意味着它不能跟外界通信，是被隔离起来的网络 使用场景： 隔离意味着安全，所以，此网络可以运行一些关于安全方面的验证码、校验码等服务 Bridge：桥接网络 1234[root@localhost ~]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.02428e69e324 no virbr0 8000.525400547d41 yes virbr0-nic docker0：在我们安装docker这个服务的时候，默认就会生产一张docker0的网卡，一般默认IP为172.17.0.1/16 12[root@localhost ~]# docker run -itd --name test1 busybox:latest [root@localhost ~]# docker exec -it test1 /bin/sh 容器默认使用的网络是docker0网络，docker0此时相当于一个路由器，基于此网络的容器，网段都是和docker0一致的 自定义网络自带了一个ContainerDNSserver功能（域名解析） bridge //创建一个bridge网络： 1[root@localhost ~]# docker network create -d bridge my_net //创建两个容器，应用自定义网络（my_net）： 12[root@localhost ~]# docker run -itd --name test3 --network my_net busybox:latest[root@localhost ~]# docker run -itd --name test4 --network my_net busybox:latest PS：自定义网络优点，它可以通过容器的名称通信 12[root@localhost ~]# docker exec -it test3 &#x2F;bin&#x2F;sh&#x2F; # ping test4 //创建一个自定义网络，并且指定网关和网段 1[root@localhost ~]# docker network create -d bridge --subnet 172.20.16.0&#x2F;24 --gateway 172.20.16.1 my_net2 //创建两个容器，应用自定义网络(my_net2)，并指定IP 12[root@localhost ~]# docker run -itd --name test5 --network my_net2 --ip 172.20.16.6 busybox:latest[root@localhost ~]# docker run -itd --name test6 --network my_net2 --ip 172.20.16.8 busybox:latest PS：如果想要给容器指定IP地址，那么自定义网络的时候，必须指定网关gateway和subnet网段选项 //实现不同网段之间的通信，在容器里再添加一块网卡： 1[root@localhost ~]# docker network connect my_net2 test4 ) 让外网能够访问容器的端口映射方法：1）手动指定端口映射关系： 1[root@localhost ~]# docker run -itd --name web1 -p 90:80 nginx:latest 2）从宿主机随机映射端口到容器： 1[root@localhost ~]# docker run -itd --name web2 -p 80 nginx:latest 3)从宿主机随机映射端口到容器，容器内所有暴露的端口，都会一一映射 1[root@localhost ~]# docker run -itd --name web4 -P nginx:latest join容器：container（共享网络协议栈）容器和容器之间 12[root@localhost ~]# docker run -itd --name web5 busybox:latest[root@localhost ~]# docker run -itd --name web6 --network container:web5 busybox:latest 123[root@localhost ~]# docker exec -it web6 &#x2F;bin&#x2F;sh&#x2F; # echo 123456 &gt; &#x2F;tmp&#x2F;index.html&#x2F; # httpd -h &#x2F;tmp&#x2F; 123[root@localhost ~]# docker exec -it web5 &#x2F;bin&#x2F;sh&#x2F; # wget -O - -q 127.0.0.1123456 //这时会发现，两个容器的IP地址一样 PS：这种方法的使用场景： 由于这种网络的特殊特性，一般在运行同一个服务，并且合格服务需要做监控，已经日志收集、或者网络监控的时候，可以选择这种网络 docker的跨主机网络解决方案 overlay（覆盖）的解决方案： 实验环境： docker01：192.168.1.70 docker02：192.168.1.60 docker03：192.168.1.50 暂时不考录防火墙和selinux安全问题 将3台dockerhost防火墙和selinux全部关闭，并分别更改主机名称 12345[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su -[root@docker01 ~]# systemctl stop firewalld[root@docker01 ~]# setenforce 0[root@docker01 ~]# systemctl disable firewalld 在docker01上操作： //运行consul服务：（数据中心–分布式的） 123[root@docker01 ~]# docker load &lt; myprogrium-consul.tar[root@docker01 ~]# docker run -d -p 8500:8500 -h consul --name consul \\ &gt; --restart always progrium&#x2F;consul -server -bootstrap PS:容器产生之后我们可以通过浏览器访问consul服务，验证consul服务是否正常，访问dockerHost加映射端口 ) 修改docker02和docker03的docker配置文件： //将IP和端口的映射关系，写入consul 123[root@docker02 ~]# vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service修改：ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd -H unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock -H tcp:&#x2F;&#x2F;0.0.0.0:2376 --cluster-store&#x3D;consul:&#x2F;&#x2F;192.168.1.70:8500 --cluster-advertise&#x3D;ens33:2376 PS:返回浏览器consul服务界面，找到KEY/VALUE—-&gt;Docker—-&gt;NODES，会看到刚刚加入的docker02和docker03的信息 ) ) ) 在docker02上创建一个自定义网络： 1234[root@docker02 ~]# docker network create -d overlay ov_net1[root@docker02 ~]# docker network lsNETWORK ID NAME DRIVER SCOPEfe92a0eff6a4 ov_net1 overlay global 在docker02上创建网络，我们可以看到它的SCOPE定义的时global（全局），意味着加入到consul这个服务的docker服务，都可以看到我们自定义的网络 同理如果是用此网络创建的容器，会有两张网卡，默认这张网卡是10.0.0.0网段，如果想要docker01也可以看到这个网络，那么也只需在docker01的docker配置文件添加相应内容即可 同理，因为是自定义网络，符合自定义网络的特性，可以直接通过docker容器的名称互相通信，当然也可以在自定义网络的时候，指定它的网段，那么使用此网络的容器也可以指定IP地址 )","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Docker跨主机网络方案之MacVlan","slug":"Docker跨主机网络方案之MacVlan","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.271Z","comments":true,"path":"2020/01/24/Docker跨主机网络方案之MacVlan/","link":"","permalink":"http://yoursite.com/2020/01/24/Docker%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%96%B9%E6%A1%88%E4%B9%8BMacVlan/","excerpt":"","text":"Docker跨主机网络方案之MacVlan实验环境： docker01 192.168.1.70 docker02 192.168.1.50 关闭防火墙和禁用selinux，更该主机名： 1234567[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# systemctl disable firewalld[root@localhost ~]# setenforce 0[root@localhost ~]# systemctl daemon-reload [root@localhost ~]# systemctl restart docker[root@localhost ~]# hostnamectl set-hostname docker01[root@localhost ~]# su - macvlan的单网络通信1）打开网卡的混杂模式//需要在docker01和docker02上都进行操作 12[root@docker01 ~]# ip link set ens33 promisc on[root@docker01 ~]# ip link show ens33 2）在docker01上创建macvlan网络 1234在这里插入代码片[root@docker01 ~]# docker network create -d macvlan --subnet 172.22.16.0&#x2F;24 --gateway 172.22.16.1 -o parent&#x3D;ens33 mac_net1[root@docker01 ~]# docker network lsNETWORK ID NAME DRIVER SCOPEe6860af70e90 mac_net1 macvlan local PS:-o parent=绑定在哪张网卡之上3）基于创建的macvlan网络运行一个容器 1[root@docker01 ~]# docker run -itd --name bbox1 --ip 172.22.16.10 --network mac_net1 busybox 4）在docker02上创建macvlan网络，注意与docker01上的macvlan网络一摸一样 1[root@docker02 ~]# docker network create -d macvlan --subnet 172.22.16.0&#x2F;24 --gateway 172.22.16.1 -o parent&#x3D;ens33 mac_net1 5）在docker02上，基于创建的macvlan网络运行一个容器，验证与docker01上容器的通信 1[root@docker02 ~]# docker run -itd --name bbox2 --network mac_net1 --ip 172.22.16.20 busybox macvlan的多网络通信1）docker01和docker02验证内核模块8021q封装macvlan需要解决的问题：基于真实的ens33网卡，生产新的虚拟网卡 123[root@docker01 ~]# modinfo 8021q&#x2F;&#x2F;如果内核模块没有开启，运行下边命令导入一下[root@docker01 ~]# modprobe 8021q 2）基于ens33创建虚拟网卡//修改ens33网卡配置文件： 1234[root@docker01 ~]# cd &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;[root@docker01 network-scripts]# vim ifcfg-ens33&#x2F;&#x2F;修改：BOOTPROTO&#x3D;manual &#x2F;&#x2F;手动模式 //手动添加虚拟网卡配置文件 123456789101112[root@docker01 network-scripts]# cp -p ifcfg-ens33 ifcfg-ens33.10&#x2F;&#x2F;PS：-p 保留源文件或目录的属性[root@docker01 network-scripts]# vim ifcfg-ens33.10BOOTPROTO&#x3D;noneIPADDR&#x3D;192.168.10.10PREFIX&#x3D;24GATEWAY&#x3D;192.168.10.1NAME&#x3D;ens33.10DEVICE&#x3D;ens33.10ONBOOT&#x3D;yesVLAN&#x3D;yes&#x2F;&#x2F;PS：这里注意，IP要和ens33网段做一个区分，保证网关和网段IP的一致性，设备名称和配置文件的一致性，并且打开VLAN支持模式 //创建第二个虚拟网卡配置文件 12345678910[root@docker01 network-scripts]# cp -p ifcfg-ens33.10 ifcfg-ens33.20[root@docker01 network-scripts]# vim ifcfg-ens33.20BOOTPROTO&#x3D;noneIPADDR&#x3D;192.168.20.20PREFIX&#x3D;24GATEWAY&#x3D;192.168.20.1NAME&#x3D;ens33.20DEVICE&#x3D;ens33.20ONBOOT&#x3D;yesVLAN&#x3D;yes 3）docker01上的操作，启用创建的虚拟网卡 12[root@docker01 network-scripts]# ifup ifcfg-ens33.10[root@docker01 network-scripts]# ifup ifcfg-ens33.20 4)基于虚拟网卡，创建macvlan网络 12[root@docker01 ~]# docker network create -d macvlan --subnet 172.16.10.0&#x2F;24 --gateway 172.16.10.1 -o parent&#x3D;ens33.10 mac_net10[root@docker01 ~]# docker network create -d macvlan --subnet 172.16.20.0&#x2F;24 --gateway 172.16.20.1 -o parent&#x3D;ens33.20 mac_net20 5)基于创建的虚拟网卡，创建macvlan网络 12[root@docker02 ~]# docker network create -d macvlan --subnet 172.16.10.0&#x2F;24 --gateway 172.16.10.1 -o parent&#x3D;ens33.10 mac_net10[root@docker02 ~]# docker network create -d macvlan --subnet 172.16.20.0&#x2F;24 --gateway 172.16.20.1 -o parent&#x3D;ens33.20 mac_net20 6)docker02上也创建虚拟网卡，并启用 1234567891011121314151617181920212223[root@docker01 ~]# scp &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33.10 &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33.20 root@192.168.1.50:&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;[root@docker02 network-scripts]# vim ifcfg-ens33BOOTPROTO&#x3D;manual[root@docker02 network-scripts]# vim ifcfg-ens33.10BOOTPROTO&#x3D;noneIPADDR&#x3D;192.168.10.11PREFIX&#x3D;24GATEWAY&#x3D;192.168.10.1NAME&#x3D;ens33.10DEVICE&#x3D;ens33.10ONBOOT&#x3D;yesVLAN&#x3D;yes[root@docker02 network-scripts]# vim ifcfg-ens33.20BOOTPROTO&#x3D;noneIPADDR&#x3D;192.168.20.21PREFIX&#x3D;24GATEWAY&#x3D;192.168.20.1NAME&#x3D;ens33.20DEVICE&#x3D;ens33.20ONBOOT&#x3D;yesVLAN&#x3D;yes[root@docker02 network-scripts]# ifup ifcfg-ens33.10 [root@docker02 network-scripts]# ifup ifcfg-ens33.20 7)基于macvlan网络创建容器，并指定IP地址，不过这里要注意，运行的同期与网络对应的网段相符合，还需要注意IP地址的唯一性 123456&#x2F;&#x2F;docker01[root@docker01 ~]# docker run -itd --name bbox10 --network mac_net10 --ip 172.16.10.10 192.168.1.70:5000&#x2F;busybox:v1 [root@docker01 ~]# docker run -itd --name bbox20 --network mac_net20 --ip 172.16.20.20 192.168.1.70:5000&#x2F;busybox:v1&#x2F;&#x2F;docker02[root@docker02 ~]# docker run -itd --name bbox11 --network mac_net10 --ip 172.16.10.11 192.168.1.70:5000&#x2F;busybox:v1[root@docker02 ~]# docker run -itd --name bbox21 --network mac_net20 --ip 172.16.20.21 192.168.1.70:5000&#x2F;busybox:v1 8)将VMware虚拟机的网络改为桥接9)进入容器测试通信在docker01上进入容器bbox10和docker02上的bbox11进行通信在docker01上进入容器bbox20和docker02上的bbox21进行通信 12345678910[root@docker01 ~]# docker exec -it bbox10 &#x2F;bin&#x2F;sh&#x2F; # ping 172.16.10.11PING 172.16.10.11 (172.16.10.11): 56 data bytes64 bytes from 172.16.10.11: seq&#x3D;0 ttl&#x3D;64 time&#x3D;0.668 ms64 bytes from 172.16.10.11: seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.335 ms[root@docker01 ~]# docker exec -it bbox20 &#x2F;bin&#x2F;sh&#x2F; # ping 172.16.20.21PING 172.16.20.21 (172.16.20.21): 56 data bytes64 bytes from 172.16.20.21: seq&#x3D;0 ttl&#x3D;64 time&#x3D;0.584 ms64 bytes from 172.16.20.21: seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.365 ms","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Kubernetes集群部署","slug":"Kubernetes集群部署","date":"2020-01-23T16:00:00.000Z","updated":"2020-02-03T02:44:38.847Z","comments":true,"path":"2020/01/24/Kubernetes集群部署/","link":"","permalink":"http://yoursite.com/2020/01/24/Kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","excerpt":"","text":"生产级别的容器编排系统 Kubernetes 是用于自动部署，扩展和管理容器化应用程序的开源系统 k8s 最基本的硬件要求 CPU:双核 Mem：2G 3台dockerhost时间必须同步 Kubeadm工具自动部署k8s集群 //给3台docker命名，禁用swap交换分区 12345678910111213[root@localhost ~]# hostnamectl set-hostname master[root@localhost ~]# su -[root@localhost ~]# hostnamectl set-hostname node01[root@localhost ~]# su -[root@localhost ~]# hostnamectl set-hostname node02[root@localhost ~]# su -[root@master ~]# swapoff -a &#x2F;&#x2F;临时禁用[root@master ~]# free total used free shared buff&#x2F;cache availableMem: 1867292 335448 908540 9256 623304 1290100Swap: 0 0 0&#x2F;&#x2F;永久禁用[root@master ~]# vim &#x2F;etc&#x2F;fstab &#x2F;&#x2F;注释掉swap那一行 //禁用selinux，防火墙，并关闭开机自启（三台都需要） 12345[root@master ~]# vim &#x2F;etc&#x2F;selinux&#x2F;configSELINUX&#x3D;disabled[root@master ~]# setenforce 0[root@master ~]# systemctl stop firewalld[root@master ~]# systemctl disable firewalld //编写hosts文件，设置域名解析 12345678[root@master ~]# vim &#x2F;etc&#x2F;hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.1.70 master192.168.1.50 node01192.168.1.40 node02[root@master ~]# scp &#x2F;etc&#x2F;hosts root@192.168.1.50:&#x2F;etc[root@master ~]# scp &#x2F;etc&#x2F;hosts root@192.168.1.40:&#x2F;etc //设置免密登录 123[root@master ~]# ssh-keygen -t rsa[root@master ~]# ssh-copy-id node01[root@master ~]# ssh-copy-id node02 //打开iptables的桥接功能，开启路由转发 1234567891011121314151617181920212223[root@master ~]# vim &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.confnet.bridge.bridge-nf-call-iptables &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1[root@master ~]# echo net.ipv4.ip_forward &#x3D; 1 &gt;&gt; &#x2F;etc&#x2F;sysctl.conf [root@master ~]# sysctl -pnet.ipv4.ip_forward &#x3D; 1[root@master ~]# sysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf &#x2F;&#x2F;如果这条命令不成功则需要添加一个模块[root@master ~]# modprobe br_netfilternet.bridge.bridge-nf-call-iptables &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1[root@master ~]# scp &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf node01:&#x2F;etc&#x2F;sysctl.d [root@master ~]# scp &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf node02:&#x2F;etc&#x2F;sysctl.d [root@master ~]# scp &#x2F;etc&#x2F;sysctl.conf node02:&#x2F;etc&#x2F; [root@master ~]# scp &#x2F;etc&#x2F;sysctl.conf node01:&#x2F;etc&#x2F;[root@node01 ~]# sysctl -pnet.ipv4.ip_forward &#x3D; 1[root@node01 ~]# sysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf net.bridge.bridge-nf-call-iptables &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1[root@node02 ~]# sysctl -pnet.ipv4.ip_forward &#x3D; 1[root@node02 ~]# sysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf net.bridge.bridge-nf-call-iptables &#x3D; 1net.bridge.bridge-nf-call-ip6tables &#x3D; 1 //获取yum源 123456789101112[root@master ~]# cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo&gt; [kubernetes]&gt; name&#x3D;Kubernetes&gt; baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;&gt; enabled&#x3D;1&gt; gpgcheck&#x3D;1&gt; repo_gpgcheck&#x3D;1&gt; gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg&gt; EOF[root@master ~]# yum repolist[root@master ~]# yum makecache&#x2F;&#x2F;三台都需要这个yum源（node01，node02步骤省略） //安装以下三个组件kubectl：k8s客户端kubeadm：自动化快速部署k8s集群工具kubelet：客户端代理 1234[root@master ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0 kubectl-1.15.0-0&#x2F;&#x2F;node01、node02不需要安装kubectl[root@node01 ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0[root@node02 ~]# yum -y install kubeadm-1.15.0-0 kubelet-1.15.0-0 //加入开机自启（三台全部加入开机自启） 1[root@master ~]# systemctl enable kubelet //导入镜像 1234567891011121314[root@master ~]# mkdir images[root@master ~]# cd images&#x2F;[root@master images]# lscoredns-1-3-1.tar kube-apiserver-1-15.tar kube-proxy-1-15.tar myflannel-11-0.taretcd-3-3-10.tar kube-controller-1-15.tar kube-scheduler-1-15.tar pause-3-1.tar[root@master ~]# cat &gt; images.sh &lt;&lt;EOF&gt; #!&#x2F;bin&#x2F;bash&gt; for i in &#x2F;root&#x2F;images&#x2F;*&gt; do&gt; docker load &lt; $i&gt; done&gt; EOF[root@master ~]# chmod +x images.sh[root@master ~]# sh images.sh //初始化k8s集群 1234[root@master ~]# kubeadm init --kubernetes-version&#x3D;v1.15.0 \\&gt; --pod-network-cidr&#x3D;10.244.0.0&#x2F;16 \\&gt; --service-cidr&#x3D;10.96.0.0&#x2F;12 \\&gt; --ignore-preflight-errors&#x3D;Swap //如果初始化失败，需要重置k8s集群 1[root@master ~]# kubeadm reset //初始化完成后的操作 123[root@master ~]# mkdir -p $HOME&#x2F;.kube[root@master ~]# cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config[root@master ~]# chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config //查看节点信息情况 123[root@master ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONmaster NotReady master 10m v1.15.0 //部署flannel网络，（k8s版本必须是1.7版本以上） 1[root@master ~]# kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;coreos&#x2F;flannel&#x2F;master&#x2F;Documentation&#x2F;kube-flannel.yml PS：这里执行不成功的话可能是网络的问题 //在node01、node02上提前导入镜像（不然在加入集群的时候，它会自动下载镜像） 12345[root@node02 ~]# mkdir images[root@node02 ~]# cd images&#x2F;[root@node02 images]# lskube-proxy-1-15.tar myflannel-11-0.tar pause-3-1.tardocker load &lt; kube-proxy-1-15.tar &amp;&amp; docker load &lt; myflannel-11-0.tar &amp;&amp; docker load &lt; pause-3-1.tar //node01、node02加入集群 1234567kubeadm join 192.168.1.70:6443 --token x85ks8.4x5qrhw87zct1vti \\ --discovery-token-ca-cert-hash sha256:227c69c29f16521a7dccb52104710b8cdd449aa0f7cb787affb62514fc8cc9eb[root@master ~]# kubectl get nodeNAME STATUS ROLES AGE VERSIONmaster Ready master 25m v1.15.0node01 Ready &lt;none&gt; 82s v1.15.0node02 Ready &lt;none&gt; 76s v1.15.0 //确保是running的状态 1234567891011121314[root@master ~]# kubectl get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-5c98db65d4-fr894 1&#x2F;1 Running 0 28mkube-system coredns-5c98db65d4-qkqh5 1&#x2F;1 Running 0 28mkube-system etcd-master 1&#x2F;1 Running 0 27mkube-system kube-apiserver-master 1&#x2F;1 Running 0 27mkube-system kube-controller-manager-master 1&#x2F;1 Running 0 27mkube-system kube-flannel-ds-amd64-rjnns 1&#x2F;1 Running 0 4m44skube-system kube-flannel-ds-amd64-tpkh5 1&#x2F;1 Running 0 4m50skube-system kube-flannel-ds-amd64-x425t 1&#x2F;1 Running 0 13mkube-system kube-proxy-4qsj2 1&#x2F;1 Running 0 4m44skube-system kube-proxy-gngnx 1&#x2F;1 Running 0 28mkube-system kube-proxy-shkw9 1&#x2F;1 Running 0 4m50skube-system kube-scheduler-master 1&#x2F;1 Running 0 27m //设置tab键的距离 123[root@master ~]# vim .vimrcset tabstop&#x3D;2[root@master ~]# source .vimrc //将kubectl命令加入tab自动补全 123[root@master ~]# source &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;bash_completion [root@master ~]# source &lt;(kubectl completion bash)[root@master ~]# echo &quot; source &lt;(kubectl completion bash)&quot; &gt;&gt; ~&#x2F;.bashrc","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"}]},{"title":"Pod资源对象+健康检查","slug":"Pod资源对象+健康检查","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.287Z","comments":true,"path":"2020/01/24/Pod资源对象+健康检查/","link":"","permalink":"http://yoursite.com/2020/01/24/Pod%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1+%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5/","excerpt":"","text":"Deployment、Service、Pod是k8s最核心的3个资源对象Deployment： 最常见的无状态的控制器，支持应用的扩容缩容、滚动更新等操作 Service： 为弹性变动且存在生命周期的Pod对象提供了一个固定的访问接口，用户服务发现和服务访问 Pod： 是运行容器以及调度的最小单位，同一个Pod可以同时运行多个容器，这些容器共享NET、UTS、IPC，除此之外还有USER、PID、MOUNT ReplicationController：（rc） 用于确保每个Pod副本在任意时刻都能满足目标数量，简单点来说，它用于保证每个容器或容器组总是运行并且可以访问：老一代无状态的Pod控制器 ReplicaSet：（rs） 新一代无状态的Pod应用控制器，它与rc的不同之处在于支持的标签选择器不同，rc只支持等值选择器，rs还额外支持基于集合的选择器 StatefulSet： 用于管理有状态的持久化应用，如database服务程序，它与Deployment不同之处在于，它会为每一个Pod创建一个独有的持久性标识符，并确保每个Pod之间的顺序性 DamonSet： 确保每一个节点都运行了某个Pod的一个副本，新增的节点一样会被添加此类Pod，在节点移除时Pod会被收回 Job： 用于管理运行完成后即可终止的银应用，例如批量处理作业任务 volume: PV PVC ConfigMap: Secret： Role： ClusterRole： ClusterRoleBinding： Service account： Helm： Namespace：名称空间 默认的名称空间：Default //查看名称空间 123456[root@master ~]# kubectl get nsNAME STATUS AGEdefault Active 6d22hkube-node-lease Active 6d22hkube-public Active 6d22hkube-system Active 6d22h //查看名称空间详细信息 1[root@master ~]# kubectl describe ns default //创建名称空间 1[root@master ~]# kubectl create ns bdqn //使用yaml创建名称空间 123456[root@master ~]# vim test-ns.yamlapiVersion: v1kind: Namespacemetadata: name: test[root@master ~]# kubectl apply -f test-ns.yaml PS： namespace资源对象仅用于资源对象的隔离，并不能隔绝不同名称空间的Pod之间的通信，那是网络策略资源的功能 //删除名称空间： 12[root@master ~]# kubectl delete ns test[root@master ~]# kubectl delete -f test-ns.yaml //查看指定名称空间的资源可以使用–namespace或者-n选项 12[root@master ~]# kubectl get pod --namespace&#x3D;bdqn [root@master ~]# kubectl get pod -n bdqn Pod //通过yaml文件手动创建pod 12345678910111213141516[root@master ~]# vim pod.yamlkind: PodapiVersion: v1metadata: name: test-pod namespace: bdqn &#x2F;&#x2F;指定名称空间spec: containers: - name: test-app image: httpd:v1[root@master ~]# kubectl apply -f pod.yaml[root@master ~]# kubectl get pod -n bdqnNAME READY STATUS RESTARTS AGEtest-pod 1&#x2F;1 Running 0 19s&#x2F;&#x2F;删除[root@master ~]# kubectl delete pod -n bdqn test-pod Pod中镜像获取策略： Always： 镜像标签为“lastest”或镜像不存在时，总是从指定的仓库中获取镜像 ifNotPresent： 仅当本地镜像不存在时从目标仓库中下载 Never： 禁止从仓库下载镜像，即只是用本地镜像 123456789101112131415kind: PodapiVersion: v1metadata: name: test-pod namespace: bdqn labels: app: test-webspec: containers: - name: test-app image: httpd:v1 imagePullPolicy: IfNotPresent &#x2F;&#x2F;指定镜像获取策略 ports: - protocol: TCP containerPort: 90 &#x2F;&#x2F;手动创建的Pod，指定容器暴露的端口也不会生效 PS： 对于标签为“latest”或者这标签不存在，其默认镜像下载策略为“Always”,而对于其他标签的镜像，默认策略为“ifNotPresent” 容器的重启策略 Always： 但凡Pod对象终止就将其重启，此为默认设定 OnFailure： 仅在Pod对象出现错误时才将其重启 Never： 从不重启 Pod的默认健康检查 12345678910111213141516171819202122[root@master ~]# vim healcheck.yamlapiVersion: v1kind: Podmetadata: labels: test: healcheck name: healcheckspec: restartPolicy: OnFailure containers: - name: healthcheck image: busybox args: &#x2F;&#x2F;由镜像运行成容器的时候去做的事情 - &#x2F;bin&#x2F;sh - -c - sleep 20; exit 1[root@master ~]# vim healcheck.yaml[root@master ~]# kubectl get pod -w &#x2F;&#x2F;实时查看它的状态[root@master ~]# kubectl get podNAME READY STATUS RESTARTS AGEhealthcheck 0&#x2F;1 Error 5 8m44s&#x2F;&#x2F;可以看到每过20s就会重启一次 LivenessProbe（活跃度探测） 1234567891011121314151617181920212223[root@master ~]# vim liveness.yamlkind: PodapiVersion: v1metadata: name: liveness labels: test: livenessspec: restartPolicy: OnFailure containers: - name: liveness image: busybox args: &#x2F;&#x2F;由镜像运行成容器的时候去做的事情 - &#x2F;bin&#x2F;sh - -c - touch &#x2F;tmp&#x2F;test; sleep 60; rm -rf &#x2F;tmp&#x2F;test; sleep 300 livenessProbe: &#x2F;&#x2F;活跃度探测 exec: command: - cat - &#x2F;tmp&#x2F;test initialDelaySeconds: 10 &#x2F;&#x2F;pod运行10秒后开始探测 periodSeconds: 5 &#x2F;&#x2F;每5秒探测一次 PS: Liveness活跃度探测，根据某个文件是否存在，来确认某个服务是否正常运行，如果存在则正常，否则，它会根据你设置的Pod的重启策略操作Pod Readiness（敏捷探测、就绪性探测） 1234567891011121314151617181920212223[root@master ~]# vim readiness.yaml kind: PodapiVersion: v1metadata: name: readiness labels: test: readinessspec: restartPolicy: OnFailure containers: - name: readiness image: busybox args: - &#x2F;bin&#x2F;sh - -c - touch &#x2F;tmp&#x2F;test; sleep 60; rm -rf &#x2F;tmp&#x2F;test; sleep 300 readinessProbe: exec: command: - cat - &#x2F;tmp&#x2F;test initialDelaySeconds: 10 periodSeconds: 5 PS：总结liveness和readiness探测 1）leveness和readiness是两种健康检查机制，如果不特意配置，k8s两种探测采取相同的默认行为，即通过判断容器启动进程的返回是否为零，来判断探测是否成功 2）两种探测配置方法完全一样，不同之处在于探测失败后的行为： liveness探测是根据Pod重启策略操作容器，大多数是重启容器 readinesss则是将容器设置为不可用，不接收Service转发的请求 3）两种探测方法可以独立存在，也可以同时使用，用liveness判断容器是否需要实现自愈；用readiness判断容器是否已经准备好对外提供服务 监控检测应用 在scale（扩容、缩容）中的应用 123456789101112131415161718192021222324252627282930313233343536373839404142[root@master ~]# vim hcscal.yamlkind: DeploymentapiVersion: extensions&#x2F;v1beta1metadata: name: webspec: replicas: 3 template: metadata: labels: run: web spec: containers: - name: web image: httpd ports: - containerPort: 80 readinessProbe: httpGet: scheme: HTTP path: &#x2F;healthy port: 80 initialDelaySeconds: 10 periodSeconds: 5---kind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort selector: run: web ports: - protocol: TCP port: 90 targetPort: 80 nodePort: 30321[root@master ~]# kubectl exec web-69d659f974-ktqbz touch &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs&#x2F;healthy[root@master ~]# kubectl describe svcEndpoints: 10.244.1.4:80 在更新过程中的使用 1234567891011121314151617181920212223242526[root@master ~]# vim app.v1.yamlkind: DeploymentapiVersion: extensions&#x2F;v1beta1metadata: name: appspec: replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - &#x2F;bin&#x2F;sh - -c - sleep 10; touch &#x2F;tmp&#x2F;healthy; sleep 3000 readinessProbe: exec: command: - cat - &#x2F;tmp&#x2F;healthy initialDelaySeconds: 10 periodSeconds: 5 //第一次升级 12345678[root@master ~]# kubectl apply -f app.v1.yaml --record[root@master ~]# cp app.v1.yaml app.v2.yaml [root@master ~]# vim app.v2.yaml&#x2F;&#x2F;修改 args: - &#x2F;bin&#x2F;sh - -c - sleep 3000 //第二次升级 123456789101112131415161718192021[root@master ~]# cp app.v1.yaml app.v3.yaml [root@master ~]# vim app.v3.yaml&#x2F;&#x2F;删除探测机制kind: DeploymentapiVersion: extensions&#x2F;v1beta1metadata: name: appspec: replicas: 10 template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - &#x2F;bin&#x2F;sh - -c - sleep 3000 maxSurge：此参数控制滚动更新过程中，副本总数超过预期数的值，可以是整数，也可以是百分比，默认为1 maxUnavilable：不可用Pod的值，默认为1","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"}]},{"title":"Prometheus（普罗米修斯）","slug":"Prometheus（普罗米修斯）","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.287Z","comments":true,"path":"2020/01/24/Prometheus（普罗米修斯）/","link":"","permalink":"http://yoursite.com/2020/01/24/Prometheus%EF%BC%88%E6%99%AE%E7%BD%97%E7%B1%B3%E4%BF%AE%E6%96%AF%EF%BC%89/","excerpt":"","text":"Prometheus（普罗米修斯）是一个系统和服务的监控平台。它可以自定义时间间隔从已配置的目标收集指标，评估规则表达式，显示结果，并在发现某些情况时触发警报 与其他监视系统相比，Prometheus的主要区别特征是： 一个多维数据模型（时间序列由指标名称定义和设置键/值尺寸） 一个灵活的查询语言来利用这一维度 不依赖于分布式存储；单服务器节点是自治的 时间序列收集通过HTTP 上的拉模型进行 通过中间网关支持推送时间序列 通过服务发现或静态配置发现目标 多种图形和仪表板支持模式 支持分层和水平联合 实验环境： docker01 192.168.1.70 NodeEXporter cAdvisor+Prometheus server+gragana docker02 192.168.1.60 NodeEXporter cAdvisor docker03 192.168.1.50 NodeEXporter cAdvisor 123456[root@localhost ~]# hostnamectl set-hostname docker1[root@localhost ~]# su -[root@localhost ~]# hostnamectl set-hostname docker2[root@localhost ~]# su -[root@localhost ~]# hostnamectl set-hostname docker3[root@localhost ~]# su - 全部关闭防火墙，禁用selinux 123[root@docker01 ~]# systemctl stop firewalld[root@docker01 ~]# systemctl disable firewalld[root@docker01 ~]# setenforce 0 需要部署的组件 Prometheus server（9090）：普罗米修斯的主机服务器 NodeEXporter（9100）：负责收集host硬件信息和操作系统信息 //谷歌开发的监控软件。收集数据，不太直观。有历史保留，方便后期做优化 cAdvisor（8080）：负责收集host上运行的容器信息 Grafana（3000）：负责展示普罗米修斯监控界面 //类似于kibana的图形界面，提供可视化web页面 1）三个节点，全部部署node-exporter和cadvisor //分别在三个节点上导入镜像 1[root@docker01 ~]# docker load &lt; node-exporter.tar &amp;&amp; docker load &lt; mycadvisor.tar //部署node-exporter，收集硬件和系统信息（三台都需要部署） 12345[root@docker01 ~]# docker run -d -p 9100:9100 -v &#x2F;proc:&#x2F;host&#x2F;proc \\&gt; -v &#x2F;sys:&#x2F;host&#x2F;sys -v &#x2F;:&#x2F;rootfs --net&#x3D;host \\&gt; prom&#x2F;node-exporter --path.procfs &#x2F;host&#x2F;proc \\&gt; --path.sysfs &#x2F;host&#x2F;sys \\&gt; --collector.filesystem.ignored-mount-points &quot;^&#x2F;(sys|proc|dev|host|etc)($|&#x2F;)&quot; PS：注意这里使用了–net=host，这样prometheus server可以直接与node-exporter通信 验证，打开浏览器验证结果（192.168.1.70:9100） ) ) //部署安装cAdvisor，收集节点容器信息（三台都需要部署） 1234[root@docker01 ~]# docker run -v &#x2F;:&#x2F;rootfs:ro -v &#x2F;var&#x2F;run:&#x2F;var&#x2F;run&#x2F;:rw \\&gt; -v &#x2F;sys:&#x2F;sys:ro -v &#x2F;var&#x2F;lib&#x2F;docker:&#x2F;var&#x2F;lib&#x2F;docker:ro \\&gt; -p 8080:8080 --detach&#x3D;true --name&#x3D;cadvisor \\&gt; --net&#x3D;host google&#x2F;cadvisor 打开浏览器验证（192.168.1.70:8080） ) 2）在docker01上部署prometheus server服务 在部署prometheus之前，我们需要对它的配置文件进行修改，所以我们先运行一个容器，将其配置文件拷贝出来 123456789[root@docker01 ~]# docker load &lt; prometheus.tar[root@docker01 ~]# docker run -d -p 9090:9090 --name prometheus --net&#x3D;host prom&#x2F;prometheus[root@docker01 ~]# docker exec -it prometheus &#x2F;bin&#x2F;sh&#x2F;prometheus $ cd &#x2F;etc&#x2F;prometheus&#x2F;&#x2F;etc&#x2F;prometheus $ lsconsole_libraries consoles prometheus.yml[root@docker01 ~]# docker cp prometheus:&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml .&#x2F;[root@docker01 ~]# vim prometheus.yml- targets: [&#39;localhost:9090&#39;,&#39;localhost:8080&#39;,&#39;localhost:9100&#39;,&#39;192.168.1.60:8080&#39;,&#39;192.168.1.60:9100&#39;,&#39;192.168.1.50:8080&#39;,&#39;192.168.1.50:9100&#39;] PS：这里制定了prometheus的监控项，包括它也会监控自己收集到的数据 1[root@docker01 ~]# docker rm -f prometheus //重新运行容器： 1[root@docker01 ~]# docker run -d -p 9090:9090 --name prometheus --net&#x3D;host -v &#x2F;root&#x2F;prometheus.yml:&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml prom&#x2F;prometheus //浏览器访问：192.168.1.70:9090 ) ) PS:这里能够看到我们各个监控项 3）在docker01上部署grafana服务，用来展示prometheus收集到的数据 1234[root@docker01 ~]# docker load &lt; grafana.tar[root@docker01 ~]# mkdir grafana-storage[root@docker01 ~]# chmod 777 -R grafana-storage&#x2F;[root@docker01 ~]# docker run -d -p 3000:3000 --name grafana -v &#x2F;root&#x2F;grafana-storage:&#x2F;var&#x2F;lib&#x2F;grafana -e &quot;GF_SECURITY_ADMIN_PASSWORD&#x3D;123.com&quot; grafana&#x2F;grafana 浏览器访问：192.168.1.70:3000 用户名：admin 密码：123.com ) //创建数据源 ) ) ) ) PS：看到这这提示，说明prometheus和grafana服务是正常连接的 此时，虽然grafana收集到了数据，但怎么显示它，仍然是个问题，grafana支持自定义显示信息，不过要自定义起来非常麻烦，不过好在，grafana官方为我们提供了一些模板，来供我们使用 Grafana官网：https://grafana.com ) //可以根据自己的喜好选择模板 ) 选中一款模板后，然后，我们又两种方式可以套用这个模板 第一种方式：通过JSON文件使用模板 ) 下载完成之后，回到grangana控制台 ) ) ) ) 第二种方式： 可以直接通过模板的ID号 ) ) ) ) ) 配置AlertManagerAlertManager：用来接收prometheus发送的报警信息，并且执行设置好的报警方式、报警内容 AlertManager.yml配置文件： global：全部配置，包括报警解决后的超时时间、SMTP相关配置、各种渠道通知的API地址等新消息 route：用来设置报警的分发策略 receivers：配置告警消息接收者信息 inhibit_rules：抑制规则配置，当存在于另一组匹配的报警时，抑制规则将禁用一组匹配的警报 //运行一个容器获取配置文件并进行配置 12345678910111213141516171819202122232425262728[root@docker01 ~]# docker run -d --name alertmanager -p 9093：993 prom&#x2F;alertmanager:latest[root@docker01 ~]# docker cp alertmanager:&#x2F;etc&#x2F;alertmanager&#x2F;alertmanager .&#x2F;[root@docker01 ~]# cat alertmanager.yml global: resolve_timeout: 5m smtp_from: &#39;2960824193@qq.com&#39; smtp_smarthost: &#39;smtp.qq.com:465&#39; smtp_auth_username: &#39;2960824193@qq.com&#39; smtp_auth_password: &#39;aseydtzejqfqdhai&#39; smtp_require_tls: false smtp_hello: &#39;qq.com&#39;route: group_by: [&#39;alertname&#39;] group_wait: 5s group_interval: 5s repeat_interval: 5m receiver: &#39;email&#39;receivers:- name: &#39;email&#39; email_configs: - to: &#39;2960824193@qq.com&#39; send_resolved: trueinhibit_rules: - source_match: severity: &#39;critical&#39; target_match: severity: &#39;warning&#39; equal: [&#39;alertname&#39;, &#39;dev&#39;, &#39;instance&#39;] //删除容器并重新运行 12[root@docker01 ~]# docker rm -f alertmanager[root@docker01 ~]# docker run -d --name alertmanager -p 9093:9093 -v &#x2F;root&#x2F;alertmanager.yml:&#x2F;etc&#x2F;alertmanager&#x2F;alertmanager.yml prom&#x2F;alertmanager:latest //浏览器访问：192.168.1.70:9093 ) Prometheus配置alertmanager报警规则 1234567891011121314151617181920212223[root@docker01 ~]# mkdir -p prometheus&#x2F;rules[root@docker01 ~]# cd prometheus&#x2F;rules&#x2F;[root@docker01 rules]# lsnode-up.rules[root@docker01 rules]# cat node-up.rules groups:- name: node-up rules: - alert: node-up expr: up&#123;job&#x3D;&quot;prometheus&quot;&#125; &#x3D;&#x3D; 0 &#x2F;&#x2F;这个job要和prometheus里的job名称一样 for: 15s labels: severity: 1 team: node annotations: summary: &quot;&#123;&#123; $labels.instance &#125;&#125; 已停止运行超过 15s！&quot;-----------------------------------------------------------------------------------------------&#x2F;etc&#x2F;prometheus $ cat prometheus.yml.......- job_name: &#39;prometheus&#39;&#x2F;&#x2F;监控的内容是： - targets: [&#39;localhost:9090&#39;,&#39;localhost:8080&#39;,&#39;localhost:9100&#39;,&#39;192.168.1.60:9100&#39;,&#39;192.168.1.60:8080&#39;,&#39;192.168.1.50:9100&#39;,&#39;192.168.1.50:8080&#39;]....... //编辑prometheus的配置文件 12345678910[root@docker01 ~]# vim prometheus.yml# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: - 192.168.1.70:9093 &#x2F;&#x2F;目标为alertmanager容器rule_files: - &quot;&#x2F;usr&#x2F;local&#x2F;prometheus&#x2F;rules&#x2F;*.rules&quot; &#x2F;&#x2F;路径是容器内的路径 //删除prometheus容器，重新挂载配置文件 12[root@docker01 ~]# docker rm -f prometheus[root@docker01 ~]# docker run -d -p 9090:9090 --name prometheus --net&#x3D;host -v &#x2F;root&#x2F;prometheus.yml:&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml -v &#x2F;root&#x2F;prometheus&#x2F;rules&#x2F;node-up.rules:&#x2F;usr&#x2F;local&#x2F;prometheus&#x2F;rules&#x2F;node-up.rules prom&#x2F;prometheus //浏览器访问：192.168.1.70:9090 ) //随便关闭一个容器进行测试 ) AlertManager配置自定义邮件模板 1234567891011121314151617[root@docker01 ~]# mkdir prometheus&#x2F;alertmanager-tmpl[root@docker01 ~]# cd prometheus&#x2F;alertmanager-tmpl&#x2F;[root@docker01 alertmanager.tmpl]# cat email.tmpl &#123;&#123; define &quot;email.from&quot; &#125;&#125;2960824193@qq.com&#123;&#123; end &#125;&#125; &#x2F;&#x2F;哪个邮箱来发送信息&#123;&#123; define &quot;email.to&quot; &#125;&#125;2960824193@qq.com&#123;&#123; end &#125;&#125; &#x2F;&#x2F;发送到哪个邮箱&#123;&#123; define &quot;email.to.html&quot; &#125;&#125;&#123;&#123; range .Alerts &#125;&#125;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;start&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&lt;br&gt;告警程序: prometheus_alert&lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; 级&lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125;&lt;br&gt;故障主机: &#123;&#123; .Labels.instance &#125;&#125;&lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125;&lt;br&gt;触发时间: &#123;&#123; .StartsAt.Format &quot;2019-08-04 16:58:15&quot; &#125;&#125; &lt;br&gt;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;end&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125; //修改altermanager的配置文件 1234567[root@docker01 ~]# vim alertmanager.yml&#x2F;&#x2F;在第九行添加templates: - &#39;&#x2F;etc&#x2F;alertmanager-tmpl&#x2F;*.tmpl&#39; &#x2F;&#x2F;容器内的路径&#x2F;&#x2F;修改第20行，和第21行： - to: &#39;&#123;&#123; template &quot;email.to&quot; &#125;&#125;&#39; &#x2F;&#x2F;必须和email.tmpl中的&#123;&#123; define “email.to”&#125;&#125; 2960824193@qq.com&#123;&#123;end&#125;&#125;对应 html: &#39;&#123;&#123; template &quot;email.to.html&quot; . &#125;&#125;&#39; &#x2F;&#x2F;必须和email.tml中的&#123;&#123; define &quot;email.to.html&quot; &#125;&#125;名字对应 //删除alertmanager容器，重新运行并挂载 12[root@docker01 ~]# docker rm -f alertmanager[root@docker01 ~]# docker run -d --name alertmanager -p 9093:9093 -v &#x2F;root&#x2F;alertmanager.yml:&#x2F;etc&#x2F;alertmanager&#x2F;alertmanager.yml -v &#x2F;root&#x2F;prometheus&#x2F;alertmanager-tmpl&#x2F;:&#x2F;etc&#x2F;altermanager-tmpl prom&#x2F;alertmanager:latest //停止容器测试 )","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"}]},{"title":"k8s架构、基本概念","slug":"k8s架构、基本概念","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.303Z","comments":true,"path":"2020/01/24/k8s架构、基本概念/","link":"","permalink":"http://yoursite.com/2020/01/24/k8s%E6%9E%B6%E6%9E%84%E3%80%81%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"","text":"k8s总架构：) Master节点：（默认不参加工作） kubectl：k8s是命令端，用来发送客户端的操作指令 k8s的原生组件：（部署k8s比必不可少的组件） API server：是k8s集群的前端接口，各种客户端工具以及k8s的其他组件可以通过它管理k8s集群的各种资源，它提供了HTTP/HTTPS RESTful API，即k8s API Scheduler：负责决定将Pod放在哪个Node上运行，在调度时，会充分考虑集群内的拓扑结构，当前各个节点的负载情况，以及对高可用、性能、数据和亲和性需求 Controller Manager：负责管理集群的各种资源，保证资源处于预期的状态，它由多种Controller组成，包括Replication Controller、Endpoints Controller、Namespace Controller、Serviceaccounts Controller等等 Etcd：负责保存k8s集群的配置信息和各种资源的状态信息，当数据发生变化时，etcd会快速的通知k8s相关组件。第三方组件，意味着它有可替换方案，比如：Consul、zookeeper Pod：k8s集群的最小组成单位，一个Pod内，可以运行一个或多个容器，大多数情况下，一个Pod内只有一个Container容器 Flannel：是k8s集群网络解决方案，可以保证Pod的跨主机通信。第三方解决方案，也有替换方案 Node节点：kubelet：它是Node的agent（代理），当Scheduler确定某个Node上运行Pod之后，会将Pod的具体配置信息发送给该节点的kubelet，kubelet会根据这些信息创建和运行容器，并向Master报告运行状态 kube-proxy：负责将访问service的TCP/UDP数据流转发到后端容器，如果有多个副本，kube-pory会实现负载均衡 运行一个例子： //创建一个deployment资源对象。Pod控制器 1[root@master ~]# kubectl run test-web --image&#x3D;httpd --replicas&#x3D;2 分析各个组件的作用以及架构流程： kubectl发送部署请求到API server API server通知Controller Manager创建一个Deployment资源 Scheduler执行调度任务，将两个副本Pod分发到node01和node02上 node1和node02上的kubelet在各个节点上创建并运行Pod 补充： 应用的配置和当前的状态信息报错在etcd中，执行kubectl get pod时API server会从etc中读取这 些数据 flannel会为每个Pod分配一个IP，但此时没有创建Service资源，目前kube-pory还没有参与进来","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"}]},{"title":"创建资源的两种方式","slug":"创建资源的两种方式","date":"2020-01-23T16:00:00.000Z","updated":"2020-01-25T13:07:32.303Z","comments":true,"path":"2020/01/24/创建资源的两种方式/","link":"","permalink":"http://yoursite.com/2020/01/24/%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/","excerpt":"","text":"创建资源的两种方式 用命令行的方式创建：//创建Pod控制器，deployments 1[root@master ~]# kubectl run web --image&#x3D;nginx --replicas&#x3D;5 //查看控制器情况 1[root@master ~]# kubectl get deployments. //查看资源详细信息 1[root@master ~]# kubectl describe deployments. web PS：查看某种资源对象，没有指名称空间，默认是在default名称空间，可以加上-n选项，查看指定名称空间 1[root@master ~]# kubectl get pod -n&#x3D;kube-system 注意：直接运行创建的Deployment资源对象，是经常使用的一个控制器类型，除了deployment，还有rc，rs等Pod控制器，Deployment是一个高级的Pod控制器 //创建Service资源类型 1[root@master ~]# kubectl expose deployment web --name&#x3D;web-svc --port&#x3D;80 --type&#x3D;NodePort PS：如果想要外网能够访问服务，可以暴露deployment资源，得到service资源，但svc资源的类型必须为NodePoet 映射端口范围：30000-32767 服务的扩容与缩容： 1[root@master ~]# kubectl scale deployment web --replicas&#x3D;8 //通过修改yuml文件进行扩容与缩容 1[root@master ~]# kubectl edit deployments. web 服务的升级与回滚: 1[root@master ~]# kubectl set image deployment web web&#x3D;nginx:1.15 //通过修改配置文件进行升级 1[root@master ~]# kubectl edit deployments. web //回滚 1[root@master ~]# kubectl rollout undo deployment web 配置清单（yml、yaml）： 常见yaml文件写法，以及字段的作用： 五个一级字段： apiVersion: api版本信息 kind: 资源对象的类别 metadata: 元数据。名称字段必须写 spec: 用户期望的状态 status: 资源现在处于什么样的状态 Deployment 12345678910111213141516[root@master ~]# vim web.yamlkind: DeploymentapiVersion: extensions&#x2F;v1beta1metadata: name: webspec: replicas: 2 template: metadata: labels: app: web_server spec: containers: - name: nginx image: nginx[root@master ~]# kubectl apply -f web.yaml service 123456789101112kind: ServiceapiVersion: v1metadata: name: web-svcspec: selector: &#x2F;&#x2F;标签选择器，和Deployment里的标签要一样 app: web_server ports: - protocol: TCP port: 80 targetPort: 80[root@master ~]# kubectl apply -f web-svc.yml 使用相同的标签和标签选择器，使两个资源对象相互关联 PS：（本质的意义：提供一个统一的接口） 创建的Service资源对象，默认的type为ClusterIP,意味着集群内任何节点都可以访问，它的作用是为后端真正提供服务的Pod提供一个统一的访问接口,如果想要外网访问服务，应该把type改为NodePort 12345678910111213kind: ServiceapiVersion: v1metadata: name: web-svcspec: type: NodePort &#x2F;&#x2F;指定类型，让外网来访问 selector: app: web_server ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30033 &#x2F;&#x2F;指定集群映射端口，范围是30000-32767","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"}]}]}